{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ ME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook is implemeted in a repository in github with input and output folders.\n",
    "#### It has 2 parts:\n",
    "##### > Part 1: Pre-defined funtions for each technique.\n",
    "##### > Part 2: Execution of model pipelines, here users can modify which combination of techniques they want to run. The scores will be printed as a csv in output folders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from BorutaShap import BorutaShap\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score, roc_auc_score, log_loss, cohen_kappa_score, make_scorer\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is to read, transform and join 2 data frame\n",
    "\n",
    "def read_features():\n",
    "    path = 'input/secom.data'\n",
    "    df = pd.read_csv(path, delimiter=' ', header=None, na_values=['NaN'])\n",
    "    df.columns = ['feature_'+str(x+1) for x in range(len(df.columns))]\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_target():\n",
    "    path = 'input/secom_labels.data'\n",
    "    df = pd.read_csv(path, delimiter=' ', header=None, na_values=['NaN'])\n",
    "    df.columns = ['status','timestamp']\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'],dayfirst=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the duplicated features (columns)\n",
    "def remove_duplicated_columns(df):\n",
    "    list_duplicate = []\n",
    "    to_remove = []\n",
    "    for i in range(0, len(df.columns)):\n",
    "        l = []\n",
    "        for j in range(i+1,len(df.columns)):\n",
    "            if df.iloc[:,i].equals(df.iloc[:,j]) == True:\n",
    "                if j not in list_duplicate:\n",
    "                    l.append(j)\n",
    "                    to_remove.append('feature_'+str(j+1))\n",
    "                list_duplicate.append(i)\n",
    "                list_duplicate.append(j)\n",
    "\n",
    "    return df.drop(columns=to_remove, axis = 1)\n",
    "\n",
    "# X = remove_duplicated_columns(X)\n",
    "# X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove columns with Constant volatility (std=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_volatility(df):\n",
    "    df_EDA= df.describe().T\n",
    "    df_EDA= df_EDA[df_EDA[\"std\"] == 0]\n",
    "    df = df.drop(axis=1, columns=df_EDA.index)\n",
    "    return df\n",
    "\n",
    "# X = remove_constant_volatility(X)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove columns with high %Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols_with_high_pct_null(df, null_threshold):\n",
    "    list_column_with_pct_null = pd.concat([df.isnull().sum(), df.isnull().sum()/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\n",
    "    list_column_with_pct_null= list_column_with_pct_null[list_column_with_pct_null[\"Percentage (%)\"] >= null_threshold]\n",
    "    df = df.drop(axis=1, columns=list_column_with_pct_null.index)\n",
    "    return df\n",
    "\n",
    "# X = remove_cols_with_high_pct_null(X, 0.8)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how = ['NaN', '3s' ,'nothing']\n",
    "def replace_outlier(df, how):\n",
    "    for col in df:\n",
    "        ll_col = df[col].mean() - 3 * df[col].std()\n",
    "        ul_col = df[col].mean() + 3 * df[col].std()\n",
    "        if how == 'NaN':\n",
    "            df[col] = np.where(df[col]>ul_col,np.NaN,np.where(df[col]<ll_col,np.NaN,df[col]))\n",
    "        elif how == '3s':\n",
    "            df[col] = np.where(df[col]>ul_col,ul_col,np.where(df[col]<ll_col,ll_col,df[col]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which_weights = ['distance','uniform']\n",
    "\n",
    "def impute_null_with_knn(X_train, X_test, which_weights):\n",
    "    #First scale the data \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns= X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns= X_test.columns)\n",
    "\n",
    "    knn = KNNImputer(n_neighbors=5, weights=which_weights) #check this neighbors = 5\n",
    "\n",
    "    X_train = pd.DataFrame(knn.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(knn.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    X_train = pd.DataFrame(scaler.inverse_transform(X_train), columns= X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.inverse_transform(X_test), columns= X_test.columns)\n",
    "    return X_train, X_test\n",
    "\n",
    "#X_train = impute_null_with_knn(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null_with_mice(X_train, X_test): \n",
    "    imp = IterativeImputer(max_iter=5, verbose=0, imputation_order='roman', random_state=0)\n",
    "    X_train = pd.DataFrame(imp.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(imp.transform(X_test), columns=X_test.columns)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is BorutaShap with TENTATIVE features\n",
    "\n",
    "#list_method=['shap','gini']\n",
    "\n",
    "def BorutaShap_FS (X, y, method_option) :\n",
    "    #modelshap = RandomForestClassifier(n_jobs=-1,n_estimators=100, class_weight='balanced_subsample', max_depth=5, random_state=100)\n",
    "    modelshap = RandomForestClassifier(n_jobs=-1,n_estimators=100, max_depth=5, random_state=100)\n",
    "\n",
    "    # define model for resp. classifier\n",
    "    modelshap.fit(X,y)\n",
    "    feature_names = np.array(X.columns)\n",
    "    # define Boruta Sahp feature selection method\n",
    "    feature_selector = BorutaShap(model=modelshap,\n",
    "                              importance_measure=method_option,\n",
    "                              classification=True)  # find all relevant features\n",
    "    feature_selector.fit(X,y,n_trials=100,sample = False, verbose = False,random_state=100)  \n",
    "    #feature_selector.plot(which_features='accepted',figsize=(20,10))\n",
    "    tentative=X.loc[:,feature_selector.tentative]\n",
    "    selected=feature_selector.Subset()\n",
    "    selten=pd.concat([selected,tentative],axis=1)\n",
    "    # call transform() on X to filter it down to selected features\n",
    "    return  selten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE\n",
    "\n",
    "#classifier = ['RF', 'SVM']\n",
    "\n",
    "def RFE_FS (X, y,classify) :\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled= pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    feature_names = np.array(X_scaled.columns)\n",
    "    if classify == 'RF':\n",
    "    # define random forest classifier\n",
    "        model = RandomForestClassifier(n_jobs=-1, class_weight='balanced_subsample', max_depth=5, random_state=100)\n",
    "       \n",
    "    if classify== 'SVM':\n",
    "        model = SVC(kernel='linear',C=5)\n",
    "        #rfe = RFECV(estimator = model,scoring='accuracy')\n",
    "    # find all relevant features\n",
    "    model.fit(X_scaled, y)\n",
    "    rfe = RFE(estimator = model,n_features_to_select = 30)\n",
    "    rfe.fit(X_scaled,y)\n",
    "\n",
    "     # zip feature names, ranks, and decisions \n",
    "    feature_ranks = list(zip(feature_names, \n",
    "                             rfe.ranking_, \n",
    "                             rfe.support_))\n",
    "\n",
    "    final_features_rfe = list()\n",
    "    indexes = np.where(rfe.ranking_ <= 2)\n",
    "    for x in np.nditer(indexes):\n",
    "        final_features_rfe.append(feature_names[x])\n",
    "    \n",
    "    \n",
    "    # unscale the data before return\n",
    "    X_unscaled=pd.DataFrame(scaler.inverse_transform(X_scaled), columns=X_scaled.columns)\n",
    "    ff_rfe=pd.DataFrame(X_unscaled.filter(final_features_rfe))\n",
    "    \n",
    "\n",
    " # call transform() on X to filter it down to selected features\n",
    "    return  ff_rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boruta function with random forest\n",
    "\n",
    "def BorutaPy_FS (X, y) :\n",
    "    feature_names = np.array(X.columns)\n",
    "\n",
    "    # define random forest classifier\n",
    "    model = RandomForestClassifier(n_jobs=-1, class_weight='balanced_subsample', max_depth=5, random_state=100)\n",
    "    model.fit(X, y)\n",
    "    # define Boruta feature selection method\n",
    "    \n",
    "    feature_selector = BorutaPy(model, n_estimators='auto', verbose=0, random_state=100, max_iter=140)\n",
    "\n",
    "    # find all relevant features\n",
    "    feature_selector.fit(X.to_numpy(),y)\n",
    "\n",
    "    # check selected features\n",
    "    ##--feature_selector.support_\n",
    "\n",
    "    # check ranking of features\n",
    "    ##--feature_ranking=feature_selector.ranking_\n",
    "\n",
    "    # zip feature names, ranks, and decisions \n",
    "    # feature_ranks = list(zip(feature_names, \n",
    "    #                          feature_selector.ranking_, \n",
    "    #                          feature_selector.support_))\n",
    "\n",
    "    # print the results\n",
    "    ##--for feat in feature_ranks:\n",
    "    ##--    print('Feature: {:<30} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    "        \n",
    "    final_features = list()\n",
    "    indexes = np.where(feature_selector.ranking_ <= 2) #change to 2\n",
    "    for x in np.nditer(indexes):\n",
    "        final_features.append(feature_names[x])\n",
    "    ##--print(final_features)\n",
    "    \n",
    " # call transform() on X to filter it down to selected features\n",
    "    return pd.DataFrame(X.filter(final_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicolinearity treatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the highly collinear features from data\n",
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model \n",
    "        to generalize and improves the interpretability of the model.\n",
    "\n",
    "    Inputs: \n",
    "        x: features dataframe\n",
    "        threshold: features with correlations greater than this value are removed\n",
    "\n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i+1):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "\n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                #Print the correlated features and the correlation value\n",
    "                #print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns=drops)\n",
    "\n",
    "    return x\n",
    "\n",
    "#remove_collinear_features(X, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(X_train, y_train, sampler):\n",
    "    \n",
    "    #SMOTE\n",
    "    if sampler == 'SMOTE':\n",
    "        sampler = SMOTE(random_state=100)    \n",
    "    \n",
    "    #ROSE\n",
    "    if sampler == 'ROSE':\n",
    "        sampler = RandomOverSampler(random_state=100, shrinkage=1)\n",
    "\n",
    "    #ADASYN\n",
    "    if sampler == 'ADASYN':\n",
    "        sampler = ADASYN(random_state=100)\n",
    "    \n",
    "\n",
    "    #SMOTTEENN\n",
    "    if sampler == 'SMOTEENN' :\n",
    "        sampler = SMOTEENN(random_state=100)\n",
    "        \n",
    "        \n",
    "    #Random under Sampling\n",
    "    if sampler == \"randomunder\":\n",
    "        sampler = RandomUnderSampler(random_state=100)\n",
    "\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    #counter = Counter(y_resampled)\n",
    "    #print(counter)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# X_train, y_train = sampling(X_train, y_train,'SMOTE')\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model: Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Note:\n",
    "<br>How to Use Keras Models in scikit-learn:\n",
    "<br>-Keras models can be used in scikit-learn by wrapping them with the KerasClassifier or KerasRegressor class.\n",
    "<br>-To use these wrappers you must define a function that creates and returns your Keras sequential model, then pass this function to the build_fn argument when constructing the KerasClassifier class.\n",
    "<br>https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the accuracy that is produced by keras is similar to the one being manually calculated by argmax, same with scikit learn evaluate\n",
    "#grid seach does not build to apply on test set, after finding the best hyperparamter, we need to fit the model on train set again and use it for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = read_features()\n",
    "y = read_target().iloc[:,0]\n",
    "\n",
    "\n",
    "#step 1:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1, stratify=y)\n",
    "\n",
    "# step 2:\n",
    "X_train = remove_duplicated_columns(X_train)\n",
    "#step 3:\n",
    "X_train = remove_constant_volatility(X_train)\n",
    "#step 4:\n",
    "X_train = remove_cols_with_high_pct_null(X_train, 0.5) #this can be in the loop too, may be later\n",
    "#step 5: remove the same columns from step 2-4 TRAIN_TEST split\n",
    "X_test = X_test.loc[:,X_train.columns]\n",
    "\n",
    "\n",
    "\n",
    "#step 6: oulier treatement (on both TRAIN & TEST split)\n",
    "X_train = replace_outlier(X_train, '3s')\n",
    "X_test = replace_outlier(X_test, '3s')\n",
    "\n",
    "#step 7: missing value imputation (on both TRAIN & TEST split)\n",
    "X_train, X_test = impute_null_with_knn(X_train, X_test, 'distance')\n",
    "\n",
    "# #step 8: feature selection (on both TRAIN & TEST split)\n",
    "# X_train = BorutaShap_FS(X_train, y_train, 'shap')\n",
    "\n",
    "#make test set have the SAME features as train set\n",
    "X_test = X_test.loc[:,X_train.columns]\n",
    "\n",
    "#step 9: balancing only on TRAIN split\n",
    "X_train, y_train = sampling(X_train, y_train, 'SMOTEENN')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_backup, y_train_backup, X_test_backup, y_test_backup = X_train, y_train, X_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1808, 446)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 10)                60        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82\n",
      "Trainable params: 82\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='softmax', input_dim=5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 10:27:16.432473: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-04 10:27:16.432611: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-04 10:27:16.432951: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-04 10:27:16.433576: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-04 10:27:16.433688: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-04 10:27:16.434884: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-04 10:27:16.435148: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-04 10:27:16.437297: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 1s 11ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 5ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 10ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 6ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 1s 3ms/step\n",
      "19/19 [==============================] - 1s 12ms/step\n",
      "19/19 [==============================] - 1s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 1s 6ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 14ms/step\n",
      "19/19 [==============================] - 0s 13ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 1s 9ms/step\n",
      "19/19 [==============================] - 1s 6ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 5ms/step\n",
      "19/19 [==============================] - 0s 5ms/step\n",
      "19/19 [==============================] - 1s 16ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 5ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 1s 4ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 5ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 9ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 5ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 1s 3ms/step\n",
      "19/19 [==============================] - 1s 4ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 9ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 5ms/step\n",
      "19/19 [==============================] - 0s 6ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 10ms/step\n",
      "19/19 [==============================] - 0s 7ms/step\n",
      "19/19 [==============================] - 0s 9ms/step\n",
      "19/19 [==============================] - 0s 5ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "Best scores: 0.850077 (+-0.064101) using {'activation': 'relu', 'batch_size': 100, 'dropout_rate': 0, 'epochs': 20, 'neurons': 1.5}\n"
     ]
    }
   ],
   "source": [
    "#NN\n",
    "batch_size = [100]\n",
    "epochs = [20,50]\n",
    "activation = ['linear','softmax','relu'] \n",
    "dropout_rate = [0,0.1]\n",
    "neurons = [1,1.5,2]\n",
    "\n",
    "def create_model_NN(batch_size=100, epochs=50, activation='linear', dropout_rate=0.0, neurons=10):\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(round((input_dim+2)/2)*neurons, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    #model.add(Dense(round((neurons*5+2)/2), activation=activation))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model_NN, epochs=100, batch_size=10, verbose=0) #epochs and batch_size here does not matter?\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, activation=activation, dropout_rate=dropout_rate, neurons=neurons)\n",
    "\n",
    "# prepare the y set: to_categorical cannot work with negative numbers\n",
    "y_train = y_train.replace(-1, 0)\n",
    "y_test = y_test.replace(-1, 0)\n",
    "\n",
    "#one hot encode outputs\n",
    "# y_train_c = to_categorical(y_train)\n",
    "# y_test_c = to_categorical(y_test)\n",
    "\n",
    "#scoring = {'recall': make_scorer(recall_score),'accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, scoring='f1_micro')\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "## summarize results\n",
    "print(\"Best scores: %f (+-%f) using %s\" % (grid_result.best_score_, grid_result.cv_results_['std_test_score'][grid_result.best_index_], grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "\n",
    "\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (+-%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# df_gs_result = pd.DataFrame({'mean_acc': means, 'std_acc': stds, 'params':str(params)}, index = [i for i in range(means.shape[0])])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([4.63455192, 4.70958273, 4.38518031, 6.69540405, 6.93011141,\n",
       "        7.23192032, 4.96219365, 5.20900734, 4.57080936, 9.03615967,\n",
       "        7.9234101 , 7.67354059, 5.38781802, 3.93163276, 3.93308798,\n",
       "        6.13986437, 7.05814131, 7.24230075, 4.29467924, 4.20269307,\n",
       "        4.29121463, 6.28310402, 6.80867942, 7.43251832, 4.71969358,\n",
       "        5.02972309, 4.40504853, 6.20263624, 7.33701173, 7.64497503,\n",
       "        7.01784603, 4.90734959, 5.90564823, 7.27509793, 7.43507775,\n",
       "        7.71234854]),\n",
       " 'std_fit_time': array([0.01933183, 0.13492623, 0.35759974, 0.15493665, 0.05029474,\n",
       "        0.35734784, 0.18493977, 0.14254074, 0.06432375, 2.37828935,\n",
       "        0.51837459, 0.07669132, 0.28020669, 0.13884094, 0.06223558,\n",
       "        0.52042024, 0.06381036, 0.07739656, 0.43396981, 0.32392887,\n",
       "        0.37187422, 0.0298944 , 0.25780565, 0.21568663, 0.11810268,\n",
       "        0.53189519, 0.36163872, 0.25688472, 0.68442742, 0.41939422,\n",
       "        0.06483841, 0.42563661, 0.65638278, 0.24007044, 0.19220877,\n",
       "        0.07362995]),\n",
       " 'mean_score_time': array([0.72315303, 0.69325805, 1.16557757, 0.95439299, 0.80733132,\n",
       "        0.83624903, 1.11483502, 1.15496127, 2.21478033, 1.39467096,\n",
       "        1.2217509 , 1.09753235, 1.01032337, 1.04083323, 0.92143488,\n",
       "        1.52202082, 0.78849371, 0.9866089 , 1.22521337, 1.18134038,\n",
       "        1.19277143, 0.86904669, 0.96998127, 1.27796872, 1.49231259,\n",
       "        0.89085293, 1.15964945, 1.05561312, 1.33274158, 1.22496565,\n",
       "        0.81489301, 1.27880732, 1.31172999, 1.2344571 , 1.49640934,\n",
       "        0.72646729]),\n",
       " 'std_score_time': array([0.01549971, 0.03293911, 0.66719014, 0.10496195, 0.01593507,\n",
       "        0.24295694, 0.08187121, 0.04675622, 0.08078063, 0.37075926,\n",
       "        0.17531173, 0.25538987, 0.12100833, 0.18094364, 0.00540899,\n",
       "        0.25327999, 0.01283136, 0.40123378, 0.15480126, 0.31346479,\n",
       "        0.45674939, 0.01652266, 0.07374415, 0.43877216, 0.06805966,\n",
       "        0.17337252, 0.05293549, 0.10478115, 0.32598833, 0.0996121 ,\n",
       "        0.0679674 , 0.07575157, 0.3965527 , 0.17757182, 0.07662925,\n",
       "        0.25860131]),\n",
       " 'param_activation': masked_array(data=['linear', 'linear', 'linear', 'linear', 'linear',\n",
       "                    'linear', 'linear', 'linear', 'linear', 'linear',\n",
       "                    'linear', 'linear', 'softmax', 'softmax', 'softmax',\n",
       "                    'softmax', 'softmax', 'softmax', 'softmax', 'softmax',\n",
       "                    'softmax', 'softmax', 'softmax', 'softmax', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_batch_size': masked_array(data=[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_dropout_rate': masked_array(data=[0, 0, 0, 0, 0, 0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0, 0,\n",
       "                    0, 0, 0, 0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0, 0, 0, 0,\n",
       "                    0, 0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_epochs': masked_array(data=[20, 20, 20, 50, 50, 50, 20, 20, 20, 50, 50, 50, 20, 20,\n",
       "                    20, 50, 50, 50, 20, 20, 20, 50, 50, 50, 20, 20, 20, 50,\n",
       "                    50, 50, 20, 20, 20, 50, 50, 50],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_neurons': masked_array(data=[1, 1.5, 2, 1, 1.5, 2, 1, 1.5, 2, 1, 1.5, 2, 1, 1.5, 2,\n",
       "                    1, 1.5, 2, 1, 1.5, 2, 1, 1.5, 2, 1, 1.5, 2, 1, 1.5, 2,\n",
       "                    1, 1.5, 2, 1, 1.5, 2],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 20,\n",
       "   'neurons': 1},\n",
       "  {'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 20,\n",
       "   'neurons': 1.5},\n",
       "  {'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 20,\n",
       "   'neurons': 2},\n",
       "  {'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 50,\n",
       "   'neurons': 1},\n",
       "  {'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 50,\n",
       "   'neurons': 1.5},\n",
       "  {'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 50,\n",
       "   'neurons': 2},\n",
       "  {'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 20,\n",
       "   'neurons': 1},\n",
       "  {'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 20,\n",
       "   'neurons': 1.5},\n",
       "  {'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 20,\n",
       "   'neurons': 2},\n",
       "  {'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 50,\n",
       "   'neurons': 1},\n",
       "  {'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 50,\n",
       "   'neurons': 1.5},\n",
       "  {'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 50,\n",
       "   'neurons': 2},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 20,\n",
       "   'neurons': 1},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 20,\n",
       "   'neurons': 1.5},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 20,\n",
       "   'neurons': 2},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 50,\n",
       "   'neurons': 1},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 50,\n",
       "   'neurons': 1.5},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 50,\n",
       "   'neurons': 2},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 20,\n",
       "   'neurons': 1},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 20,\n",
       "   'neurons': 1.5},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 20,\n",
       "   'neurons': 2},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 50,\n",
       "   'neurons': 1},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 50,\n",
       "   'neurons': 1.5},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 50,\n",
       "   'neurons': 2},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 20,\n",
       "   'neurons': 1},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 20,\n",
       "   'neurons': 1.5},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 20,\n",
       "   'neurons': 2},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 50,\n",
       "   'neurons': 1},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 50,\n",
       "   'neurons': 1.5},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0,\n",
       "   'epochs': 50,\n",
       "   'neurons': 2},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 20,\n",
       "   'neurons': 1},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 20,\n",
       "   'neurons': 1.5},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 20,\n",
       "   'neurons': 2},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 50,\n",
       "   'neurons': 1},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 50,\n",
       "   'neurons': 1.5},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'dropout_rate': 0.1,\n",
       "   'epochs': 50,\n",
       "   'neurons': 2}],\n",
       " 'split0_test_score': array([0.18076285, 0.01658375, 0.21227197, 0.0066335 , 0.59701493,\n",
       "        0.36484245, 0.03482587, 0.03814262, 0.026534  , 0.14096186,\n",
       "        0.19568823, 0.00995025, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.43283582,\n",
       "        0.8225539 , 0.12271973, 0.6119403 , 0.36650083, 0.48922056,\n",
       "        0.23217247, 0.05804312, 0.07296849, 0.40298507, 0.50580431,\n",
       "        0.17412935]),\n",
       " 'split1_test_score': array([0.93200663, 0.38640133, 0.89883914, 0.55721393, 0.52404643,\n",
       "        0.92371476, 0.69651741, 0.87562189, 0.66169154, 0.45936982,\n",
       "        0.44941957, 0.83084577, 0.92537313, 0.07462687, 0.66832504,\n",
       "        0.46932007, 0.92868988, 0.07462687, 0.67330017, 0.91376451,\n",
       "        0.28855721, 0.86567164, 0.07462687, 0.67495854, 0.94693201,\n",
       "        0.93864013, 0.40630182, 0.96351575, 0.95356551, 0.85240464,\n",
       "        0.92371476, 0.94361526, 0.90878939, 0.94527363, 0.94527363,\n",
       "        0.95522388]),\n",
       " 'split2_test_score': array([0.88704319, 0.57475083, 0.97840532, 0.94850498, 0.99501661,\n",
       "        0.26245847, 0.72259136, 0.81561462, 0.20265781, 0.67940199,\n",
       "        0.34717608, 0.33388704, 0.        , 0.81893688, 1.        ,\n",
       "        0.        , 0.        , 0.41860465, 0.        , 0.09136213,\n",
       "        0.        , 0.        , 0.        , 0.18106312, 0.72425249,\n",
       "        0.78903654, 0.99003322, 0.87541528, 0.98671096, 0.97840532,\n",
       "        0.77242525, 0.9717608 , 1.        , 0.75415282, 0.8255814 ,\n",
       "        1.        ]),\n",
       " 'mean_test_score': array([0.66660423, 0.32591197, 0.69650548, 0.50411747, 0.70535932,\n",
       "        0.51700523, 0.48464488, 0.57645971, 0.29696112, 0.42657789,\n",
       "        0.33076129, 0.39156102, 0.30845771, 0.29785458, 0.55610835,\n",
       "        0.15644002, 0.30956329, 0.16441051, 0.22443339, 0.33504221,\n",
       "        0.09618574, 0.28855721, 0.02487562, 0.28534055, 0.70134011,\n",
       "        0.85007686, 0.50635159, 0.81695711, 0.76892577, 0.77334351,\n",
       "        0.64277083, 0.65780639, 0.66058596, 0.70080384, 0.75888645,\n",
       "        0.70978441]),\n",
       " 'std_test_score': array([0.34403179, 0.2318503 , 0.34394211, 0.38634604, 0.2069736 ,\n",
       "        0.29060867, 0.31824814, 0.38143517, 0.26773882, 0.22103684,\n",
       "        0.10423366, 0.33760142, 0.43622508, 0.36971824, 0.41588816,\n",
       "        0.2212396 , 0.43778861, 0.18230613, 0.31739674, 0.41091474,\n",
       "        0.13602718, 0.40808153, 0.03517944, 0.28524562, 0.21050329,\n",
       "        0.06410119, 0.36107768, 0.14936386, 0.28487895, 0.207386  ,\n",
       "        0.29683372, 0.42425231, 0.41717347, 0.22457932, 0.18550738,\n",
       "        0.37920617]),\n",
       " 'rank_test_score': array([11, 25, 10, 19,  7, 17, 20, 15, 29, 21, 24, 22, 27, 28, 16, 34, 26,\n",
       "        33, 32, 23, 35, 30, 36, 31,  8,  1, 18,  2,  4,  3, 14, 13, 12,  9,\n",
       "         5,  6], dtype=int32)}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_dropout_rate</th>\n",
       "      <th>param_epochs</th>\n",
       "      <th>param_neurons</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.634552</td>\n",
       "      <td>0.019332</td>\n",
       "      <td>0.723153</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>0.180763</td>\n",
       "      <td>0.932007</td>\n",
       "      <td>0.887043</td>\n",
       "      <td>0.666604</td>\n",
       "      <td>0.344032</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.709583</td>\n",
       "      <td>0.134926</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.032939</td>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>0.016584</td>\n",
       "      <td>0.386401</td>\n",
       "      <td>0.574751</td>\n",
       "      <td>0.325912</td>\n",
       "      <td>0.231850</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.385180</td>\n",
       "      <td>0.357600</td>\n",
       "      <td>1.165578</td>\n",
       "      <td>0.667190</td>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>0.212272</td>\n",
       "      <td>0.898839</td>\n",
       "      <td>0.978405</td>\n",
       "      <td>0.696505</td>\n",
       "      <td>0.343942</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.695404</td>\n",
       "      <td>0.154937</td>\n",
       "      <td>0.954393</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.557214</td>\n",
       "      <td>0.948505</td>\n",
       "      <td>0.504117</td>\n",
       "      <td>0.386346</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.930111</td>\n",
       "      <td>0.050295</td>\n",
       "      <td>0.807331</td>\n",
       "      <td>0.015935</td>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.524046</td>\n",
       "      <td>0.995017</td>\n",
       "      <td>0.705359</td>\n",
       "      <td>0.206974</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.231920</td>\n",
       "      <td>0.357348</td>\n",
       "      <td>0.836249</td>\n",
       "      <td>0.242957</td>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>0.364842</td>\n",
       "      <td>0.923715</td>\n",
       "      <td>0.262458</td>\n",
       "      <td>0.517005</td>\n",
       "      <td>0.290609</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.962194</td>\n",
       "      <td>0.184940</td>\n",
       "      <td>1.114835</td>\n",
       "      <td>0.081871</td>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>0.034826</td>\n",
       "      <td>0.696517</td>\n",
       "      <td>0.722591</td>\n",
       "      <td>0.484645</td>\n",
       "      <td>0.318248</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.209007</td>\n",
       "      <td>0.142541</td>\n",
       "      <td>1.154961</td>\n",
       "      <td>0.046756</td>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>0.038143</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>0.815615</td>\n",
       "      <td>0.576460</td>\n",
       "      <td>0.381435</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.570809</td>\n",
       "      <td>0.064324</td>\n",
       "      <td>2.214780</td>\n",
       "      <td>0.080781</td>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>0.026534</td>\n",
       "      <td>0.661692</td>\n",
       "      <td>0.202658</td>\n",
       "      <td>0.296961</td>\n",
       "      <td>0.267739</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.036160</td>\n",
       "      <td>2.378289</td>\n",
       "      <td>1.394671</td>\n",
       "      <td>0.370759</td>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>0.140962</td>\n",
       "      <td>0.459370</td>\n",
       "      <td>0.679402</td>\n",
       "      <td>0.426578</td>\n",
       "      <td>0.221037</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.923410</td>\n",
       "      <td>0.518375</td>\n",
       "      <td>1.221751</td>\n",
       "      <td>0.175312</td>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>0.195688</td>\n",
       "      <td>0.449420</td>\n",
       "      <td>0.347176</td>\n",
       "      <td>0.330761</td>\n",
       "      <td>0.104234</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.673541</td>\n",
       "      <td>0.076691</td>\n",
       "      <td>1.097532</td>\n",
       "      <td>0.255390</td>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.830846</td>\n",
       "      <td>0.333887</td>\n",
       "      <td>0.391561</td>\n",
       "      <td>0.337601</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.387818</td>\n",
       "      <td>0.280207</td>\n",
       "      <td>1.010323</td>\n",
       "      <td>0.121008</td>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308458</td>\n",
       "      <td>0.436225</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.931633</td>\n",
       "      <td>0.138841</td>\n",
       "      <td>1.040833</td>\n",
       "      <td>0.180944</td>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.818937</td>\n",
       "      <td>0.297855</td>\n",
       "      <td>0.369718</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.933088</td>\n",
       "      <td>0.062236</td>\n",
       "      <td>0.921435</td>\n",
       "      <td>0.005409</td>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.668325</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.556108</td>\n",
       "      <td>0.415888</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.139864</td>\n",
       "      <td>0.520420</td>\n",
       "      <td>1.522021</td>\n",
       "      <td>0.253280</td>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156440</td>\n",
       "      <td>0.221240</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.058141</td>\n",
       "      <td>0.063810</td>\n",
       "      <td>0.788494</td>\n",
       "      <td>0.012831</td>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.928690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309563</td>\n",
       "      <td>0.437789</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.242301</td>\n",
       "      <td>0.077397</td>\n",
       "      <td>0.986609</td>\n",
       "      <td>0.401234</td>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.164411</td>\n",
       "      <td>0.182306</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.294679</td>\n",
       "      <td>0.433970</td>\n",
       "      <td>1.225213</td>\n",
       "      <td>0.154801</td>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.673300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224433</td>\n",
       "      <td>0.317397</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.202693</td>\n",
       "      <td>0.323929</td>\n",
       "      <td>1.181340</td>\n",
       "      <td>0.313465</td>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.913765</td>\n",
       "      <td>0.091362</td>\n",
       "      <td>0.335042</td>\n",
       "      <td>0.410915</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.291215</td>\n",
       "      <td>0.371874</td>\n",
       "      <td>1.192771</td>\n",
       "      <td>0.456749</td>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096186</td>\n",
       "      <td>0.136027</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.283104</td>\n",
       "      <td>0.029894</td>\n",
       "      <td>0.869047</td>\n",
       "      <td>0.016523</td>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288557</td>\n",
       "      <td>0.408082</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.808679</td>\n",
       "      <td>0.257806</td>\n",
       "      <td>0.969981</td>\n",
       "      <td>0.073744</td>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>0.035179</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7.432518</td>\n",
       "      <td>0.215687</td>\n",
       "      <td>1.277969</td>\n",
       "      <td>0.438772</td>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.674959</td>\n",
       "      <td>0.181063</td>\n",
       "      <td>0.285341</td>\n",
       "      <td>0.285246</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.719694</td>\n",
       "      <td>0.118103</td>\n",
       "      <td>1.492313</td>\n",
       "      <td>0.068060</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.432836</td>\n",
       "      <td>0.946932</td>\n",
       "      <td>0.724252</td>\n",
       "      <td>0.701340</td>\n",
       "      <td>0.210503</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.029723</td>\n",
       "      <td>0.531895</td>\n",
       "      <td>0.890853</td>\n",
       "      <td>0.173373</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.822554</td>\n",
       "      <td>0.938640</td>\n",
       "      <td>0.789037</td>\n",
       "      <td>0.850077</td>\n",
       "      <td>0.064101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.405049</td>\n",
       "      <td>0.361639</td>\n",
       "      <td>1.159649</td>\n",
       "      <td>0.052935</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.122720</td>\n",
       "      <td>0.406302</td>\n",
       "      <td>0.990033</td>\n",
       "      <td>0.506352</td>\n",
       "      <td>0.361078</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.202636</td>\n",
       "      <td>0.256885</td>\n",
       "      <td>1.055613</td>\n",
       "      <td>0.104781</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>0.963516</td>\n",
       "      <td>0.875415</td>\n",
       "      <td>0.816957</td>\n",
       "      <td>0.149364</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.337012</td>\n",
       "      <td>0.684427</td>\n",
       "      <td>1.332742</td>\n",
       "      <td>0.325988</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.366501</td>\n",
       "      <td>0.953566</td>\n",
       "      <td>0.986711</td>\n",
       "      <td>0.768926</td>\n",
       "      <td>0.284879</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.644975</td>\n",
       "      <td>0.419394</td>\n",
       "      <td>1.224966</td>\n",
       "      <td>0.099612</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.489221</td>\n",
       "      <td>0.852405</td>\n",
       "      <td>0.978405</td>\n",
       "      <td>0.773344</td>\n",
       "      <td>0.207386</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7.017846</td>\n",
       "      <td>0.064838</td>\n",
       "      <td>0.814893</td>\n",
       "      <td>0.067967</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.232172</td>\n",
       "      <td>0.923715</td>\n",
       "      <td>0.772425</td>\n",
       "      <td>0.642771</td>\n",
       "      <td>0.296834</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.907350</td>\n",
       "      <td>0.425637</td>\n",
       "      <td>1.278807</td>\n",
       "      <td>0.075752</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.058043</td>\n",
       "      <td>0.943615</td>\n",
       "      <td>0.971761</td>\n",
       "      <td>0.657806</td>\n",
       "      <td>0.424252</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.905648</td>\n",
       "      <td>0.656383</td>\n",
       "      <td>1.311730</td>\n",
       "      <td>0.396553</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.072968</td>\n",
       "      <td>0.908789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660586</td>\n",
       "      <td>0.417173</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7.275098</td>\n",
       "      <td>0.240070</td>\n",
       "      <td>1.234457</td>\n",
       "      <td>0.177572</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.754153</td>\n",
       "      <td>0.700804</td>\n",
       "      <td>0.224579</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>7.435078</td>\n",
       "      <td>0.192209</td>\n",
       "      <td>1.496409</td>\n",
       "      <td>0.076629</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.505804</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.758886</td>\n",
       "      <td>0.185507</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7.712349</td>\n",
       "      <td>0.073630</td>\n",
       "      <td>0.726467</td>\n",
       "      <td>0.258601</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>0.174129</td>\n",
       "      <td>0.955224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.709784</td>\n",
       "      <td>0.379206</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        4.634552      0.019332         0.723153        0.015500   \n",
       "1        4.709583      0.134926         0.693258        0.032939   \n",
       "2        4.385180      0.357600         1.165578        0.667190   \n",
       "3        6.695404      0.154937         0.954393        0.104962   \n",
       "4        6.930111      0.050295         0.807331        0.015935   \n",
       "5        7.231920      0.357348         0.836249        0.242957   \n",
       "6        4.962194      0.184940         1.114835        0.081871   \n",
       "7        5.209007      0.142541         1.154961        0.046756   \n",
       "8        4.570809      0.064324         2.214780        0.080781   \n",
       "9        9.036160      2.378289         1.394671        0.370759   \n",
       "10       7.923410      0.518375         1.221751        0.175312   \n",
       "11       7.673541      0.076691         1.097532        0.255390   \n",
       "12       5.387818      0.280207         1.010323        0.121008   \n",
       "13       3.931633      0.138841         1.040833        0.180944   \n",
       "14       3.933088      0.062236         0.921435        0.005409   \n",
       "15       6.139864      0.520420         1.522021        0.253280   \n",
       "16       7.058141      0.063810         0.788494        0.012831   \n",
       "17       7.242301      0.077397         0.986609        0.401234   \n",
       "18       4.294679      0.433970         1.225213        0.154801   \n",
       "19       4.202693      0.323929         1.181340        0.313465   \n",
       "20       4.291215      0.371874         1.192771        0.456749   \n",
       "21       6.283104      0.029894         0.869047        0.016523   \n",
       "22       6.808679      0.257806         0.969981        0.073744   \n",
       "23       7.432518      0.215687         1.277969        0.438772   \n",
       "24       4.719694      0.118103         1.492313        0.068060   \n",
       "25       5.029723      0.531895         0.890853        0.173373   \n",
       "26       4.405049      0.361639         1.159649        0.052935   \n",
       "27       6.202636      0.256885         1.055613        0.104781   \n",
       "28       7.337012      0.684427         1.332742        0.325988   \n",
       "29       7.644975      0.419394         1.224966        0.099612   \n",
       "30       7.017846      0.064838         0.814893        0.067967   \n",
       "31       4.907350      0.425637         1.278807        0.075752   \n",
       "32       5.905648      0.656383         1.311730        0.396553   \n",
       "33       7.275098      0.240070         1.234457        0.177572   \n",
       "34       7.435078      0.192209         1.496409        0.076629   \n",
       "35       7.712349      0.073630         0.726467        0.258601   \n",
       "\n",
       "   param_activation param_batch_size param_dropout_rate param_epochs  \\\n",
       "0            linear              100                  0           20   \n",
       "1            linear              100                  0           20   \n",
       "2            linear              100                  0           20   \n",
       "3            linear              100                  0           50   \n",
       "4            linear              100                  0           50   \n",
       "5            linear              100                  0           50   \n",
       "6            linear              100                0.1           20   \n",
       "7            linear              100                0.1           20   \n",
       "8            linear              100                0.1           20   \n",
       "9            linear              100                0.1           50   \n",
       "10           linear              100                0.1           50   \n",
       "11           linear              100                0.1           50   \n",
       "12          softmax              100                  0           20   \n",
       "13          softmax              100                  0           20   \n",
       "14          softmax              100                  0           20   \n",
       "15          softmax              100                  0           50   \n",
       "16          softmax              100                  0           50   \n",
       "17          softmax              100                  0           50   \n",
       "18          softmax              100                0.1           20   \n",
       "19          softmax              100                0.1           20   \n",
       "20          softmax              100                0.1           20   \n",
       "21          softmax              100                0.1           50   \n",
       "22          softmax              100                0.1           50   \n",
       "23          softmax              100                0.1           50   \n",
       "24             relu              100                  0           20   \n",
       "25             relu              100                  0           20   \n",
       "26             relu              100                  0           20   \n",
       "27             relu              100                  0           50   \n",
       "28             relu              100                  0           50   \n",
       "29             relu              100                  0           50   \n",
       "30             relu              100                0.1           20   \n",
       "31             relu              100                0.1           20   \n",
       "32             relu              100                0.1           20   \n",
       "33             relu              100                0.1           50   \n",
       "34             relu              100                0.1           50   \n",
       "35             relu              100                0.1           50   \n",
       "\n",
       "   param_neurons                                             params  \\\n",
       "0              1  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "1            1.5  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "2              2  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "3              1  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "4            1.5  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "5              2  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "6              1  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "7            1.5  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "8              2  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "9              1  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "10           1.5  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "11             2  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "12             1  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "13           1.5  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "14             2  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "15             1  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "16           1.5  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "17             2  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "18             1  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "19           1.5  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "20             2  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "21             1  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "22           1.5  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "23             2  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "24             1  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "25           1.5  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "26             2  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "27             1  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "28           1.5  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "29             2  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "30             1  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "31           1.5  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "32             2  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "33             1  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "34           1.5  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "35             2  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0            0.180763           0.932007           0.887043         0.666604   \n",
       "1            0.016584           0.386401           0.574751         0.325912   \n",
       "2            0.212272           0.898839           0.978405         0.696505   \n",
       "3            0.006633           0.557214           0.948505         0.504117   \n",
       "4            0.597015           0.524046           0.995017         0.705359   \n",
       "5            0.364842           0.923715           0.262458         0.517005   \n",
       "6            0.034826           0.696517           0.722591         0.484645   \n",
       "7            0.038143           0.875622           0.815615         0.576460   \n",
       "8            0.026534           0.661692           0.202658         0.296961   \n",
       "9            0.140962           0.459370           0.679402         0.426578   \n",
       "10           0.195688           0.449420           0.347176         0.330761   \n",
       "11           0.009950           0.830846           0.333887         0.391561   \n",
       "12           0.000000           0.925373           0.000000         0.308458   \n",
       "13           0.000000           0.074627           0.818937         0.297855   \n",
       "14           0.000000           0.668325           1.000000         0.556108   \n",
       "15           0.000000           0.469320           0.000000         0.156440   \n",
       "16           0.000000           0.928690           0.000000         0.309563   \n",
       "17           0.000000           0.074627           0.418605         0.164411   \n",
       "18           0.000000           0.673300           0.000000         0.224433   \n",
       "19           0.000000           0.913765           0.091362         0.335042   \n",
       "20           0.000000           0.288557           0.000000         0.096186   \n",
       "21           0.000000           0.865672           0.000000         0.288557   \n",
       "22           0.000000           0.074627           0.000000         0.024876   \n",
       "23           0.000000           0.674959           0.181063         0.285341   \n",
       "24           0.432836           0.946932           0.724252         0.701340   \n",
       "25           0.822554           0.938640           0.789037         0.850077   \n",
       "26           0.122720           0.406302           0.990033         0.506352   \n",
       "27           0.611940           0.963516           0.875415         0.816957   \n",
       "28           0.366501           0.953566           0.986711         0.768926   \n",
       "29           0.489221           0.852405           0.978405         0.773344   \n",
       "30           0.232172           0.923715           0.772425         0.642771   \n",
       "31           0.058043           0.943615           0.971761         0.657806   \n",
       "32           0.072968           0.908789           1.000000         0.660586   \n",
       "33           0.402985           0.945274           0.754153         0.700804   \n",
       "34           0.505804           0.945274           0.825581         0.758886   \n",
       "35           0.174129           0.955224           1.000000         0.709784   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "0         0.344032               11  \n",
       "1         0.231850               25  \n",
       "2         0.343942               10  \n",
       "3         0.386346               19  \n",
       "4         0.206974                7  \n",
       "5         0.290609               17  \n",
       "6         0.318248               20  \n",
       "7         0.381435               15  \n",
       "8         0.267739               29  \n",
       "9         0.221037               21  \n",
       "10        0.104234               24  \n",
       "11        0.337601               22  \n",
       "12        0.436225               27  \n",
       "13        0.369718               28  \n",
       "14        0.415888               16  \n",
       "15        0.221240               34  \n",
       "16        0.437789               26  \n",
       "17        0.182306               33  \n",
       "18        0.317397               32  \n",
       "19        0.410915               23  \n",
       "20        0.136027               35  \n",
       "21        0.408082               30  \n",
       "22        0.035179               36  \n",
       "23        0.285246               31  \n",
       "24        0.210503                8  \n",
       "25        0.064101                1  \n",
       "26        0.361078               18  \n",
       "27        0.149364                2  \n",
       "28        0.284879                4  \n",
       "29        0.207386                3  \n",
       "30        0.296834               14  \n",
       "31        0.424252               13  \n",
       "32        0.417173               12  \n",
       "33        0.224579                9  \n",
       "34        0.185507                5  \n",
       "35        0.379206                6  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame.from_dict(grid_result.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "57/57 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 4ms/step\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 6ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 3ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 3ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 4ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "57/57 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "57/57 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 7ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "57/57 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "57/57 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "57/57 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyperparam</th>\n",
       "      <th>train_cf_matrix</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>train_type_1_error_FP</th>\n",
       "      <th>train_type_2_error_FN</th>\n",
       "      <th>train_log_loss</th>\n",
       "      <th>...</th>\n",
       "      <th>test_cf_matrix</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>test_type_1_error_FP</th>\n",
       "      <th>test_type_2_error_FN</th>\n",
       "      <th>test_log_loss</th>\n",
       "      <th>test_cohen_kappa_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[42, 251], [0, 21]]</td>\n",
       "      <td>0.200637</td>\n",
       "      <td>0.143345</td>\n",
       "      <td>0.077206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571672</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>27.609661</td>\n",
       "      <td>...</td>\n",
       "      <td>[[103, 545], [0, 1160]]</td>\n",
       "      <td>0.698562</td>\n",
       "      <td>0.809773</td>\n",
       "      <td>0.680352</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.579475</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>10.411543</td>\n",
       "      <td>0.195178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[104, 189], [5, 16]]</td>\n",
       "      <td>0.382166</td>\n",
       "      <td>0.141593</td>\n",
       "      <td>0.078049</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.558427</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>21.339725</td>\n",
       "      <td>...</td>\n",
       "      <td>[[282, 366], [38, 1122]]</td>\n",
       "      <td>0.776549</td>\n",
       "      <td>0.847432</td>\n",
       "      <td>0.754032</td>\n",
       "      <td>0.967241</td>\n",
       "      <td>0.701213</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>7.717897</td>\n",
       "      <td>0.453034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[239, 54], [15, 6]]</td>\n",
       "      <td>0.780255</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.550707</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>7.589869</td>\n",
       "      <td>...</td>\n",
       "      <td>[[562, 86], [441, 719]]</td>\n",
       "      <td>0.708518</td>\n",
       "      <td>0.731807</td>\n",
       "      <td>0.893168</td>\n",
       "      <td>0.619828</td>\n",
       "      <td>0.743556</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>10.067480</td>\n",
       "      <td>0.434571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[264, 29], [19, 2]]</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.498131</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>5.279887</td>\n",
       "      <td>...</td>\n",
       "      <td>[[617, 31], [753, 407]]</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>0.509387</td>\n",
       "      <td>0.929224</td>\n",
       "      <td>0.350862</td>\n",
       "      <td>0.651511</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>14.977005</td>\n",
       "      <td>0.243217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[85, 208], [3, 18]]</td>\n",
       "      <td>0.328025</td>\n",
       "      <td>0.145749</td>\n",
       "      <td>0.079646</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.573623</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>23.209707</td>\n",
       "      <td>...</td>\n",
       "      <td>[[190, 458], [10, 1150]]</td>\n",
       "      <td>0.741150</td>\n",
       "      <td>0.830925</td>\n",
       "      <td>0.715174</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.642295</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>8.940550</td>\n",
       "      <td>0.335828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[222, 71], [10, 11]]</td>\n",
       "      <td>0.742038</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.640744</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>8.909865</td>\n",
       "      <td>...</td>\n",
       "      <td>[[522, 126], [420, 740]]</td>\n",
       "      <td>0.698009</td>\n",
       "      <td>0.730503</td>\n",
       "      <td>0.854503</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.721743</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>10.430461</td>\n",
       "      <td>0.403123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[158, 135], [8, 13]]</td>\n",
       "      <td>0.544586</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.087838</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.579148</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>15.729787</td>\n",
       "      <td>...</td>\n",
       "      <td>[[389, 259], [118, 1042]]</td>\n",
       "      <td>0.791482</td>\n",
       "      <td>0.846810</td>\n",
       "      <td>0.800922</td>\n",
       "      <td>0.898276</td>\n",
       "      <td>0.749292</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>7.202061</td>\n",
       "      <td>0.523734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[167, 126], [5, 16]]</td>\n",
       "      <td>0.582803</td>\n",
       "      <td>0.196319</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.665935</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>14.409810</td>\n",
       "      <td>...</td>\n",
       "      <td>[[410, 238], [80, 1080]]</td>\n",
       "      <td>0.824115</td>\n",
       "      <td>0.871671</td>\n",
       "      <td>0.819423</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.781875</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>6.074956</td>\n",
       "      <td>0.595811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[236, 57], [12, 9]]</td>\n",
       "      <td>0.780255</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.617016</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>7.589876</td>\n",
       "      <td>...</td>\n",
       "      <td>[[555, 93], [396, 764]]</td>\n",
       "      <td>0.729535</td>\n",
       "      <td>0.757561</td>\n",
       "      <td>0.891482</td>\n",
       "      <td>0.658621</td>\n",
       "      <td>0.757551</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>9.341558</td>\n",
       "      <td>0.466919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[243, 50], [14, 7]]</td>\n",
       "      <td>0.796178</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.581342</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>7.039878</td>\n",
       "      <td>...</td>\n",
       "      <td>[[580, 68], [561, 599]]</td>\n",
       "      <td>0.652102</td>\n",
       "      <td>0.655720</td>\n",
       "      <td>0.898051</td>\n",
       "      <td>0.516379</td>\n",
       "      <td>0.705721</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.016009</td>\n",
       "      <td>0.352291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[95, 198], [4, 17]]</td>\n",
       "      <td>0.356688</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.079070</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.566878</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>22.219717</td>\n",
       "      <td>...</td>\n",
       "      <td>[[238, 410], [11, 1149]]</td>\n",
       "      <td>0.767146</td>\n",
       "      <td>0.845164</td>\n",
       "      <td>0.737011</td>\n",
       "      <td>0.990517</td>\n",
       "      <td>0.678901</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>8.042673</td>\n",
       "      <td>0.414068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[265, 28], [20, 1]]</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.476028</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>5.279884</td>\n",
       "      <td>...</td>\n",
       "      <td>[[626, 22], [873, 287]]</td>\n",
       "      <td>0.504978</td>\n",
       "      <td>0.390742</td>\n",
       "      <td>0.928803</td>\n",
       "      <td>0.247414</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>17.097468</td>\n",
       "      <td>0.165498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[11, 282], [0, 21]]</td>\n",
       "      <td>0.101911</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.518771</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>31.019619</td>\n",
       "      <td>...</td>\n",
       "      <td>[[21, 627], [0, 1160]]</td>\n",
       "      <td>0.653208</td>\n",
       "      <td>0.787241</td>\n",
       "      <td>0.649133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.516204</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>11.978050</td>\n",
       "      <td>0.041207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>32.229604</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>32.229604</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[53, 240], [7, 14]]</td>\n",
       "      <td>0.213376</td>\n",
       "      <td>0.101818</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.423777</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>27.169649</td>\n",
       "      <td>...</td>\n",
       "      <td>[[146, 502], [89, 1071]]</td>\n",
       "      <td>0.673119</td>\n",
       "      <td>0.783754</td>\n",
       "      <td>0.680865</td>\n",
       "      <td>0.923276</td>\n",
       "      <td>0.574292</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>11.290276</td>\n",
       "      <td>0.172904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>32.229604</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>32.229604</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>32.229604</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>32.229604</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>32.229604</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>32.229604</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[23, 270], [3, 18]]</td>\n",
       "      <td>0.130573</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.467821</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>30.029624</td>\n",
       "      <td>...</td>\n",
       "      <td>[[82, 566], [11, 1149]]</td>\n",
       "      <td>0.680863</td>\n",
       "      <td>0.799304</td>\n",
       "      <td>0.669971</td>\n",
       "      <td>0.990517</td>\n",
       "      <td>0.558530</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>11.022858</td>\n",
       "      <td>0.144344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[2, 291], [0, 21]]</td>\n",
       "      <td>0.073248</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>0.067308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.503413</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>32.009607</td>\n",
       "      <td>...</td>\n",
       "      <td>[[5, 643], [0, 1160]]</td>\n",
       "      <td>0.644358</td>\n",
       "      <td>0.782990</td>\n",
       "      <td>0.643372</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.503858</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.283710</td>\n",
       "      <td>0.009880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[205, 88], [13, 8]]</td>\n",
       "      <td>0.678344</td>\n",
       "      <td>0.136752</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.540306</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>11.109831</td>\n",
       "      <td>...</td>\n",
       "      <td>[[552, 96], [26, 1134]]</td>\n",
       "      <td>0.932522</td>\n",
       "      <td>0.948954</td>\n",
       "      <td>0.921951</td>\n",
       "      <td>0.977586</td>\n",
       "      <td>0.914719</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>2.330646</td>\n",
       "      <td>0.849695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[274, 19], [21, 0]]</td>\n",
       "      <td>0.872611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467577</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>4.399893</td>\n",
       "      <td>...</td>\n",
       "      <td>[[641, 7], [455, 705]]</td>\n",
       "      <td>0.744469</td>\n",
       "      <td>0.753205</td>\n",
       "      <td>0.990169</td>\n",
       "      <td>0.607759</td>\n",
       "      <td>0.798478</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>8.825730</td>\n",
       "      <td>0.517932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[106, 187], [9, 12]]</td>\n",
       "      <td>0.375796</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.466602</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>21.559712</td>\n",
       "      <td>...</td>\n",
       "      <td>[[353, 295], [0, 1160]]</td>\n",
       "      <td>0.836836</td>\n",
       "      <td>0.887189</td>\n",
       "      <td>0.797251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.772377</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>5.635606</td>\n",
       "      <td>0.605596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[233, 60], [15, 6]]</td>\n",
       "      <td>0.761146</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.540468</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>8.249861</td>\n",
       "      <td>...</td>\n",
       "      <td>[[595, 53], [32, 1128]]</td>\n",
       "      <td>0.952987</td>\n",
       "      <td>0.963691</td>\n",
       "      <td>0.955123</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.945312</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>1.623804</td>\n",
       "      <td>0.897039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[180, 113], [12, 9]]</td>\n",
       "      <td>0.601911</td>\n",
       "      <td>0.125874</td>\n",
       "      <td>0.073770</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.521453</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>13.749801</td>\n",
       "      <td>...</td>\n",
       "      <td>[[520, 128], [0, 1160]]</td>\n",
       "      <td>0.929204</td>\n",
       "      <td>0.947712</td>\n",
       "      <td>0.900621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901235</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>2.445280</td>\n",
       "      <td>0.839046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[186, 107], [12, 9]]</td>\n",
       "      <td>0.621019</td>\n",
       "      <td>0.131387</td>\n",
       "      <td>0.077586</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.531692</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>13.089809</td>\n",
       "      <td>...</td>\n",
       "      <td>[[497, 151], [23, 1137]]</td>\n",
       "      <td>0.903761</td>\n",
       "      <td>0.928922</td>\n",
       "      <td>0.882764</td>\n",
       "      <td>0.980172</td>\n",
       "      <td>0.873574</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>3.324042</td>\n",
       "      <td>0.781203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[223, 70], [14, 7]]</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.547213</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>9.239851</td>\n",
       "      <td>...</td>\n",
       "      <td>[[586, 62], [30, 1130]]</td>\n",
       "      <td>0.949115</td>\n",
       "      <td>0.960884</td>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>0.939229</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>1.757532</td>\n",
       "      <td>0.888138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[260, 33], [18, 3]]</td>\n",
       "      <td>0.837580</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.515115</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>5.609885</td>\n",
       "      <td>...</td>\n",
       "      <td>[[627, 21], [288, 872]]</td>\n",
       "      <td>0.829093</td>\n",
       "      <td>0.849489</td>\n",
       "      <td>0.976484</td>\n",
       "      <td>0.751724</td>\n",
       "      <td>0.859658</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>5.902931</td>\n",
       "      <td>0.659360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[217, 76], [15, 6]]</td>\n",
       "      <td>0.710191</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.513164</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>10.009839</td>\n",
       "      <td>...</td>\n",
       "      <td>[[585, 63], [18, 1142]]</td>\n",
       "      <td>0.955199</td>\n",
       "      <td>0.965751</td>\n",
       "      <td>0.947718</td>\n",
       "      <td>0.984483</td>\n",
       "      <td>0.943630</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>1.547396</td>\n",
       "      <td>0.901070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[236, 57], [13, 8]]</td>\n",
       "      <td>0.777070</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.593207</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>7.699872</td>\n",
       "      <td>...</td>\n",
       "      <td>[[600, 48], [61, 1099]]</td>\n",
       "      <td>0.939712</td>\n",
       "      <td>0.952752</td>\n",
       "      <td>0.958152</td>\n",
       "      <td>0.947414</td>\n",
       "      <td>0.936670</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>2.082282</td>\n",
       "      <td>0.869490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[219, 74], [13, 8]]</td>\n",
       "      <td>0.722930</td>\n",
       "      <td>0.155340</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.564196</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>9.569849</td>\n",
       "      <td>...</td>\n",
       "      <td>[[574, 74], [48, 1112]]</td>\n",
       "      <td>0.932522</td>\n",
       "      <td>0.947997</td>\n",
       "      <td>0.937605</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.922212</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>2.330636</td>\n",
       "      <td>0.851967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[260, 33], [17, 4]]</td>\n",
       "      <td>0.840764</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.538924</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>5.499889</td>\n",
       "      <td>...</td>\n",
       "      <td>[[626, 22], [246, 914]]</td>\n",
       "      <td>0.851770</td>\n",
       "      <td>0.872137</td>\n",
       "      <td>0.976496</td>\n",
       "      <td>0.787931</td>\n",
       "      <td>0.876990</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>5.119696</td>\n",
       "      <td>0.700538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           hyperparam        train_cf_matrix  \\\n",
       "0   {'activation': 'linear', 'batch_size': 100, 'd...   [[42, 251], [0, 21]]   \n",
       "1   {'activation': 'linear', 'batch_size': 100, 'd...  [[104, 189], [5, 16]]   \n",
       "2   {'activation': 'linear', 'batch_size': 100, 'd...   [[239, 54], [15, 6]]   \n",
       "3   {'activation': 'linear', 'batch_size': 100, 'd...   [[264, 29], [19, 2]]   \n",
       "4   {'activation': 'linear', 'batch_size': 100, 'd...   [[85, 208], [3, 18]]   \n",
       "5   {'activation': 'linear', 'batch_size': 100, 'd...  [[222, 71], [10, 11]]   \n",
       "6   {'activation': 'linear', 'batch_size': 100, 'd...  [[158, 135], [8, 13]]   \n",
       "7   {'activation': 'linear', 'batch_size': 100, 'd...  [[167, 126], [5, 16]]   \n",
       "8   {'activation': 'linear', 'batch_size': 100, 'd...   [[236, 57], [12, 9]]   \n",
       "9   {'activation': 'linear', 'batch_size': 100, 'd...   [[243, 50], [14, 7]]   \n",
       "10  {'activation': 'linear', 'batch_size': 100, 'd...   [[95, 198], [4, 17]]   \n",
       "11  {'activation': 'linear', 'batch_size': 100, 'd...   [[265, 28], [20, 1]]   \n",
       "12  {'activation': 'softmax', 'batch_size': 100, '...   [[11, 282], [0, 21]]   \n",
       "13  {'activation': 'softmax', 'batch_size': 100, '...    [[0, 293], [0, 21]]   \n",
       "14  {'activation': 'softmax', 'batch_size': 100, '...    [[0, 293], [0, 21]]   \n",
       "15  {'activation': 'softmax', 'batch_size': 100, '...   [[53, 240], [7, 14]]   \n",
       "16  {'activation': 'softmax', 'batch_size': 100, '...    [[0, 293], [0, 21]]   \n",
       "17  {'activation': 'softmax', 'batch_size': 100, '...    [[0, 293], [0, 21]]   \n",
       "18  {'activation': 'softmax', 'batch_size': 100, '...    [[0, 293], [0, 21]]   \n",
       "19  {'activation': 'softmax', 'batch_size': 100, '...    [[0, 293], [0, 21]]   \n",
       "20  {'activation': 'softmax', 'batch_size': 100, '...    [[0, 293], [0, 21]]   \n",
       "21  {'activation': 'softmax', 'batch_size': 100, '...    [[0, 293], [0, 21]]   \n",
       "22  {'activation': 'softmax', 'batch_size': 100, '...   [[23, 270], [3, 18]]   \n",
       "23  {'activation': 'softmax', 'batch_size': 100, '...    [[2, 291], [0, 21]]   \n",
       "24  {'activation': 'relu', 'batch_size': 100, 'dro...   [[205, 88], [13, 8]]   \n",
       "25  {'activation': 'relu', 'batch_size': 100, 'dro...   [[274, 19], [21, 0]]   \n",
       "26  {'activation': 'relu', 'batch_size': 100, 'dro...  [[106, 187], [9, 12]]   \n",
       "27  {'activation': 'relu', 'batch_size': 100, 'dro...   [[233, 60], [15, 6]]   \n",
       "28  {'activation': 'relu', 'batch_size': 100, 'dro...  [[180, 113], [12, 9]]   \n",
       "29  {'activation': 'relu', 'batch_size': 100, 'dro...  [[186, 107], [12, 9]]   \n",
       "30  {'activation': 'relu', 'batch_size': 100, 'dro...   [[223, 70], [14, 7]]   \n",
       "31  {'activation': 'relu', 'batch_size': 100, 'dro...   [[260, 33], [18, 3]]   \n",
       "32  {'activation': 'relu', 'batch_size': 100, 'dro...   [[217, 76], [15, 6]]   \n",
       "33  {'activation': 'relu', 'batch_size': 100, 'dro...   [[236, 57], [13, 8]]   \n",
       "34  {'activation': 'relu', 'batch_size': 100, 'dro...   [[219, 74], [13, 8]]   \n",
       "35  {'activation': 'relu', 'batch_size': 100, 'dro...   [[260, 33], [17, 4]]   \n",
       "\n",
       "    train_accuracy  train_f1  train_precision  train_recall  train_auc  \\\n",
       "0         0.200637  0.143345         0.077206      1.000000   0.571672   \n",
       "1         0.382166  0.141593         0.078049      0.761905   0.558427   \n",
       "2         0.780255  0.148148         0.100000      0.285714   0.550707   \n",
       "3         0.847134  0.076923         0.064516      0.095238   0.498131   \n",
       "4         0.328025  0.145749         0.079646      0.857143   0.573623   \n",
       "5         0.742038  0.213592         0.134146      0.523810   0.640744   \n",
       "6         0.544586  0.153846         0.087838      0.619048   0.579148   \n",
       "7         0.582803  0.196319         0.112676      0.761905   0.665935   \n",
       "8         0.780255  0.206897         0.136364      0.428571   0.617016   \n",
       "9         0.796178  0.179487         0.122807      0.333333   0.581342   \n",
       "10        0.356688  0.144068         0.079070      0.809524   0.566878   \n",
       "11        0.847134  0.040000         0.034483      0.047619   0.476028   \n",
       "12        0.101911  0.129630         0.069307      1.000000   0.518771   \n",
       "13        0.066879  0.125373         0.066879      1.000000   0.500000   \n",
       "14        0.066879  0.125373         0.066879      1.000000   0.500000   \n",
       "15        0.213376  0.101818         0.055118      0.666667   0.423777   \n",
       "16        0.066879  0.125373         0.066879      1.000000   0.500000   \n",
       "17        0.066879  0.125373         0.066879      1.000000   0.500000   \n",
       "18        0.066879  0.125373         0.066879      1.000000   0.500000   \n",
       "19        0.066879  0.125373         0.066879      1.000000   0.500000   \n",
       "20        0.066879  0.125373         0.066879      1.000000   0.500000   \n",
       "21        0.066879  0.125373         0.066879      1.000000   0.500000   \n",
       "22        0.130573  0.116505         0.062500      0.857143   0.467821   \n",
       "23        0.073248  0.126126         0.067308      1.000000   0.503413   \n",
       "24        0.678344  0.136752         0.083333      0.380952   0.540306   \n",
       "25        0.872611  0.000000         0.000000      0.000000   0.467577   \n",
       "26        0.375796  0.109091         0.060302      0.571429   0.466602   \n",
       "27        0.761146  0.137931         0.090909      0.285714   0.540468   \n",
       "28        0.601911  0.125874         0.073770      0.428571   0.521453   \n",
       "29        0.621019  0.131387         0.077586      0.428571   0.531692   \n",
       "30        0.732484  0.142857         0.090909      0.333333   0.547213   \n",
       "31        0.837580  0.105263         0.083333      0.142857   0.515115   \n",
       "32        0.710191  0.116505         0.073171      0.285714   0.513164   \n",
       "33        0.777070  0.186047         0.123077      0.380952   0.593207   \n",
       "34        0.722930  0.155340         0.097561      0.380952   0.564196   \n",
       "35        0.840764  0.137931         0.108108      0.190476   0.538924   \n",
       "\n",
       "    train_type_1_error_FP  train_type_2_error_FN  train_log_loss  ...  \\\n",
       "0                      12                    110       27.609661  ...   \n",
       "1                      12                    110       21.339725  ...   \n",
       "2                      12                    110        7.589869  ...   \n",
       "3                      12                    110        5.279887  ...   \n",
       "4                      12                    110       23.209707  ...   \n",
       "5                      12                    110        8.909865  ...   \n",
       "6                      12                    110       15.729787  ...   \n",
       "7                      12                    110       14.409810  ...   \n",
       "8                      12                    110        7.589876  ...   \n",
       "9                      12                    110        7.039878  ...   \n",
       "10                     12                    110       22.219717  ...   \n",
       "11                     12                    110        5.279884  ...   \n",
       "12                     12                    110       31.019619  ...   \n",
       "13                     12                    110       32.229604  ...   \n",
       "14                     12                    110       32.229604  ...   \n",
       "15                     12                    110       27.169649  ...   \n",
       "16                     12                    110       32.229604  ...   \n",
       "17                     12                    110       32.229604  ...   \n",
       "18                     12                    110       32.229604  ...   \n",
       "19                     12                    110       32.229604  ...   \n",
       "20                     12                    110       32.229604  ...   \n",
       "21                     12                    110       32.229604  ...   \n",
       "22                     12                    110       30.029624  ...   \n",
       "23                     12                    110       32.009607  ...   \n",
       "24                     12                    110       11.109831  ...   \n",
       "25                     12                    110        4.399893  ...   \n",
       "26                     12                    110       21.559712  ...   \n",
       "27                     12                    110        8.249861  ...   \n",
       "28                     12                    110       13.749801  ...   \n",
       "29                     12                    110       13.089809  ...   \n",
       "30                     12                    110        9.239851  ...   \n",
       "31                     12                    110        5.609885  ...   \n",
       "32                     12                    110       10.009839  ...   \n",
       "33                     12                    110        7.699872  ...   \n",
       "34                     12                    110        9.569849  ...   \n",
       "35                     12                    110        5.499889  ...   \n",
       "\n",
       "               test_cf_matrix test_accuracy   test_f1  test_precision  \\\n",
       "0     [[103, 545], [0, 1160]]      0.698562  0.809773        0.680352   \n",
       "1    [[282, 366], [38, 1122]]      0.776549  0.847432        0.754032   \n",
       "2     [[562, 86], [441, 719]]      0.708518  0.731807        0.893168   \n",
       "3     [[617, 31], [753, 407]]      0.566372  0.509387        0.929224   \n",
       "4    [[190, 458], [10, 1150]]      0.741150  0.830925        0.715174   \n",
       "5    [[522, 126], [420, 740]]      0.698009  0.730503        0.854503   \n",
       "6   [[389, 259], [118, 1042]]      0.791482  0.846810        0.800922   \n",
       "7    [[410, 238], [80, 1080]]      0.824115  0.871671        0.819423   \n",
       "8     [[555, 93], [396, 764]]      0.729535  0.757561        0.891482   \n",
       "9     [[580, 68], [561, 599]]      0.652102  0.655720        0.898051   \n",
       "10   [[238, 410], [11, 1149]]      0.767146  0.845164        0.737011   \n",
       "11    [[626, 22], [873, 287]]      0.504978  0.390742        0.928803   \n",
       "12     [[21, 627], [0, 1160]]      0.653208  0.787241        0.649133   \n",
       "13      [[0, 648], [0, 1160]]      0.641593  0.781671        0.641593   \n",
       "14      [[0, 648], [0, 1160]]      0.641593  0.781671        0.641593   \n",
       "15   [[146, 502], [89, 1071]]      0.673119  0.783754        0.680865   \n",
       "16      [[0, 648], [0, 1160]]      0.641593  0.781671        0.641593   \n",
       "17      [[0, 648], [0, 1160]]      0.641593  0.781671        0.641593   \n",
       "18      [[0, 648], [0, 1160]]      0.641593  0.781671        0.641593   \n",
       "19      [[0, 648], [0, 1160]]      0.641593  0.781671        0.641593   \n",
       "20      [[0, 648], [0, 1160]]      0.641593  0.781671        0.641593   \n",
       "21      [[0, 648], [0, 1160]]      0.641593  0.781671        0.641593   \n",
       "22    [[82, 566], [11, 1149]]      0.680863  0.799304        0.669971   \n",
       "23      [[5, 643], [0, 1160]]      0.644358  0.782990        0.643372   \n",
       "24    [[552, 96], [26, 1134]]      0.932522  0.948954        0.921951   \n",
       "25     [[641, 7], [455, 705]]      0.744469  0.753205        0.990169   \n",
       "26    [[353, 295], [0, 1160]]      0.836836  0.887189        0.797251   \n",
       "27    [[595, 53], [32, 1128]]      0.952987  0.963691        0.955123   \n",
       "28    [[520, 128], [0, 1160]]      0.929204  0.947712        0.900621   \n",
       "29   [[497, 151], [23, 1137]]      0.903761  0.928922        0.882764   \n",
       "30    [[586, 62], [30, 1130]]      0.949115  0.960884        0.947987   \n",
       "31    [[627, 21], [288, 872]]      0.829093  0.849489        0.976484   \n",
       "32    [[585, 63], [18, 1142]]      0.955199  0.965751        0.947718   \n",
       "33    [[600, 48], [61, 1099]]      0.939712  0.952752        0.958152   \n",
       "34    [[574, 74], [48, 1112]]      0.932522  0.947997        0.937605   \n",
       "35    [[626, 22], [246, 914]]      0.851770  0.872137        0.976496   \n",
       "\n",
       "    test_recall  test_auc  test_type_1_error_FP  test_type_2_error_FN  \\\n",
       "0      1.000000  0.579475                    12                   110   \n",
       "1      0.967241  0.701213                    12                   110   \n",
       "2      0.619828  0.743556                    12                   110   \n",
       "3      0.350862  0.651511                    12                   110   \n",
       "4      0.991379  0.642295                    12                   110   \n",
       "5      0.637931  0.721743                    12                   110   \n",
       "6      0.898276  0.749292                    12                   110   \n",
       "7      0.931034  0.781875                    12                   110   \n",
       "8      0.658621  0.757551                    12                   110   \n",
       "9      0.516379  0.705721                    12                   110   \n",
       "10     0.990517  0.678901                    12                   110   \n",
       "11     0.247414  0.606732                    12                   110   \n",
       "12     1.000000  0.516204                    12                   110   \n",
       "13     1.000000  0.500000                    12                   110   \n",
       "14     1.000000  0.500000                    12                   110   \n",
       "15     0.923276  0.574292                    12                   110   \n",
       "16     1.000000  0.500000                    12                   110   \n",
       "17     1.000000  0.500000                    12                   110   \n",
       "18     1.000000  0.500000                    12                   110   \n",
       "19     1.000000  0.500000                    12                   110   \n",
       "20     1.000000  0.500000                    12                   110   \n",
       "21     1.000000  0.500000                    12                   110   \n",
       "22     0.990517  0.558530                    12                   110   \n",
       "23     1.000000  0.503858                    12                   110   \n",
       "24     0.977586  0.914719                    12                   110   \n",
       "25     0.607759  0.798478                    12                   110   \n",
       "26     1.000000  0.772377                    12                   110   \n",
       "27     0.972414  0.945312                    12                   110   \n",
       "28     1.000000  0.901235                    12                   110   \n",
       "29     0.980172  0.873574                    12                   110   \n",
       "30     0.974138  0.939229                    12                   110   \n",
       "31     0.751724  0.859658                    12                   110   \n",
       "32     0.984483  0.943630                    12                   110   \n",
       "33     0.947414  0.936670                    12                   110   \n",
       "34     0.958621  0.922212                    12                   110   \n",
       "35     0.787931  0.876990                    12                   110   \n",
       "\n",
       "    test_log_loss  test_cohen_kappa_score  \n",
       "0       10.411543                0.195178  \n",
       "1        7.717897                0.453034  \n",
       "2       10.067480                0.434571  \n",
       "3       14.977005                0.243217  \n",
       "4        8.940550                0.335828  \n",
       "5       10.430461                0.403123  \n",
       "6        7.202061                0.523734  \n",
       "7        6.074956                0.595811  \n",
       "8        9.341558                0.466919  \n",
       "9       12.016009                0.352291  \n",
       "10       8.042673                0.414068  \n",
       "11      17.097468                0.165498  \n",
       "12      11.978050                0.041207  \n",
       "13      12.379229                0.000000  \n",
       "14      12.379229                0.000000  \n",
       "15      11.290276                0.172904  \n",
       "16      12.379229                0.000000  \n",
       "17      12.379229                0.000000  \n",
       "18      12.379229                0.000000  \n",
       "19      12.379229                0.000000  \n",
       "20      12.379229                0.000000  \n",
       "21      12.379229                0.000000  \n",
       "22      11.022858                0.144344  \n",
       "23      12.283710                0.009880  \n",
       "24       2.330646                0.849695  \n",
       "25       8.825730                0.517932  \n",
       "26       5.635606                0.605596  \n",
       "27       1.623804                0.897039  \n",
       "28       2.445280                0.839046  \n",
       "29       3.324042                0.781203  \n",
       "30       1.757532                0.888138  \n",
       "31       5.902931                0.659360  \n",
       "32       1.547396                0.901070  \n",
       "33       2.082282                0.869490  \n",
       "34       2.330636                0.851967  \n",
       "35       5.119696                0.700538  \n",
       "\n",
       "[36 rows x 21 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the best param to apply for test set\n",
    "\n",
    "list_of_params = grid_result.cv_results_['params']\n",
    "dict_scores = {}\n",
    "train_cf_matrix=[]\n",
    "train_accuracy=[]\n",
    "train_f1=[]\n",
    "train_precision=[]\n",
    "train_recall=[]\n",
    "train_auc=[]\n",
    "train_type_1_error_FP=[]\n",
    "train_type_2_error_FN=[]\n",
    "train_log_loss=[]\n",
    "train_cohen_kappa_score=[]\n",
    "\n",
    "test_cf_matrix=[]\n",
    "test_accuracy=[]\n",
    "test_f1=[]\n",
    "test_precision=[]\n",
    "test_recall=[]\n",
    "test_auc=[]\n",
    "test_type_1_error_FP=[]\n",
    "test_type_2_error_FN=[]\n",
    "test_log_loss=[]\n",
    "test_cohen_kappa_score=[]\n",
    "\n",
    "hyperparam = []\n",
    "\n",
    "for p in list_of_params:\n",
    "    real_model = create_model_NN(**p)\n",
    "    real_model.fit(X_train,y_train_c, epochs= grid_result.best_params_['epochs'], batch_size = grid_result.best_params_['batch_size'], verbose=0)   \n",
    "\n",
    "    hyperparam.append(p)\n",
    "\n",
    "\n",
    "    #create predicted y\n",
    "    y_pred = real_model.predict(X_train)\n",
    "    #Converting y-predicted to labels\n",
    "    pred = list()\n",
    "    for i in range(len(y_pred)):\n",
    "        pred.append(np.argmax(y_pred[i]))\n",
    "\n",
    "    #Converting y-test to labels\n",
    "    test = list()\n",
    "    for i in range(len(y_train_c)):\n",
    "        test.append(np.argmax(y_train_c[i]))\n",
    "\n",
    "    train_cf_matrix.append(confusion_matrix(test, pred))\n",
    "    train_accuracy.append(accuracy_score(test, pred)) #these are library from scikit learn, this yield the same result as the one of keras mode.evaluate()\n",
    "    train_f1.append(f1_score(test, pred)) \n",
    "    train_precision.append(precision_score(test, pred))\n",
    "    train_recall.append(recall_score(test, pred))\n",
    "    train_auc.append(roc_auc_score(test, pred))\n",
    "    train_type_1_error_FP.append(cf_matrix[1][0])\n",
    "    train_type_2_error_FN.append(cf_matrix[0][1])\n",
    "    train_log_loss.append(log_loss(test, pred))\n",
    "    train_cohen_kappa_score.append(cohen_kappa_score(test, pred))\n",
    "\n",
    "\n",
    "    #create predicted y\n",
    "    y_pred = real_model.predict(X_test)\n",
    "    #Converting y-predicted to labels\n",
    "    pred = list()\n",
    "    for i in range(len(y_pred)):\n",
    "        pred.append(np.argmax(y_pred[i]))\n",
    "\n",
    "    #Converting y-test to labels\n",
    "    test = list()\n",
    "    for i in range(len(y_test_c)):\n",
    "        test.append(np.argmax(y_test_c[i]))\n",
    "        \n",
    "    test_cf_matrix.append(confusion_matrix(test, pred))\n",
    "    test_accuracy.append(accuracy_score(test, pred)) #these are library from scikit learn, this yield the same result as the one of keras mode.evaluate()\n",
    "    test_f1.append(f1_score(test, pred)) \n",
    "    test_precision.append(precision_score(test, pred))\n",
    "    test_recall.append(recall_score(test, pred))\n",
    "    test_auc.append(roc_auc_score(test, pred))\n",
    "    test_type_1_error_FP.append(cf_matrix[1][0])\n",
    "    test_type_2_error_FN.append(cf_matrix[0][1])\n",
    "    test_log_loss.append(log_loss(test, pred))\n",
    "    test_cohen_kappa_score.append(cohen_kappa_score(test, pred))\n",
    "\n",
    "\n",
    "#create a dict of list\n",
    "dict_scores['hyperparam']=hyperparam\n",
    "dict_scores['train_cf_matrix']=train_cf_matrix\n",
    "dict_scores['train_accuracy']=train_accuracy\n",
    "dict_scores['train_f1']=train_f1\n",
    "dict_scores['train_precision']=train_precision\n",
    "dict_scores['train_recall']=train_recall\n",
    "dict_scores['train_auc']=train_auc\n",
    "dict_scores['train_type_1_error_FP']=train_type_1_error_FP\n",
    "dict_scores['train_type_2_error_FN']=train_type_2_error_FN\n",
    "dict_scores['train_log_loss']=train_log_loss\n",
    "dict_scores['train_cohen_kappa_score']=train_cohen_kappa_score\n",
    "dict_scores['test_cf_matrix']=test_cf_matrix\n",
    "dict_scores['test_accuracy']=test_accuracy\n",
    "dict_scores['test_f1']=test_f1\n",
    "dict_scores['test_precision']=test_precision\n",
    "dict_scores['test_recall']=test_recall\n",
    "dict_scores['test_auc']=test_auc\n",
    "dict_scores['test_type_1_error_FP']=test_type_1_error_FP\n",
    "dict_scores['test_type_2_error_FN']=test_type_2_error_FN\n",
    "dict_scores['test_log_loss']=test_log_loss\n",
    "dict_scores['test_cohen_kappa_score']=test_cohen_kappa_score\n",
    "\n",
    "\n",
    "df_scores = pd.DataFrame.from_dict(dict_scores)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>hyperparam</th>\n",
       "      <th>train_cf_matrix</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>train_type_1_error_FP</th>\n",
       "      <th>...</th>\n",
       "      <th>test_cf_matrix</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>test_type_1_error_FP</th>\n",
       "      <th>test_type_2_error_FN</th>\n",
       "      <th>test_log_loss</th>\n",
       "      <th>test_cohen_kappa_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[274, 19], [21, 0]]</td>\n",
       "      <td>0.872611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467577</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[641, 7], [455, 705]]</td>\n",
       "      <td>0.744469</td>\n",
       "      <td>0.753205</td>\n",
       "      <td>0.990169</td>\n",
       "      <td>0.607759</td>\n",
       "      <td>0.798478</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>8.825730</td>\n",
       "      <td>0.517932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[265, 28], [20, 1]]</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.476028</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[626, 22], [873, 287]]</td>\n",
       "      <td>0.504978</td>\n",
       "      <td>0.390742</td>\n",
       "      <td>0.928803</td>\n",
       "      <td>0.247414</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>17.097468</td>\n",
       "      <td>0.165498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[264, 29], [19, 2]]</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.498131</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[617, 31], [753, 407]]</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>0.509387</td>\n",
       "      <td>0.929224</td>\n",
       "      <td>0.350862</td>\n",
       "      <td>0.651511</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>14.977005</td>\n",
       "      <td>0.243217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[53, 240], [7, 14]]</td>\n",
       "      <td>0.213376</td>\n",
       "      <td>0.101818</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.423777</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[146, 502], [89, 1071]]</td>\n",
       "      <td>0.673119</td>\n",
       "      <td>0.783754</td>\n",
       "      <td>0.680865</td>\n",
       "      <td>0.923276</td>\n",
       "      <td>0.574292</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>11.290276</td>\n",
       "      <td>0.172904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[260, 33], [18, 3]]</td>\n",
       "      <td>0.837580</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.515115</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[627, 21], [288, 872]]</td>\n",
       "      <td>0.829093</td>\n",
       "      <td>0.849489</td>\n",
       "      <td>0.976484</td>\n",
       "      <td>0.751724</td>\n",
       "      <td>0.859658</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>5.902931</td>\n",
       "      <td>0.659360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[106, 187], [9, 12]]</td>\n",
       "      <td>0.375796</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.466602</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[353, 295], [0, 1160]]</td>\n",
       "      <td>0.836836</td>\n",
       "      <td>0.887189</td>\n",
       "      <td>0.797251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.772377</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>5.635606</td>\n",
       "      <td>0.605596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[217, 76], [15, 6]]</td>\n",
       "      <td>0.710191</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.513164</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[585, 63], [18, 1142]]</td>\n",
       "      <td>0.955199</td>\n",
       "      <td>0.965751</td>\n",
       "      <td>0.947718</td>\n",
       "      <td>0.984483</td>\n",
       "      <td>0.943630</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>1.547396</td>\n",
       "      <td>0.901070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[23, 270], [3, 18]]</td>\n",
       "      <td>0.130573</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.467821</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[82, 566], [11, 1149]]</td>\n",
       "      <td>0.680863</td>\n",
       "      <td>0.799304</td>\n",
       "      <td>0.669971</td>\n",
       "      <td>0.990517</td>\n",
       "      <td>0.558530</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>11.022858</td>\n",
       "      <td>0.144344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[0, 293], [0, 21]]</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.066879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0, 648], [0, 1160]]</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>0.781671</td>\n",
       "      <td>0.641593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.379229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[180, 113], [12, 9]]</td>\n",
       "      <td>0.601911</td>\n",
       "      <td>0.125874</td>\n",
       "      <td>0.073770</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.521453</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[520, 128], [0, 1160]]</td>\n",
       "      <td>0.929204</td>\n",
       "      <td>0.947712</td>\n",
       "      <td>0.900621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901235</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>2.445280</td>\n",
       "      <td>0.839046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[2, 291], [0, 21]]</td>\n",
       "      <td>0.073248</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>0.067308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.503413</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[5, 643], [0, 1160]]</td>\n",
       "      <td>0.644358</td>\n",
       "      <td>0.782990</td>\n",
       "      <td>0.643372</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.503858</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.283710</td>\n",
       "      <td>0.009880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>{'activation': 'softmax', 'batch_size': 100, '...</td>\n",
       "      <td>[[11, 282], [0, 21]]</td>\n",
       "      <td>0.101911</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.518771</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[21, 627], [0, 1160]]</td>\n",
       "      <td>0.653208</td>\n",
       "      <td>0.787241</td>\n",
       "      <td>0.649133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.516204</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>11.978050</td>\n",
       "      <td>0.041207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[186, 107], [12, 9]]</td>\n",
       "      <td>0.621019</td>\n",
       "      <td>0.131387</td>\n",
       "      <td>0.077586</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.531692</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[497, 151], [23, 1137]]</td>\n",
       "      <td>0.903761</td>\n",
       "      <td>0.928922</td>\n",
       "      <td>0.882764</td>\n",
       "      <td>0.980172</td>\n",
       "      <td>0.873574</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>3.324042</td>\n",
       "      <td>0.781203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[205, 88], [13, 8]]</td>\n",
       "      <td>0.678344</td>\n",
       "      <td>0.136752</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.540306</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[552, 96], [26, 1134]]</td>\n",
       "      <td>0.932522</td>\n",
       "      <td>0.948954</td>\n",
       "      <td>0.921951</td>\n",
       "      <td>0.977586</td>\n",
       "      <td>0.914719</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>2.330646</td>\n",
       "      <td>0.849695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[233, 60], [15, 6]]</td>\n",
       "      <td>0.761146</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.540468</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[595, 53], [32, 1128]]</td>\n",
       "      <td>0.952987</td>\n",
       "      <td>0.963691</td>\n",
       "      <td>0.955123</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.945312</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>1.623804</td>\n",
       "      <td>0.897039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[260, 33], [17, 4]]</td>\n",
       "      <td>0.840764</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.538924</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[626, 22], [246, 914]]</td>\n",
       "      <td>0.851770</td>\n",
       "      <td>0.872137</td>\n",
       "      <td>0.976496</td>\n",
       "      <td>0.787931</td>\n",
       "      <td>0.876990</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>5.119696</td>\n",
       "      <td>0.700538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[104, 189], [5, 16]]</td>\n",
       "      <td>0.382166</td>\n",
       "      <td>0.141593</td>\n",
       "      <td>0.078049</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.558427</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[282, 366], [38, 1122]]</td>\n",
       "      <td>0.776549</td>\n",
       "      <td>0.847432</td>\n",
       "      <td>0.754032</td>\n",
       "      <td>0.967241</td>\n",
       "      <td>0.701213</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>7.717897</td>\n",
       "      <td>0.453034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[223, 70], [14, 7]]</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.547213</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[586, 62], [30, 1130]]</td>\n",
       "      <td>0.949115</td>\n",
       "      <td>0.960884</td>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>0.939229</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>1.757532</td>\n",
       "      <td>0.888138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[42, 251], [0, 21]]</td>\n",
       "      <td>0.200637</td>\n",
       "      <td>0.143345</td>\n",
       "      <td>0.077206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571672</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[103, 545], [0, 1160]]</td>\n",
       "      <td>0.698562</td>\n",
       "      <td>0.809773</td>\n",
       "      <td>0.680352</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.579475</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>10.411543</td>\n",
       "      <td>0.195178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[95, 198], [4, 17]]</td>\n",
       "      <td>0.356688</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.079070</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.566878</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[238, 410], [11, 1149]]</td>\n",
       "      <td>0.767146</td>\n",
       "      <td>0.845164</td>\n",
       "      <td>0.737011</td>\n",
       "      <td>0.990517</td>\n",
       "      <td>0.678901</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>8.042673</td>\n",
       "      <td>0.414068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[85, 208], [3, 18]]</td>\n",
       "      <td>0.328025</td>\n",
       "      <td>0.145749</td>\n",
       "      <td>0.079646</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.573623</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[190, 458], [10, 1150]]</td>\n",
       "      <td>0.741150</td>\n",
       "      <td>0.830925</td>\n",
       "      <td>0.715174</td>\n",
       "      <td>0.991379</td>\n",
       "      <td>0.642295</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>8.940550</td>\n",
       "      <td>0.335828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[239, 54], [15, 6]]</td>\n",
       "      <td>0.780255</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.550707</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[562, 86], [441, 719]]</td>\n",
       "      <td>0.708518</td>\n",
       "      <td>0.731807</td>\n",
       "      <td>0.893168</td>\n",
       "      <td>0.619828</td>\n",
       "      <td>0.743556</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>10.067480</td>\n",
       "      <td>0.434571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[158, 135], [8, 13]]</td>\n",
       "      <td>0.544586</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.087838</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.579148</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[389, 259], [118, 1042]]</td>\n",
       "      <td>0.791482</td>\n",
       "      <td>0.846810</td>\n",
       "      <td>0.800922</td>\n",
       "      <td>0.898276</td>\n",
       "      <td>0.749292</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>7.202061</td>\n",
       "      <td>0.523734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[219, 74], [13, 8]]</td>\n",
       "      <td>0.722930</td>\n",
       "      <td>0.155340</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.564196</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[574, 74], [48, 1112]]</td>\n",
       "      <td>0.932522</td>\n",
       "      <td>0.947997</td>\n",
       "      <td>0.937605</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.922212</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>2.330636</td>\n",
       "      <td>0.851967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[243, 50], [14, 7]]</td>\n",
       "      <td>0.796178</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.581342</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[580, 68], [561, 599]]</td>\n",
       "      <td>0.652102</td>\n",
       "      <td>0.655720</td>\n",
       "      <td>0.898051</td>\n",
       "      <td>0.516379</td>\n",
       "      <td>0.705721</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>12.016009</td>\n",
       "      <td>0.352291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 100, 'dro...</td>\n",
       "      <td>[[236, 57], [13, 8]]</td>\n",
       "      <td>0.777070</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.593207</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[600, 48], [61, 1099]]</td>\n",
       "      <td>0.939712</td>\n",
       "      <td>0.952752</td>\n",
       "      <td>0.958152</td>\n",
       "      <td>0.947414</td>\n",
       "      <td>0.936670</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>2.082282</td>\n",
       "      <td>0.869490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[167, 126], [5, 16]]</td>\n",
       "      <td>0.582803</td>\n",
       "      <td>0.196319</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.665935</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[410, 238], [80, 1080]]</td>\n",
       "      <td>0.824115</td>\n",
       "      <td>0.871671</td>\n",
       "      <td>0.819423</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.781875</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>6.074956</td>\n",
       "      <td>0.595811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[236, 57], [12, 9]]</td>\n",
       "      <td>0.780255</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.617016</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[555, 93], [396, 764]]</td>\n",
       "      <td>0.729535</td>\n",
       "      <td>0.757561</td>\n",
       "      <td>0.891482</td>\n",
       "      <td>0.658621</td>\n",
       "      <td>0.757551</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>9.341558</td>\n",
       "      <td>0.466919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'activation': 'linear', 'batch_size': 100, 'd...</td>\n",
       "      <td>[[222, 71], [10, 11]]</td>\n",
       "      <td>0.742038</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.640744</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[522, 126], [420, 740]]</td>\n",
       "      <td>0.698009</td>\n",
       "      <td>0.730503</td>\n",
       "      <td>0.854503</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.721743</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>10.430461</td>\n",
       "      <td>0.403123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    level_0  index                                         hyperparam  \\\n",
       "25       25     25  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "11       11     11  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "3         3      3  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "15       15     15  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "31       31     31  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "26       26     26  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "32       32     32  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "22       22     22  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "17       17     17  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "19       19     19  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "18       18     18  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "16       16     16  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "14       14     14  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "21       21     21  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "20       20     20  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "13       13     13  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "28       28     28  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "23       23     23  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "12       12     12  {'activation': 'softmax', 'batch_size': 100, '...   \n",
       "29       29     29  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "24       24     24  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "27       27     27  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "35       35     35  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "1         1      1  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "30       30     30  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "0         0      0  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "10       10     10  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "4         4      4  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "2         2      2  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "6         6      6  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "34       34     34  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "9         9      9  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "33       33     33  {'activation': 'relu', 'batch_size': 100, 'dro...   \n",
       "7         7      7  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "8         8      8  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "5         5      5  {'activation': 'linear', 'batch_size': 100, 'd...   \n",
       "\n",
       "          train_cf_matrix  train_accuracy  train_f1  train_precision  \\\n",
       "25   [[274, 19], [21, 0]]        0.872611  0.000000         0.000000   \n",
       "11   [[265, 28], [20, 1]]        0.847134  0.040000         0.034483   \n",
       "3    [[264, 29], [19, 2]]        0.847134  0.076923         0.064516   \n",
       "15   [[53, 240], [7, 14]]        0.213376  0.101818         0.055118   \n",
       "31   [[260, 33], [18, 3]]        0.837580  0.105263         0.083333   \n",
       "26  [[106, 187], [9, 12]]        0.375796  0.109091         0.060302   \n",
       "32   [[217, 76], [15, 6]]        0.710191  0.116505         0.073171   \n",
       "22   [[23, 270], [3, 18]]        0.130573  0.116505         0.062500   \n",
       "17    [[0, 293], [0, 21]]        0.066879  0.125373         0.066879   \n",
       "19    [[0, 293], [0, 21]]        0.066879  0.125373         0.066879   \n",
       "18    [[0, 293], [0, 21]]        0.066879  0.125373         0.066879   \n",
       "16    [[0, 293], [0, 21]]        0.066879  0.125373         0.066879   \n",
       "14    [[0, 293], [0, 21]]        0.066879  0.125373         0.066879   \n",
       "21    [[0, 293], [0, 21]]        0.066879  0.125373         0.066879   \n",
       "20    [[0, 293], [0, 21]]        0.066879  0.125373         0.066879   \n",
       "13    [[0, 293], [0, 21]]        0.066879  0.125373         0.066879   \n",
       "28  [[180, 113], [12, 9]]        0.601911  0.125874         0.073770   \n",
       "23    [[2, 291], [0, 21]]        0.073248  0.126126         0.067308   \n",
       "12   [[11, 282], [0, 21]]        0.101911  0.129630         0.069307   \n",
       "29  [[186, 107], [12, 9]]        0.621019  0.131387         0.077586   \n",
       "24   [[205, 88], [13, 8]]        0.678344  0.136752         0.083333   \n",
       "27   [[233, 60], [15, 6]]        0.761146  0.137931         0.090909   \n",
       "35   [[260, 33], [17, 4]]        0.840764  0.137931         0.108108   \n",
       "1   [[104, 189], [5, 16]]        0.382166  0.141593         0.078049   \n",
       "30   [[223, 70], [14, 7]]        0.732484  0.142857         0.090909   \n",
       "0    [[42, 251], [0, 21]]        0.200637  0.143345         0.077206   \n",
       "10   [[95, 198], [4, 17]]        0.356688  0.144068         0.079070   \n",
       "4    [[85, 208], [3, 18]]        0.328025  0.145749         0.079646   \n",
       "2    [[239, 54], [15, 6]]        0.780255  0.148148         0.100000   \n",
       "6   [[158, 135], [8, 13]]        0.544586  0.153846         0.087838   \n",
       "34   [[219, 74], [13, 8]]        0.722930  0.155340         0.097561   \n",
       "9    [[243, 50], [14, 7]]        0.796178  0.179487         0.122807   \n",
       "33   [[236, 57], [13, 8]]        0.777070  0.186047         0.123077   \n",
       "7   [[167, 126], [5, 16]]        0.582803  0.196319         0.112676   \n",
       "8    [[236, 57], [12, 9]]        0.780255  0.206897         0.136364   \n",
       "5   [[222, 71], [10, 11]]        0.742038  0.213592         0.134146   \n",
       "\n",
       "    train_recall  train_auc  train_type_1_error_FP  ...  \\\n",
       "25      0.000000   0.467577                     12  ...   \n",
       "11      0.047619   0.476028                     12  ...   \n",
       "3       0.095238   0.498131                     12  ...   \n",
       "15      0.666667   0.423777                     12  ...   \n",
       "31      0.142857   0.515115                     12  ...   \n",
       "26      0.571429   0.466602                     12  ...   \n",
       "32      0.285714   0.513164                     12  ...   \n",
       "22      0.857143   0.467821                     12  ...   \n",
       "17      1.000000   0.500000                     12  ...   \n",
       "19      1.000000   0.500000                     12  ...   \n",
       "18      1.000000   0.500000                     12  ...   \n",
       "16      1.000000   0.500000                     12  ...   \n",
       "14      1.000000   0.500000                     12  ...   \n",
       "21      1.000000   0.500000                     12  ...   \n",
       "20      1.000000   0.500000                     12  ...   \n",
       "13      1.000000   0.500000                     12  ...   \n",
       "28      0.428571   0.521453                     12  ...   \n",
       "23      1.000000   0.503413                     12  ...   \n",
       "12      1.000000   0.518771                     12  ...   \n",
       "29      0.428571   0.531692                     12  ...   \n",
       "24      0.380952   0.540306                     12  ...   \n",
       "27      0.285714   0.540468                     12  ...   \n",
       "35      0.190476   0.538924                     12  ...   \n",
       "1       0.761905   0.558427                     12  ...   \n",
       "30      0.333333   0.547213                     12  ...   \n",
       "0       1.000000   0.571672                     12  ...   \n",
       "10      0.809524   0.566878                     12  ...   \n",
       "4       0.857143   0.573623                     12  ...   \n",
       "2       0.285714   0.550707                     12  ...   \n",
       "6       0.619048   0.579148                     12  ...   \n",
       "34      0.380952   0.564196                     12  ...   \n",
       "9       0.333333   0.581342                     12  ...   \n",
       "33      0.380952   0.593207                     12  ...   \n",
       "7       0.761905   0.665935                     12  ...   \n",
       "8       0.428571   0.617016                     12  ...   \n",
       "5       0.523810   0.640744                     12  ...   \n",
       "\n",
       "               test_cf_matrix  test_accuracy   test_f1 test_precision  \\\n",
       "25     [[641, 7], [455, 705]]       0.744469  0.753205       0.990169   \n",
       "11    [[626, 22], [873, 287]]       0.504978  0.390742       0.928803   \n",
       "3     [[617, 31], [753, 407]]       0.566372  0.509387       0.929224   \n",
       "15   [[146, 502], [89, 1071]]       0.673119  0.783754       0.680865   \n",
       "31    [[627, 21], [288, 872]]       0.829093  0.849489       0.976484   \n",
       "26    [[353, 295], [0, 1160]]       0.836836  0.887189       0.797251   \n",
       "32    [[585, 63], [18, 1142]]       0.955199  0.965751       0.947718   \n",
       "22    [[82, 566], [11, 1149]]       0.680863  0.799304       0.669971   \n",
       "17      [[0, 648], [0, 1160]]       0.641593  0.781671       0.641593   \n",
       "19      [[0, 648], [0, 1160]]       0.641593  0.781671       0.641593   \n",
       "18      [[0, 648], [0, 1160]]       0.641593  0.781671       0.641593   \n",
       "16      [[0, 648], [0, 1160]]       0.641593  0.781671       0.641593   \n",
       "14      [[0, 648], [0, 1160]]       0.641593  0.781671       0.641593   \n",
       "21      [[0, 648], [0, 1160]]       0.641593  0.781671       0.641593   \n",
       "20      [[0, 648], [0, 1160]]       0.641593  0.781671       0.641593   \n",
       "13      [[0, 648], [0, 1160]]       0.641593  0.781671       0.641593   \n",
       "28    [[520, 128], [0, 1160]]       0.929204  0.947712       0.900621   \n",
       "23      [[5, 643], [0, 1160]]       0.644358  0.782990       0.643372   \n",
       "12     [[21, 627], [0, 1160]]       0.653208  0.787241       0.649133   \n",
       "29   [[497, 151], [23, 1137]]       0.903761  0.928922       0.882764   \n",
       "24    [[552, 96], [26, 1134]]       0.932522  0.948954       0.921951   \n",
       "27    [[595, 53], [32, 1128]]       0.952987  0.963691       0.955123   \n",
       "35    [[626, 22], [246, 914]]       0.851770  0.872137       0.976496   \n",
       "1    [[282, 366], [38, 1122]]       0.776549  0.847432       0.754032   \n",
       "30    [[586, 62], [30, 1130]]       0.949115  0.960884       0.947987   \n",
       "0     [[103, 545], [0, 1160]]       0.698562  0.809773       0.680352   \n",
       "10   [[238, 410], [11, 1149]]       0.767146  0.845164       0.737011   \n",
       "4    [[190, 458], [10, 1150]]       0.741150  0.830925       0.715174   \n",
       "2     [[562, 86], [441, 719]]       0.708518  0.731807       0.893168   \n",
       "6   [[389, 259], [118, 1042]]       0.791482  0.846810       0.800922   \n",
       "34    [[574, 74], [48, 1112]]       0.932522  0.947997       0.937605   \n",
       "9     [[580, 68], [561, 599]]       0.652102  0.655720       0.898051   \n",
       "33    [[600, 48], [61, 1099]]       0.939712  0.952752       0.958152   \n",
       "7    [[410, 238], [80, 1080]]       0.824115  0.871671       0.819423   \n",
       "8     [[555, 93], [396, 764]]       0.729535  0.757561       0.891482   \n",
       "5    [[522, 126], [420, 740]]       0.698009  0.730503       0.854503   \n",
       "\n",
       "    test_recall  test_auc  test_type_1_error_FP  test_type_2_error_FN  \\\n",
       "25     0.607759  0.798478                    12                   110   \n",
       "11     0.247414  0.606732                    12                   110   \n",
       "3      0.350862  0.651511                    12                   110   \n",
       "15     0.923276  0.574292                    12                   110   \n",
       "31     0.751724  0.859658                    12                   110   \n",
       "26     1.000000  0.772377                    12                   110   \n",
       "32     0.984483  0.943630                    12                   110   \n",
       "22     0.990517  0.558530                    12                   110   \n",
       "17     1.000000  0.500000                    12                   110   \n",
       "19     1.000000  0.500000                    12                   110   \n",
       "18     1.000000  0.500000                    12                   110   \n",
       "16     1.000000  0.500000                    12                   110   \n",
       "14     1.000000  0.500000                    12                   110   \n",
       "21     1.000000  0.500000                    12                   110   \n",
       "20     1.000000  0.500000                    12                   110   \n",
       "13     1.000000  0.500000                    12                   110   \n",
       "28     1.000000  0.901235                    12                   110   \n",
       "23     1.000000  0.503858                    12                   110   \n",
       "12     1.000000  0.516204                    12                   110   \n",
       "29     0.980172  0.873574                    12                   110   \n",
       "24     0.977586  0.914719                    12                   110   \n",
       "27     0.972414  0.945312                    12                   110   \n",
       "35     0.787931  0.876990                    12                   110   \n",
       "1      0.967241  0.701213                    12                   110   \n",
       "30     0.974138  0.939229                    12                   110   \n",
       "0      1.000000  0.579475                    12                   110   \n",
       "10     0.990517  0.678901                    12                   110   \n",
       "4      0.991379  0.642295                    12                   110   \n",
       "2      0.619828  0.743556                    12                   110   \n",
       "6      0.898276  0.749292                    12                   110   \n",
       "34     0.958621  0.922212                    12                   110   \n",
       "9      0.516379  0.705721                    12                   110   \n",
       "33     0.947414  0.936670                    12                   110   \n",
       "7      0.931034  0.781875                    12                   110   \n",
       "8      0.658621  0.757551                    12                   110   \n",
       "5      0.637931  0.721743                    12                   110   \n",
       "\n",
       "    test_log_loss  test_cohen_kappa_score  \n",
       "25       8.825730                0.517932  \n",
       "11      17.097468                0.165498  \n",
       "3       14.977005                0.243217  \n",
       "15      11.290276                0.172904  \n",
       "31       5.902931                0.659360  \n",
       "26       5.635606                0.605596  \n",
       "32       1.547396                0.901070  \n",
       "22      11.022858                0.144344  \n",
       "17      12.379229                0.000000  \n",
       "19      12.379229                0.000000  \n",
       "18      12.379229                0.000000  \n",
       "16      12.379229                0.000000  \n",
       "14      12.379229                0.000000  \n",
       "21      12.379229                0.000000  \n",
       "20      12.379229                0.000000  \n",
       "13      12.379229                0.000000  \n",
       "28       2.445280                0.839046  \n",
       "23      12.283710                0.009880  \n",
       "12      11.978050                0.041207  \n",
       "29       3.324042                0.781203  \n",
       "24       2.330646                0.849695  \n",
       "27       1.623804                0.897039  \n",
       "35       5.119696                0.700538  \n",
       "1        7.717897                0.453034  \n",
       "30       1.757532                0.888138  \n",
       "0       10.411543                0.195178  \n",
       "10       8.042673                0.414068  \n",
       "4        8.940550                0.335828  \n",
       "2       10.067480                0.434571  \n",
       "6        7.202061                0.523734  \n",
       "34       2.330636                0.851967  \n",
       "9       12.016009                0.352291  \n",
       "33       2.082282                0.869490  \n",
       "7        6.074956                0.595811  \n",
       "8        9.341558                0.466919  \n",
       "5       10.430461                0.403123  \n",
       "\n",
       "[36 rows x 23 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv('df_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3619'>3620</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/dle/Documents/VSCode Repo/Fault-Detection-SECOM/3rd presentation/daniel/console-step-wise.ipynb Cell 43'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dle/Documents/VSCode%20Repo/Fault-Detection-SECOM/3rd%20presentation/daniel/console-step-wise.ipynb#ch0000050?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dle/Documents/VSCode%20Repo/Fault-Detection-SECOM/3rd%20presentation/daniel/console-step-wise.ipynb#ch0000050?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39;49mloc[\u001b[39m0\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py:960\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py?line=957'>958</a>\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(com\u001b[39m.\u001b[39mapply_if_callable(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py?line=958'>959</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py?line=959'>960</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_get_value(\u001b[39m*\u001b[39;49mkey, takeable\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_takeable)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py?line=960'>961</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple(key)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py?line=961'>962</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py?line=962'>963</a>\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py:3622\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py?line=3615'>3616</a>\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_engine\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py?line=3617'>3618</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, MultiIndex):\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py?line=3618'>3619</a>\u001b[0m     \u001b[39m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py?line=3619'>3620</a>\u001b[0m     \u001b[39m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py?line=3620'>3621</a>\u001b[0m     \u001b[39m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py?line=3621'>3622</a>\u001b[0m     row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(index)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py?line=3622'>3623</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m series\u001b[39m.\u001b[39m_values[row]\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py?line=3624'>3625</a>\u001b[0m \u001b[39m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py?line=3625'>3626</a>\u001b[0m \u001b[39m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3622'>3623</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3623'>3624</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3624'>3625</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3625'>3626</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3626'>3627</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3627'>3628</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['a','b'])\n",
    "df.loc[0,'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without FS\n",
    "cfm [[181 112]\n",
    " [ 11  10]]\n",
    "acc 0.60828025477707\n",
    "\n",
    "#with FS\n",
    "Best scores: 0.644106 (+-0.286622) using {'activation': 'linear', 'batch_size': 100, 'dropout_rate': 0, 'epochs': 100, 'neurons': 10}\n",
    "cfm [[182 111]\n",
    " [  8  13]]\n",
    "acc 0.6210191082802548\n",
    "recall_score 0.6190476190476191"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3aeb15c8c31224d9ef37a76c0046f703a279f439b6efd04fb2681e5f2715bf2f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
