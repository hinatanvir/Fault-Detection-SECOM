{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #importing libraries\n",
    "# import csv\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from boruta import BorutaPy\n",
    "# from BorutaShap import BorutaShap\n",
    "# from sklearn.feature_selection import RFE\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.combine import SMOTEENN\n",
    "\n",
    "# from collections import Counter\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is to read, transform and join 2 data frame\n",
    "\n",
    "def read_features():\n",
    "    path = 'input/secom.data'\n",
    "    df = pd.read_csv(path, delimiter=' ', header=None, na_values=['NaN'])\n",
    "    df.columns = ['feature_'+str(x+1) for x in range(len(df.columns))]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def read_target():\n",
    "    path = 'input/secom_labels.data'\n",
    "    df = pd.read_csv(path, delimiter=' ', header=None, na_values=['NaN'])\n",
    "    df.columns = ['status','timestamp']\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'],dayfirst=True)\n",
    "    return df\n",
    "\n",
    "#for the testing purporse, trim to remain first 100 rows only\n",
    "X = read_features()\n",
    "y = read_target().iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the duplicated features (columns)\n",
    "def remove_duplicated_columns(df):\n",
    "    dict_duplicate_pair = {}\n",
    "    dict_duplicate_matches = {}\n",
    "    list_duplicate = []\n",
    "    to_remove = []\n",
    "    for i in range(0, len(df.columns)):\n",
    "        l = []\n",
    "        for j in range(i+1,len(df.columns)):\n",
    "            dict_duplicate_pair[str(i+1)+';'+str(j+1)] = df.iloc[:,i].equals(df.iloc[:,j])\n",
    "            if df.iloc[:,i].equals(df.iloc[:,j]) == True:\n",
    "                if j not in list_duplicate:\n",
    "                    l.append(j)\n",
    "                    to_remove.append('feature_'+str(j+1))\n",
    "                list_duplicate.append(i)\n",
    "                list_duplicate.append(j)\n",
    "        if len(l)!=0:\n",
    "            dict_duplicate_matches[i] = l\n",
    "\n",
    "\n",
    "    df_duplicate_pair = pd.DataFrame.from_dict(dict_duplicate_pair, orient='index')\n",
    "    df_duplicate_pair.columns=['duplicate']\n",
    "\n",
    "    df_duplicate_matches = pd.DataFrame.from_dict(dict_duplicate_matches, orient='index')\n",
    "\n",
    "    \n",
    "    df = df.drop(columns=to_remove, axis = 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# X = remove_duplicated_columns(X)\n",
    "# X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove columns with Constant volatility (std=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_volatility(df):\n",
    "    df_EDA= df.describe().T\n",
    "    df_EDA= df_EDA[df_EDA[\"std\"] == 0]\n",
    "    df = df.drop(axis=1, columns=df_EDA.index)\n",
    "    return df\n",
    "\n",
    "# X = remove_constant_volatility(X)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove columns with high %Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols_with_high_pct_null(df, null_threshold):\n",
    "    list_column_with_pct_null = pd.concat([df.isnull().sum(), df.isnull().sum()/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\n",
    "    list_column_with_pct_null= list_column_with_pct_null[list_column_with_pct_null[\"Percentage (%)\"] >= null_threshold]\n",
    "    df = df.drop(axis=1, columns=list_column_with_pct_null.index)\n",
    "    return df\n",
    "\n",
    "# X = remove_cols_with_high_pct_null(X, 0.8)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how = ['NaN', '3s']\n",
    "def replace_outlier(df, how):\n",
    "    for col in df:\n",
    "        ll_col = df[col].mean() - 3 * df[col].std()\n",
    "        ul_col = df[col].mean() + 3 * df[col].std()\n",
    "        if how == 'NaN':\n",
    "            df[col] = np.where(df[col]>ul_col,np.NaN,np.where(df[col]<ll_col,np.NaN,df[col]))\n",
    "        elif how == '3s':\n",
    "            df[col] = np.where(df[col]>ul_col,ul_col,np.where(df[col]<ll_col,ll_col,df[col]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which_weights = ['distance','uniform']\n",
    "\n",
    "def impute_null_with_knn(X_train, X_test, which_weights):\n",
    "    #First scale the data \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns= X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns= X_test.columns)\n",
    "\n",
    "    knn = KNNImputer(n_neighbors=5, weights=which_weights) #check this neighbors = 5\n",
    "\n",
    "    X_train = pd.DataFrame(knn.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(knn.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    X_train = pd.DataFrame(scaler.inverse_transform(X_train), columns= X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.inverse_transform(X_test), columns= X_test.columns)\n",
    "    return X_train, X_test\n",
    "\n",
    "#X_train = impute_null_with_knn(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null_with_mice(X_train, X_test): \n",
    "    imp = IterativeImputer(max_iter=5, verbose=0, imputation_order='roman', random_state=0)\n",
    "    X_train = pd.DataFrame(imp.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(imp.transform(X_test), columns=X_test.columns)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_method=['shap','gini']\n",
    "\n",
    "def BorutaShap_FS (X, y, method_option) :\n",
    "    modelshap = RandomForestClassifier(n_jobs=-1,n_estimators=100, max_depth=5, random_state=100)\n",
    "    # define model for resp. classifier\n",
    "    modelshap.fit(X,y)\n",
    "    ##-- feature_names = np.array(X.columns)\n",
    "    # define Boruta Sahp feature selection method\n",
    "    feature_selector = BorutaShap(model=modelshap,\n",
    "                              importance_measure=method_option,\n",
    "                              classification=True)  # find all relevant features\n",
    "    feature_selector.fit(X,y,n_trials=100, sample=False, verbose=False, random_state=100)  \n",
    "    ##-- feature_selector.plot(which_features='accepted',figsize=(20,10))\n",
    "    # call transform() on X to filter it down to selected features\n",
    "    return  feature_selector.Subset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE function with random forest\n",
    "\n",
    "def RFE_FS (X, y, classify) :\n",
    "    feature_names = np.array(X.columns)\n",
    "    if classify == 'RF':\n",
    "    # define random forest classifier\n",
    "        model = RandomForestClassifier(n_jobs=-1, class_weight='balanced_subsample', max_depth=5, random_state=100)\n",
    "        #model.fit(X, y)\n",
    "        #rfe = RFE(estimator = model,n_features_to_select = 15)\n",
    "    if classify== 'SVM':\n",
    "        model = SVC(kernel='linear',C=5)\n",
    "        #model.fit(X, y)\n",
    "        #rfe = RFECV(estimator = model,scoring='accuracy')\n",
    "    # find all relevant features\n",
    "    model.fit(X, y)\n",
    "    rfe = RFE(estimator = model,n_features_to_select = 15)\n",
    "    rfe.fit(X,y)\n",
    "\n",
    "    # check selected features\n",
    "    ##-- rfe.support_\n",
    "\n",
    "    # check ranking of features\n",
    "    ##--rfe.ranking_\n",
    "\n",
    "    # zip feature names, ranks, and decisions \n",
    "    # feature_ranks = list(zip(feature_names, \n",
    "    #                          rfe.ranking_, \n",
    "    #                          rfe.support_))\n",
    "\n",
    "    # print the results\n",
    "    ##--for feat in feature_ranks:\n",
    "    ##--    print('Feature: {:<30} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    "        \n",
    "    final_features_rfe = list()\n",
    "    indexes = np.where(rfe.ranking_ <= 1)\n",
    "    for x in np.nditer(indexes):\n",
    "        final_features_rfe.append(feature_names[x])\n",
    "    ##-- print(final_features_rfe)\n",
    "    \n",
    " # call transform() on X to filter it down to selected features\n",
    "    return pd.DataFrame(X.filter(final_features_rfe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boruta function with random forest\n",
    "\n",
    "def BorutaPy_FS (X, y) :\n",
    "    feature_names = np.array(X.columns)\n",
    "\n",
    "    # define random forest classifier\n",
    "    model = RandomForestClassifier(n_jobs=-1, class_weight='balanced_subsample', max_depth=5, random_state=100)\n",
    "    model.fit(X, y)\n",
    "    # define Boruta feature selection method\n",
    "    \n",
    "    feature_selector = BorutaPy(model, n_estimators='auto', verbose=0, random_state=100, max_iter=140)\n",
    "\n",
    "    # find all relevant features\n",
    "    feature_selector.fit(X.to_numpy(),y)\n",
    "\n",
    "    # check selected features\n",
    "    ##--feature_selector.support_\n",
    "\n",
    "    # check ranking of features\n",
    "    ##--feature_ranking=feature_selector.ranking_\n",
    "\n",
    "    # zip feature names, ranks, and decisions \n",
    "    # feature_ranks = list(zip(feature_names, \n",
    "    #                          feature_selector.ranking_, \n",
    "    #                          feature_selector.support_))\n",
    "\n",
    "    # print the results\n",
    "    ##--for feat in feature_ranks:\n",
    "    ##--    print('Feature: {:<30} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    "        \n",
    "    final_features = list()\n",
    "    indexes = np.where(feature_selector.ranking_ <= 1)\n",
    "    for x in np.nditer(indexes):\n",
    "        final_features.append(feature_names[x])\n",
    "    ##--print(final_features)\n",
    "    \n",
    " # call transform() on X to filter it down to selected features\n",
    "    return pd.DataFrame(X.filter(final_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(X_train, y_train, sampler):\n",
    "    \n",
    "    #SMOTE\n",
    "    if sampler == 'SMOTE':\n",
    "        sampler = SMOTE(random_state=100)    \n",
    "    \n",
    "    #ROSE\n",
    "    if sampler == 'ROSE':\n",
    "        sampler = RandomOverSampler(random_state=100, shrinkage=1)\n",
    "\n",
    "    #ADASYN\n",
    "    if sampler == 'ADASYN':\n",
    "        sampler = ADASYN(random_state=100)\n",
    "    \n",
    "\n",
    "    #SMOTTEENN\n",
    "    if sampler == 'SMOTEENN' :\n",
    "        sampler = SMOTEENN(random_state=100)\n",
    "        \n",
    "        \n",
    "    #Random under Sampling\n",
    "    if sampler == \"randomunder\":\n",
    "        sampler = RandomUnderSampler(random_state=100)\n",
    "\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    counter = Counter(y_resampled)\n",
    "    print(counter)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# X_train, y_train = sampling(X_train, y_train,'SMOTE')\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X_train, y_train, X_test, y_test):\n",
    "    # building model before balancing data\n",
    "    model = RandomForestClassifier(random_state=1, n_estimators=1000, max_depth=5)\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "    #For TEST SPLIT\n",
    "    y_pred= model.predict(X_test)\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy= accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred) ##\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    specificity = cf_matrix[1][1] / ( cf_matrix[1][1] + cf_matrix[1][0] )\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    type_1_error_FP = cf_matrix[0][1]\n",
    "    type_2_error_FN = cf_matrix[1][0]\n",
    "    #Note by default 1 is the positive label. Therefore, -1 is negative\n",
    "\n",
    "    #For TRAIN SPLIT\n",
    "    y_pred_train= model.predict(X_train)\n",
    "    cf_matrix_train = confusion_matrix(y_train, y_pred_train)\n",
    "    accuracy_train= accuracy_score(y_train, y_pred_train)\n",
    "    f1_train = f1_score(y_train, y_pred_train) ##\n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    recall_train = recall_score(y_train, y_pred_train)\n",
    "    specificity_train = cf_matrix_train[1][1] / ( cf_matrix_train[1][1] + cf_matrix_train[1][0] )\n",
    "    auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    type_1_error_FP_train = cf_matrix_train[0][1]\n",
    "    type_2_error_FN_train = cf_matrix_train[1][0]\n",
    "\n",
    "\n",
    "    return cf_matrix, accuracy, f1, precision, recall, specificity, type_1_error_FP, type_2_error_FN, auc, cf_matrix_train, accuracy_train, f1_train, precision_train, recall_train, specificity_train, type_1_error_FP_train, type_2_error_FN_train, auc_train\n",
    "\n",
    "#run_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_track_cols = X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>combination</th>\n",
       "      <th>cols</th>\n",
       "      <th>cf_matrix</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>type_1_error_FP</th>\n",
       "      <th>...</th>\n",
       "      <th>auc</th>\n",
       "      <th>cf_matrix_train</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>recall_train</th>\n",
       "      <th>specificity_train</th>\n",
       "      <th>type_1_error_FP_train</th>\n",
       "      <th>type_2_error_FN_train</th>\n",
       "      <th>auc_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3s&amp;knn__distance&amp;BoP&amp;SMOTE</td>\n",
       "      <td>Index(['feature_20', 'feature_22', 'feature_34...</td>\n",
       "      <td>[[238, 55], [12, 9]]</td>\n",
       "      <td>0.786624</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620429</td>\n",
       "      <td>[[953, 217], [96, 1074]]</td>\n",
       "      <td>0.866239</td>\n",
       "      <td>0.872816</td>\n",
       "      <td>0.831913</td>\n",
       "      <td>0.917949</td>\n",
       "      <td>0.917949</td>\n",
       "      <td>217</td>\n",
       "      <td>96</td>\n",
       "      <td>0.866239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3s&amp;knn__distance&amp;BoS__shap&amp;SMOTE</td>\n",
       "      <td>Index(['feature_34', 'feature_60', 'feature_47...</td>\n",
       "      <td>[[228, 65], [6, 15]]</td>\n",
       "      <td>0.773885</td>\n",
       "      <td>0.297030</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746221</td>\n",
       "      <td>[[921, 249], [185, 985]]</td>\n",
       "      <td>0.814530</td>\n",
       "      <td>0.819468</td>\n",
       "      <td>0.798217</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>249</td>\n",
       "      <td>185</td>\n",
       "      <td>0.814530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3s&amp;knn__distance&amp;RFE__RF&amp;SMOTE</td>\n",
       "      <td>Index(['feature_1', 'feature_20', 'feature_22'...</td>\n",
       "      <td>[[248, 45], [13, 8]]</td>\n",
       "      <td>0.815287</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613684</td>\n",
       "      <td>[[1022, 148], [37, 1133]]</td>\n",
       "      <td>0.920940</td>\n",
       "      <td>0.924521</td>\n",
       "      <td>0.884465</td>\n",
       "      <td>0.968376</td>\n",
       "      <td>0.968376</td>\n",
       "      <td>148</td>\n",
       "      <td>37</td>\n",
       "      <td>0.920940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3s&amp;MICE&amp;BoP&amp;SMOTE</td>\n",
       "      <td>Index(['feature_1', 'feature_20', 'feature_22'...</td>\n",
       "      <td>[[233, 60], [9, 12]]</td>\n",
       "      <td>0.780255</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683325</td>\n",
       "      <td>[[958, 212], [35, 1135]]</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.901867</td>\n",
       "      <td>0.842613</td>\n",
       "      <td>0.970085</td>\n",
       "      <td>0.970085</td>\n",
       "      <td>212</td>\n",
       "      <td>35</td>\n",
       "      <td>0.894444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   No.                       combination  \\\n",
       "0    0        3s&knn__distance&BoP&SMOTE   \n",
       "1    1  3s&knn__distance&BoS__shap&SMOTE   \n",
       "2    2    3s&knn__distance&RFE__RF&SMOTE   \n",
       "3    3                 3s&MICE&BoP&SMOTE   \n",
       "\n",
       "                                                cols             cf_matrix  \\\n",
       "0  Index(['feature_20', 'feature_22', 'feature_34...  [[238, 55], [12, 9]]   \n",
       "1  Index(['feature_34', 'feature_60', 'feature_47...  [[228, 65], [6, 15]]   \n",
       "2  Index(['feature_1', 'feature_20', 'feature_22'...  [[248, 45], [13, 8]]   \n",
       "3  Index(['feature_1', 'feature_20', 'feature_22'...  [[233, 60], [9, 12]]   \n",
       "\n",
       "   accuracy        f1  precision    recall  specificity  type_1_error_FP  ...  \\\n",
       "0  0.786624  0.211765   0.140625  0.428571     0.428571               55  ...   \n",
       "1  0.773885  0.297030   0.187500  0.714286     0.714286               65  ...   \n",
       "2  0.815287  0.216216   0.150943  0.380952     0.380952               45  ...   \n",
       "3  0.780255  0.258065   0.166667  0.571429     0.571429               60  ...   \n",
       "\n",
       "        auc            cf_matrix_train accuracy_train  f1_train  \\\n",
       "0  0.620429   [[953, 217], [96, 1074]]       0.866239  0.872816   \n",
       "1  0.746221   [[921, 249], [185, 985]]       0.814530  0.819468   \n",
       "2  0.613684  [[1022, 148], [37, 1133]]       0.920940  0.924521   \n",
       "3  0.683325   [[958, 212], [35, 1135]]       0.894444  0.901867   \n",
       "\n",
       "   precision_train  recall_train  specificity_train  type_1_error_FP_train  \\\n",
       "0         0.831913      0.917949           0.917949                    217   \n",
       "1         0.798217      0.841880           0.841880                    249   \n",
       "2         0.884465      0.968376           0.968376                    148   \n",
       "3         0.842613      0.970085           0.970085                    212   \n",
       "\n",
       "   type_2_error_FN_train  auc_train  \n",
       "0                     96   0.866239  \n",
       "1                    185   0.814530  \n",
       "2                     37   0.920940  \n",
       "3                     35   0.894444  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.DataFrame(result, columns = ['No.','combination','cols','cf_matrix', 'accuracy', 'f1', 'precision', 'recall', 'specificity', 'type_1_error_FP', 'type_2_error_FN', 'auc', 'cf_matrix_train', 'accuracy_train', 'f1_train', 'precision_train', 'recall_train', 'specificity_train', 'type_1_error_FP_train', 'type_2_error_FN_train', 'auc_train'])\n",
    "df_result.to_csv('output/result.csv')\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 1170, 1: 1170})\n",
      "0 3s&knn__distance&BoP&SMOTE \n",
      " acc 0.7866242038216561 \n",
      " accuracy_train 0.8662393162393163 \n",
      " f1 0.2117647058823529 \n",
      " f1_train 0.8728159284843559 \n",
      " spec 0.42857142857142855 \n",
      " auc 0.6204290589956118 \n",
      " cfm \n",
      " [[238  55]\n",
      " [ 12   9]] \n",
      " cols Index(['feature_20', 'feature_22', 'feature_34', 'feature_60', 'feature_122',\n",
      "       'feature_130', 'feature_131', 'feature_206', 'feature_248',\n",
      "       'feature_342', 'feature_478', 'feature_520'],\n",
      "      dtype='object') \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "X = read_features()\n",
    "y = read_target().iloc[:,0]\n",
    "\n",
    "result = []\n",
    "i = 0\n",
    "f = open('output/tracker.csv', 'w')\n",
    "# create the csv writer\n",
    "writer = csv.writer(f)\n",
    "\n",
    "\n",
    "\n",
    "#step 1:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1, stratify=y)\n",
    "\n",
    "\n",
    "#-----------\n",
    "# #step 2:\n",
    "# X_train = remove_duplicated_columns(X_train)\n",
    "\n",
    "# #step 3:\n",
    "# X_train = remove_constant_volatility(X_train)\n",
    "# #step 4:\n",
    "# X_train = remove_cols_with_high_pct_null(X_train, 0.8) #this can be in the loop too, may be later\n",
    "\n",
    "# #step 5: remove the same columns from step 2-3 TRAIN_TEST split\n",
    "# X_test = X_test.loc[:,X_train.columns]\n",
    "\n",
    "#------------\n",
    "\n",
    "\n",
    "X_train = X_train.loc[:, fast_track_cols]\n",
    "X_test = X_test.loc[:, fast_track_cols]\n",
    "\n",
    "#------------\n",
    "\n",
    "\n",
    "#step 6-9\n",
    "replace_outlier_options = ['3s']\n",
    "#replace_outlier_options = ['3s','NaN']\n",
    "impute_null_options = ['knn__distance', 'MICE']\n",
    "#impute_null_options = ['knn__distance', 'MICE', 'knn__uniform']\n",
    "FS_options = ['BoP','BoS__shap', 'RFE__RF' ]\n",
    "#FS_options = ['BoP','BoS__shap', 'BoS__gini', 'RFE__RF', 'RFE__SVM' ]\n",
    "sampling_options = ['SMOTE']\n",
    "#sampling_options = ['SMOTE','ROSE','ADASYN','SMOTEENN']\n",
    "\n",
    "for replace_with in replace_outlier_options:\n",
    "    for knn_weight in impute_null_options:\n",
    "        for classifier_model in FS_options:\n",
    "            #<remove correlated columns, decide on the thresold 70%>\n",
    "            for sampling_technique in sampling_options:\n",
    "                X_train_temp = X_train\n",
    "                X_test_temp = X_test\n",
    "                y_train_temp = y_train\n",
    "                y_test_temp = y_test\n",
    "\n",
    "                #step 6: oulier treatement (on both TRAIN & TEST split)\n",
    "                X_train_temp = replace_outlier(X_train_temp, replace_with)\n",
    "                X_test_temp = replace_outlier(X_test_temp, replace_with)\n",
    "                \n",
    "                #step 7: missing value imputation (on both TRAIN & TEST split)\n",
    "                if knn_weight == 'knn__distance' or knn_weight == 'knn__uniform':\n",
    "                    X_train_temp, X_test_temp = impute_null_with_knn(X_train_temp, X_test_temp, knn_weight[-(len(knn_weight)-5):])\n",
    "                elif knn_weight == 'MICE':\n",
    "                    X_train_temp, X_test_temp = impute_null_with_mice(X_train_temp, X_test_temp)\n",
    "\n",
    "                #step 8: feature selection (on both TRAIN & TEST split)\n",
    "                if classifier_model == 'BoS__shap' or classifier_model == 'BoS__gini':\n",
    "                    X_train_temp = BorutaShap_FS(X_train_temp, y_train_temp, classifier_model[-(len(classifier_model)-5):])\n",
    "                elif classifier_model == 'RFE__RF' or classifier_model == 'RFE__SVM':\n",
    "                    X_train_temp = RFE_FS(X_train_temp, y_train_temp, classifier_model[-(len(classifier_model)-5):])\n",
    "                elif classifier_model == 'BoP':\n",
    "                    X_train_temp = BorutaPy_FS(X_train_temp, y_train_temp)\n",
    "                \n",
    "                #apply the same result for TEST\n",
    "                X_test_temp = X_test_temp.loc[:,X_train_temp.columns]\n",
    "\n",
    "\n",
    "                #step 9: balancing only on TRAIN split\n",
    "                X_train_temp, y_train_temp = sampling(X_train_temp, y_train_temp, sampling_technique)\n",
    "\n",
    "                #print out datasets for backup\n",
    "                X_train_temp.to_csv('output/X_train_temp_'+str(i)+'.csv')\n",
    "                X_test_temp.to_csv('output/X_test_temp_'+str(i)+'.csv')\n",
    "                y_train_temp.to_csv('output/y_train_temp_'+str(i)+'.csv')\n",
    "                y_test_temp.to_csv('output/y_test_temp_'+str(i)+'.csv')\n",
    "                \n",
    "\n",
    "\n",
    "                #step 10: train model, predict, and print scores\n",
    "                cf_matrix, accuracy, f1, precision, recall, specificity, type_1_error_FP, type_2_error_FN, auc, cf_matrix_train, accuracy_train, f1_train, precision_train, recall_train, specificity_train, type_1_error_FP_train, type_2_error_FN_train, auc_train = run_model(X_train_temp, y_train_temp, X_test_temp, y_test_temp)\n",
    "\n",
    "                combined_technique = replace_with +'&'+ knn_weight +'&'+ classifier_model +'&'+ sampling_technique\n",
    "                result.append((i, combined_technique, X_train_temp.columns, cf_matrix, accuracy, f1, precision, recall, specificity, type_1_error_FP, type_2_error_FN, auc, cf_matrix_train, accuracy_train, f1_train, precision_train, recall_train, specificity_train, type_1_error_FP_train, type_2_error_FN_train, auc_train))\n",
    "                \n",
    "                print(i, combined_technique,'\\n acc', accuracy,'\\n accuracy_train',accuracy_train, '\\n f1', f1,'\\n f1_train',f1_train, '\\n spec', specificity, '\\n auc', auc, '\\n cfm', '\\n', cf_matrix, '\\n cols', X_train_temp.columns, '\\n')\n",
    "                #print(i, combined_technique)\n",
    "                row = str(i) +' '+ combined_technique +' acc '+ str(accuracy) +' f1 '+ str(f1) +' acc_train '+ str(accuracy_train) +' f1_train '+ str(f1_train)\n",
    "                writer.writerow(row)\n",
    "\n",
    "                i+=1\n",
    "\n",
    "\n",
    "\n",
    "df_result = pd.DataFrame(result, columns = ['No.','combination','cols','cf_matrix', 'accuracy', 'f1', 'precision', 'recall', 'specificity', 'type_1_error_FP', 'type_2_error_FN', 'auc', 'cf_matrix_train', 'accuracy_train', 'f1_train', 'precision_train', 'recall_train', 'specificity_train', 'type_1_error_FP_train', 'type_2_error_FN_train', 'auc_train'])\n",
    "df_result.to_csv('output/result.csv')\n",
    "print(result)\n",
    "# close the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name  val\n",
       "0     1    2\n",
       "1     2    1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst =[]\n",
    "a=1\n",
    "b=2\n",
    "lst.append((a,b))\n",
    "lst.append((b,a))\n",
    "\n",
    "df = pd.DataFrame(lst,\n",
    "               columns =['Name', 'val'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Geeks</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geeks</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>portal</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>for</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Geeks</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name  val\n",
       "0   Geeks   11\n",
       "1     For   22\n",
       "2   Geeks   33\n",
       "3      is   44\n",
       "4  portal   55\n",
       "5     for   66\n",
       "6   Geeks   77"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of strings\n",
    "lst = ['Geeks', 'For', 'Geeks', 'is', 'portal', 'for', 'Geeks']\n",
    "  \n",
    "# list of int\n",
    "lst2 = [11, 22, 33, 44, 55, 66, 77]\n",
    "  \n",
    "# Calling DataFrame constructor after zipping\n",
    "# both lists, with columns specified\n",
    "df = pd.DataFrame(list(zip(lst, lst2)),\n",
    "               columns =['Name', 'val'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------Appendix--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #------------------------------------\n",
    "# X_train = replace_outlier(X_train, how)\n",
    "# replace_outlier_options = ['NaN', '3s']\n",
    "# #--\n",
    "# X_train = impute_null_with_knn(X_train, which_weights) #\n",
    "# impute_null_options = ['knn__distance', 'knn__uniform', 'MICE']\n",
    "\n",
    "# X_train = impute_null_with_mice(X_train)\n",
    "# #--\n",
    "# BorutaShap_FS(X_train, y_train, method_option)\n",
    "# list_method=['shap','gini']\n",
    "\n",
    "# RFE_FS(X_train, y_train, classify) \n",
    "# list_clf=['RF','SVM']\n",
    "\n",
    "# BorutaPy_FS(X_train, y_train)\n",
    "\n",
    "# FS_options = ['BoS__shap', 'BoS__gini', 'RFE__RF', 'RFE__SVM', 'BoP']\n",
    "# #--\n",
    "# sampling(X_train, y_train, sampler)\n",
    "# sampling_options = ['SMOTE','ROSE','ADASYN','SMOTEENN']\n",
    "\n",
    "# #--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [2, 3, 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l = []\n",
    "# a=1\n",
    "# b=2\n",
    "# c=3\n",
    "\n",
    "# l.append([a,b,c])\n",
    "# l.append([b,c,a])\n",
    "\n",
    "# l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_null_impute = [knnimputation_distance, MICEimputation_distance]\n",
    "# list_null_outlier = [outlier_knn,outlier_3s]\n",
    "# list_feat_selection = ['Boruta_RF', 'Boruta_shap', 'RFE']\n",
    "# Boruta = ['RF', 'XGB']\n",
    "# Boruta_shap = ['RF', 'XGB', 'kNN']\n",
    "# RFE = ['RF', 'SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9\n",
       "3  1  1  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #TESTING\n",
    "# df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [1,1,1]]),\n",
    "#                    columns=['a', 'b', 'c'])\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3aeb15c8c31224d9ef37a76c0046f703a279f439b6efd04fb2681e5f2715bf2f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
