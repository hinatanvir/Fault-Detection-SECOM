{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #importing libraries\n",
    "# import csv\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from boruta import BorutaPy\n",
    "# from BorutaShap import BorutaShap\n",
    "# from sklearn.feature_selection import RFE\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.combine import SMOTEENN\n",
    "\n",
    "# from collections import Counter\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is to read, transform and join 2 data frame\n",
    "\n",
    "def read_features():\n",
    "    path = 'input/secom.data'\n",
    "    df = pd.read_csv(path, delimiter=' ', header=None, na_values=['NaN'])\n",
    "    df.columns = ['feature_'+str(x+1) for x in range(len(df.columns))]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def read_target():\n",
    "    path = 'input/secom_labels.data'\n",
    "    df = pd.read_csv(path, delimiter=' ', header=None, na_values=['NaN'])\n",
    "    df.columns = ['status','timestamp']\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'],dayfirst=True)\n",
    "    return df\n",
    "\n",
    "#for the testing purporse, trim to remain first 100 rows only\n",
    "X = read_features()\n",
    "y = read_target().iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the duplicated features (columns)\n",
    "def remove_duplicated_columns(df):\n",
    "    dict_duplicate_pair = {}\n",
    "    dict_duplicate_matches = {}\n",
    "    list_duplicate = []\n",
    "    to_remove = []\n",
    "    for i in range(0, len(df.columns)):\n",
    "        l = []\n",
    "        for j in range(i+1,len(df.columns)):\n",
    "            dict_duplicate_pair[str(i+1)+';'+str(j+1)] = df.iloc[:,i].equals(df.iloc[:,j])\n",
    "            if df.iloc[:,i].equals(df.iloc[:,j]) == True:\n",
    "                if j not in list_duplicate:\n",
    "                    l.append(j)\n",
    "                    to_remove.append('feature_'+str(j+1))\n",
    "                list_duplicate.append(i)\n",
    "                list_duplicate.append(j)\n",
    "        if len(l)!=0:\n",
    "            dict_duplicate_matches[i] = l\n",
    "\n",
    "\n",
    "    df_duplicate_pair = pd.DataFrame.from_dict(dict_duplicate_pair, orient='index')\n",
    "    df_duplicate_pair.columns=['duplicate']\n",
    "\n",
    "    df_duplicate_matches = pd.DataFrame.from_dict(dict_duplicate_matches, orient='index')\n",
    "\n",
    "    \n",
    "    df = df.drop(columns=to_remove, axis = 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# X = remove_duplicated_columns(X)\n",
    "# X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove columns with Constant volatility (std=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_volatility(df):\n",
    "    df_EDA= df.describe().T\n",
    "    df_EDA= df_EDA[df_EDA[\"std\"] == 0]\n",
    "    df = df.drop(axis=1, columns=df_EDA.index)\n",
    "    return df\n",
    "\n",
    "# X = remove_constant_volatility(X)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove columns with high %Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols_with_high_pct_null(df, null_threshold):\n",
    "    list_column_with_pct_null = pd.concat([df.isnull().sum(), df.isnull().sum()/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\n",
    "    list_column_with_pct_null= list_column_with_pct_null[list_column_with_pct_null[\"Percentage (%)\"] >= null_threshold]\n",
    "    df = df.drop(axis=1, columns=list_column_with_pct_null.index)\n",
    "    return df\n",
    "\n",
    "# X = remove_cols_with_high_pct_null(X, 0.8)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how = ['NaN', '3s']\n",
    "def replace_outlier(df, how):\n",
    "    for col in df:\n",
    "        ll_col = df[col].mean() - 3 * df[col].std()\n",
    "        ul_col = df[col].mean() + 3 * df[col].std()\n",
    "        if how == 'NaN':\n",
    "            df[col] = np.where(df[col]>ul_col,np.NaN,np.where(df[col]<ll_col,np.NaN,df[col]))\n",
    "        elif how == '3s':\n",
    "            df[col] = np.where(df[col]>ul_col,ul_col,np.where(df[col]<ll_col,ll_col,df[col]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which_weights = ['distance','uniform']\n",
    "\n",
    "def impute_null_with_knn(X_train, X_test, which_weights):\n",
    "    #First scale the data \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns= X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns= X_test.columns)\n",
    "\n",
    "    knn = KNNImputer(n_neighbors=5, weights=which_weights) #check this neighbors = 5\n",
    "\n",
    "    X_train = pd.DataFrame(knn.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(knn.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    X_train = pd.DataFrame(scaler.inverse_transform(X_train), columns= X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.inverse_transform(X_test), columns= X_test.columns)\n",
    "    return X_train, X_test\n",
    "\n",
    "#X_train = impute_null_with_knn(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null_with_mice(X_train, X_test): \n",
    "    imp = IterativeImputer(max_iter=5, verbose=0, imputation_order='roman', random_state=0)\n",
    "    X_train = pd.DataFrame(imp.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(imp.transform(X_test), columns=X_test.columns)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_method=['shap','gini']\n",
    "\n",
    "def BorutaShap_FS (X, y, method_option) :\n",
    "    modelshap = RandomForestClassifier(n_jobs=-1,n_estimators=100, max_depth=5, random_state=100)\n",
    "    # define model for resp. classifier\n",
    "    modelshap.fit(X,y)\n",
    "    ##-- feature_names = np.array(X.columns)\n",
    "    # define Boruta Sahp feature selection method\n",
    "    feature_selector = BorutaShap(model=modelshap,\n",
    "                              importance_measure=method_option,\n",
    "                              classification=True)  # find all relevant features\n",
    "    feature_selector.fit(X,y,n_trials=100, sample=False, verbose=False, random_state=100)  \n",
    "    ##-- feature_selector.plot(which_features='accepted',figsize=(20,10))\n",
    "    # call transform() on X to filter it down to selected features\n",
    "    return  feature_selector.Subset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE function with random forest\n",
    "\n",
    "def RFE_FS (X, y, classify) :\n",
    "    feature_names = np.array(X.columns)\n",
    "    if classify == 'RF':\n",
    "    # define random forest classifier\n",
    "        model = RandomForestClassifier(n_jobs=-1, class_weight='balanced_subsample', max_depth=5, random_state=100)\n",
    "        #model.fit(X, y)\n",
    "        #rfe = RFE(estimator = model,n_features_to_select = 15)\n",
    "    if classify== 'SVM':\n",
    "        model = SVC(kernel='linear',C=5)\n",
    "        #model.fit(X, y)\n",
    "        #rfe = RFECV(estimator = model,scoring='accuracy')\n",
    "    # find all relevant features\n",
    "    model.fit(X, y)\n",
    "    rfe = RFE(estimator = model,n_features_to_select = 15)\n",
    "    rfe.fit(X,y)\n",
    "\n",
    "    # check selected features\n",
    "    ##-- rfe.support_\n",
    "\n",
    "    # check ranking of features\n",
    "    ##--rfe.ranking_\n",
    "\n",
    "    # zip feature names, ranks, and decisions \n",
    "    # feature_ranks = list(zip(feature_names, \n",
    "    #                          rfe.ranking_, \n",
    "    #                          rfe.support_))\n",
    "\n",
    "    # print the results\n",
    "    ##--for feat in feature_ranks:\n",
    "    ##--    print('Feature: {:<30} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    "        \n",
    "    final_features_rfe = list()\n",
    "    indexes = np.where(rfe.ranking_ <= 2) #change to 2\n",
    "    for x in np.nditer(indexes):\n",
    "        final_features_rfe.append(feature_names[x])\n",
    "    ##-- print(final_features_rfe)\n",
    "    \n",
    " # call transform() on X to filter it down to selected features\n",
    "    return pd.DataFrame(X.filter(final_features_rfe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boruta function with random forest\n",
    "\n",
    "def BorutaPy_FS (X, y) :\n",
    "    feature_names = np.array(X.columns)\n",
    "\n",
    "    # define random forest classifier\n",
    "    model = RandomForestClassifier(n_jobs=-1, class_weight='balanced_subsample', max_depth=5, random_state=100)\n",
    "    model.fit(X, y)\n",
    "    # define Boruta feature selection method\n",
    "    \n",
    "    feature_selector = BorutaPy(model, n_estimators='auto', verbose=0, random_state=100, max_iter=140)\n",
    "\n",
    "    # find all relevant features\n",
    "    feature_selector.fit(X.to_numpy(),y)\n",
    "\n",
    "    # check selected features\n",
    "    ##--feature_selector.support_\n",
    "\n",
    "    # check ranking of features\n",
    "    ##--feature_ranking=feature_selector.ranking_\n",
    "\n",
    "    # zip feature names, ranks, and decisions \n",
    "    # feature_ranks = list(zip(feature_names, \n",
    "    #                          feature_selector.ranking_, \n",
    "    #                          feature_selector.support_))\n",
    "\n",
    "    # print the results\n",
    "    ##--for feat in feature_ranks:\n",
    "    ##--    print('Feature: {:<30} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    "        \n",
    "    final_features = list()\n",
    "    indexes = np.where(feature_selector.ranking_ <= 2) #change to 2\n",
    "    for x in np.nditer(indexes):\n",
    "        final_features.append(feature_names[x])\n",
    "    ##--print(final_features)\n",
    "    \n",
    " # call transform() on X to filter it down to selected features\n",
    "    return pd.DataFrame(X.filter(final_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicolinearity treatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the highly collinear features from data\n",
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model \n",
    "        to generalize and improves the interpretability of the model.\n",
    "\n",
    "    Inputs: \n",
    "        x: features dataframe\n",
    "        threshold: features with correlations greater than this value are removed\n",
    "\n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i+1):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "\n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                #Print the correlated features and the correlation value\n",
    "                #print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns=drops)\n",
    "\n",
    "    return x\n",
    "\n",
    "#remove_collinear_features(X, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(X_train, y_train, sampler):\n",
    "    \n",
    "    #SMOTE\n",
    "    if sampler == 'SMOTE':\n",
    "        sampler = SMOTE(random_state=100)    \n",
    "    \n",
    "    #ROSE\n",
    "    if sampler == 'ROSE':\n",
    "        sampler = RandomOverSampler(random_state=100, shrinkage=1)\n",
    "\n",
    "    #ADASYN\n",
    "    if sampler == 'ADASYN':\n",
    "        sampler = ADASYN(random_state=100)\n",
    "    \n",
    "\n",
    "    #SMOTTEENN\n",
    "    if sampler == 'SMOTEENN' :\n",
    "        sampler = SMOTEENN(random_state=100)\n",
    "        \n",
    "        \n",
    "    #Random under Sampling\n",
    "    if sampler == \"randomunder\":\n",
    "        sampler = RandomUnderSampler(random_state=100)\n",
    "\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    #counter = Counter(y_resampled)\n",
    "    #print(counter)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# X_train, y_train = sampling(X_train, y_train,'SMOTE')\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X_train, y_train, X_test, y_test):\n",
    "    # building model before balancing data\n",
    "    model = RandomForestClassifier(random_state=1, n_estimators=1000, max_depth=5)\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "    #For TEST SPLIT\n",
    "    y_pred= model.predict(X_test)\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy= accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred) ##\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    specificity = cf_matrix[1][1] / ( cf_matrix[1][1] + cf_matrix[1][0] )\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    type_1_error_FP = cf_matrix[0][1]\n",
    "    type_2_error_FN = cf_matrix[1][0]\n",
    "    #Note by default 1 is the positive label. Therefore, -1 is negative\n",
    "    #bad waffe -> 2 line of matrix -> POSITIVE -> data = -1\n",
    "\n",
    "    #For TRAIN SPLIT\n",
    "    y_pred_train= model.predict(X_train)\n",
    "    cf_matrix_train = confusion_matrix(y_train, y_pred_train)\n",
    "    accuracy_train= accuracy_score(y_train, y_pred_train)\n",
    "    f1_train = f1_score(y_train, y_pred_train) ##\n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    recall_train = recall_score(y_train, y_pred_train)\n",
    "    specificity_train = cf_matrix_train[1][1] / ( cf_matrix_train[1][1] + cf_matrix_train[1][0] )\n",
    "    auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    type_1_error_FP_train = cf_matrix_train[0][1]\n",
    "    type_2_error_FN_train = cf_matrix_train[1][0]\n",
    "\n",
    "\n",
    "    return cf_matrix, accuracy, f1, precision, recall, specificity, type_1_error_FP, type_2_error_FN, auc, cf_matrix_train, accuracy_train, f1_train, precision_train, recall_train, specificity_train, type_1_error_FP_train, type_2_error_FN_train, auc_train\n",
    "\n",
    "#run_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast_track_cols = X_test.columns\n",
    "# fast_track_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 1170, 1: 1170})\n",
      "0 3s&knn__uniform&BoP&SMOTE \n",
      " acc 0.8121019108280255 \n",
      " accuracy_train 0.8901709401709401 \n",
      " f1 0.2337662337662338 \n",
      " f1_train 0.8950592078399346 \n",
      " spec 0.42857142857142855 \n",
      " auc 0.6340809361287179 \n",
      " cfm \n",
      " [[246  47]\n",
      " [ 12   9]] \n",
      " cols Index(['feature_20', 'feature_22', 'feature_34', 'feature_60', 'feature_104',\n",
      "       'feature_122', 'feature_130', 'feature_131', 'feature_206',\n",
      "       'feature_248', 'feature_342', 'feature_349', 'feature_427',\n",
      "       'feature_478', 'feature_511', 'feature_520'],\n",
      "      dtype='object') \n",
      "\n",
      "Counter({-1: 1170, 1: 1170})\n",
      "1 3s&knn__uniform&BoP&ROSE \n",
      " acc 0.7961783439490446 \n",
      " accuracy_train 0.8487179487179487 \n",
      " f1 0.23809523809523808 \n",
      " f1_train 0.847938144329897 \n",
      " spec 0.47619047619047616 \n",
      " auc 0.6476515520884121 \n",
      " cfm \n",
      " [[240  53]\n",
      " [ 11  10]] \n",
      " cols Index(['feature_1', 'feature_20', 'feature_22', 'feature_34', 'feature_60',\n",
      "       'feature_104', 'feature_118', 'feature_122', 'feature_130',\n",
      "       'feature_131', 'feature_206', 'feature_248', 'feature_342',\n",
      "       'feature_349', 'feature_478', 'feature_511'],\n",
      "      dtype='object') \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:24<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 1170, 1: 1170})\n",
      "2 3s&knn__uniform&BoS__shap&SMOTE \n",
      " acc 0.8280254777070064 \n",
      " accuracy_train 0.8726495726495727 \n",
      " f1 0.3076923076923077 \n",
      " f1_train 0.8773662551440329 \n",
      " spec 0.5714285714285714 \n",
      " auc 0.7089224768405655 \n",
      " cfm \n",
      " [[248  45]\n",
      " [  9  12]] \n",
      " cols Index(['feature_118', 'feature_442', 'feature_22', 'feature_164',\n",
      "       'feature_435', 'feature_133', 'feature_461', 'feature_148',\n",
      "       'feature_34', 'feature_60', 'feature_478', 'feature_342', 'feature_65',\n",
      "       'feature_124', 'feature_332', 'feature_206', 'feature_66',\n",
      "       'feature_349', 'feature_299'],\n",
      "      dtype='object') \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 1170, 1: 1170})\n",
      "3 3s&knn__uniform&BoS__shap&ROSE \n",
      " acc 0.821656050955414 \n",
      " accuracy_train 0.8474358974358974 \n",
      " f1 0.3170731707317073 \n",
      " f1_train 0.8448500651890481 \n",
      " spec 0.6190476190476191 \n",
      " auc 0.7276125467251747 \n",
      " cfm \n",
      " [[245  48]\n",
      " [  8  13]] \n",
      " cols Index(['feature_34', 'feature_60', 'feature_478', 'feature_342', 'feature_118',\n",
      "       'feature_22', 'feature_133', 'feature_65', 'feature_164', 'feature_435',\n",
      "       'feature_206', 'feature_66', 'feature_349', 'feature_299',\n",
      "       'feature_148'],\n",
      "      dtype='object') \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:40<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 1170, 1: 1170})\n",
      "4 3s&knn__uniform&BoS__gini&SMOTE \n",
      " acc 0.7261146496815286 \n",
      " accuracy_train 0.8038461538461539 \n",
      " f1 0.14 \n",
      " f1_train 0.806899453092133 \n",
      " spec 0.3333333333333333 \n",
      " auc 0.5437997724687145 \n",
      " cfm \n",
      " [[221  72]\n",
      " [ 14   7]] \n",
      " cols Index(['feature_427', 'feature_154', 'feature_354', 'feature_133',\n",
      "       'feature_65', 'feature_442', 'feature_171', 'feature_66', 'feature_289',\n",
      "       'feature_299', 'feature_461'],\n",
      "      dtype='object') \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:42<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 1170, 1: 1170})\n",
      "5 3s&knn__uniform&BoS__gini&ROSE \n",
      " acc 0.8248407643312102 \n",
      " accuracy_train 0.8038461538461539 \n",
      " f1 0.24657534246575344 \n",
      " f1_train 0.7875983341045811 \n",
      " spec 0.42857142857142855 \n",
      " auc 0.6409068746952706 \n",
      " cfm \n",
      " [[250  43]\n",
      " [ 12   9]] \n",
      " cols Index(['feature_427', 'feature_354', 'feature_133', 'feature_65',\n",
      "       'feature_442', 'feature_171', 'feature_66', 'feature_289',\n",
      "       'feature_299', 'feature_461'],\n",
      "      dtype='object') \n",
      "\n",
      "Counter({-1: 1170, 1: 1170})\n",
      "6 3s&knn__uniform&RFE__RF&SMOTE \n",
      " acc 0.8407643312101911 \n",
      " accuracy_train 0.9273504273504274 \n",
      " f1 0.21875 \n",
      " f1_train 0.9299258037922505 \n",
      " spec 0.3333333333333333 \n",
      " auc 0.6052332195676905 \n",
      " cfm \n",
      " [[257  36]\n",
      " [ 14   7]] \n",
      " cols Index(['feature_1', 'feature_20', 'feature_22', 'feature_32', 'feature_34',\n",
      "       'feature_60', 'feature_92', 'feature_122', 'feature_130', 'feature_131',\n",
      "       'feature_153', 'feature_206', 'feature_337', 'feature_427',\n",
      "       'feature_478', 'feature_489'],\n",
      "      dtype='object') \n",
      "\n",
      "Counter({-1: 1170, 1: 1170})\n",
      "7 3s&knn__uniform&RFE__RF&ROSE \n",
      " acc 0.8280254777070064 \n",
      " accuracy_train 0.8713675213675214 \n",
      " f1 0.27027027027027023 \n",
      " f1_train 0.8683865325754264 \n",
      " spec 0.47619047619047616 \n",
      " auc 0.6647163985047945 \n",
      " cfm \n",
      " [[250  43]\n",
      " [ 11  10]] \n",
      " cols Index(['feature_1', 'feature_20', 'feature_22', 'feature_32', 'feature_34',\n",
      "       'feature_60', 'feature_104', 'feature_122', 'feature_130',\n",
      "       'feature_131', 'feature_342', 'feature_349', 'feature_424',\n",
      "       'feature_426', 'feature_427', 'feature_461'],\n",
      "      dtype='object') \n",
      "\n",
      "Counter({-1: 1170, 1: 1170})\n",
      "8 3s&MICE&BoP&SMOTE \n",
      " acc 0.7707006369426752 \n",
      " accuracy_train 0.8978632478632479 \n",
      " f1 0.2340425531914893 \n",
      " f1_train 0.9051963506545022 \n",
      " spec 0.5238095238095238 \n",
      " auc 0.6561027141231919 \n",
      " cfm \n",
      " [[231  62]\n",
      " [ 10  11]] \n",
      " cols Index(['feature_1', 'feature_20', 'feature_22', 'feature_34', 'feature_60',\n",
      "       'feature_104', 'feature_113', 'feature_122', 'feature_130',\n",
      "       'feature_131', 'feature_206', 'feature_248', 'feature_342',\n",
      "       'feature_349', 'feature_383', 'feature_478', 'feature_520'],\n",
      "      dtype='object') \n",
      "\n",
      "Counter({-1: 1170, 1: 1170})\n",
      "9 3s&MICE&BoP&ROSE \n",
      " acc 0.8152866242038217 \n",
      " accuracy_train 0.8824786324786325 \n",
      " f1 0.2564102564102564 \n",
      " f1_train 0.883127921801955 \n",
      " spec 0.47619047619047616 \n",
      " auc 0.6578904599382416 \n",
      " cfm \n",
      " [[246  47]\n",
      " [ 11  10]] \n",
      " cols Index(['feature_1', 'feature_20', 'feature_22', 'feature_34', 'feature_60',\n",
      "       'feature_104', 'feature_113', 'feature_122', 'feature_130',\n",
      "       'feature_131', 'feature_206', 'feature_248', 'feature_342',\n",
      "       'feature_349', 'feature_383', 'feature_478', 'feature_520'],\n",
      "      dtype='object') \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:23<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 1170, 1: 1170})\n",
      "10 3s&MICE&BoS__shap&SMOTE \n",
      " acc 0.8248407643312102 \n",
      " accuracy_train 0.8504273504273504 \n",
      " f1 0.32098765432098764 \n",
      " f1_train 0.8530646515533165 \n",
      " spec 0.6190476190476191 \n",
      " auc 0.729319031366813 \n",
      " cfm \n",
      " [[246  47]\n",
      " [  8  13]] \n",
      " cols Index(['feature_34', 'feature_60', 'feature_478', 'feature_342', 'feature_118',\n",
      "       'feature_22', 'feature_65', 'feature_164', 'feature_66', 'feature_349',\n",
      "       'feature_206', 'feature_299'],\n",
      "      dtype='object') \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:22<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 1170, 1: 1170})\n",
      "11 3s&MICE&BoS__shap&ROSE \n",
      " acc 0.8121019108280255 \n",
      " accuracy_train 0.8273504273504273 \n",
      " f1 0.2716049382716049 \n",
      " f1_train 0.8252595155709342 \n",
      " spec 0.5238095238095238 \n",
      " auc 0.6782870144644888 \n",
      " cfm \n",
      " [[244  49]\n",
      " [ 10  11]] \n",
      " cols Index(['feature_34', 'feature_60', 'feature_478', 'feature_342', 'feature_65',\n",
      "       'feature_435', 'feature_332', 'feature_206', 'feature_349',\n",
      "       'feature_66', 'feature_299', 'feature_148'],\n",
      "      dtype='object') \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:42<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 1170, 1: 1170})\n",
      "12 3s&MICE&BoS__gini&SMOTE \n",
      " acc 0.7834394904458599 \n",
      " accuracy_train 0.7619658119658119 \n",
      " f1 0.19047619047619047 \n",
      " f1_train 0.7455459113750571 \n",
      " spec 0.38095238095238093 \n",
      " auc 0.596619535186088 \n",
      " cfm \n",
      " [[238  55]\n",
      " [ 13   8]] \n",
      " cols Index(['feature_427', 'feature_154', 'feature_65', 'feature_442', 'feature_66',\n",
      "       'feature_3'],\n",
      "      dtype='object') \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:45<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 1170, 1: 1170})\n",
      "13 3s&MICE&BoS__gini&ROSE \n",
      " acc 0.8121019108280255 \n",
      " accuracy_train 0.75 \n",
      " f1 0.25316455696202533 \n",
      " f1_train 0.7096774193548387 \n",
      " spec 0.47619047619047616 \n",
      " auc 0.6561839752966033 \n",
      " cfm \n",
      " [[245  48]\n",
      " [ 11  10]] \n",
      " cols Index(['feature_154', 'feature_442', 'feature_66', 'feature_299', 'feature_3'], dtype='object') \n",
      "\n",
      "Counter({-1: 1170, 1: 1170})\n",
      "14 3s&MICE&RFE__RF&SMOTE \n",
      " acc 0.8535031847133758 \n",
      " accuracy_train 0.9256410256410257 \n",
      " f1 0.20689655172413793 \n",
      " f1_train 0.927860696517413 \n",
      " spec 0.2857142857142857 \n",
      " auc 0.5899561189663578 \n",
      " cfm \n",
      " [[262  31]\n",
      " [ 15   6]] \n",
      " cols Index(['feature_20', 'feature_22', 'feature_34', 'feature_60', 'feature_104',\n",
      "       'feature_112', 'feature_122', 'feature_131', 'feature_288',\n",
      "       'feature_337', 'feature_342', 'feature_383', 'feature_427',\n",
      "       'feature_478', 'feature_489', 'feature_520'],\n",
      "      dtype='object') \n",
      "\n",
      "Counter({-1: 1170, 1: 1170})\n",
      "15 3s&MICE&RFE__RF&ROSE \n",
      " acc 0.7961783439490446 \n",
      " accuracy_train 0.852991452991453 \n",
      " f1 0.23809523809523808 \n",
      " f1_train 0.8533674339300937 \n",
      " spec 0.47619047619047616 \n",
      " auc 0.6476515520884121 \n",
      " cfm \n",
      " [[240  53]\n",
      " [ 11  10]] \n",
      " cols Index(['feature_1', 'feature_20', 'feature_22', 'feature_34', 'feature_60',\n",
      "       'feature_104', 'feature_112', 'feature_122', 'feature_130',\n",
      "       'feature_131', 'feature_248', 'feature_342', 'feature_383',\n",
      "       'feature_427', 'feature_461', 'feature_478'],\n",
      "      dtype='object') \n",
      "\n",
      "[(0, '3s&knn__uniform&BoP&SMOTE', '3s', 'knn__uniform', 'BoP', 'SMOTE', Index(['feature_20', 'feature_22', 'feature_34', 'feature_60', 'feature_104',\n",
      "       'feature_122', 'feature_130', 'feature_131', 'feature_206',\n",
      "       'feature_248', 'feature_342', 'feature_349', 'feature_427',\n",
      "       'feature_478', 'feature_511', 'feature_520'],\n",
      "      dtype='object'), array([[246,  47],\n",
      "       [ 12,   9]]), 0.8121019108280255, 0.2337662337662338, 0.16071428571428573, 0.42857142857142855, 0.42857142857142855, 47, 12, 0.6340809361287179, array([[ 987,  183],\n",
      "       [  74, 1096]]), 0.8901709401709401, 0.8950592078399346, 0.8569194683346364, 0.9367521367521368, 0.9367521367521368, 183, 74, 0.8901709401709402), (1, '3s&knn__uniform&BoP&ROSE', '3s', 'knn__uniform', 'BoP', 'ROSE', Index(['feature_1', 'feature_20', 'feature_22', 'feature_34', 'feature_60',\n",
      "       'feature_104', 'feature_118', 'feature_122', 'feature_130',\n",
      "       'feature_131', 'feature_206', 'feature_248', 'feature_342',\n",
      "       'feature_349', 'feature_478', 'feature_511'],\n",
      "      dtype='object'), array([[240,  53],\n",
      "       [ 11,  10]]), 0.7961783439490446, 0.23809523809523808, 0.15873015873015872, 0.47619047619047616, 0.47619047619047616, 53, 11, 0.6476515520884121, array([[999, 171],\n",
      "       [183, 987]]), 0.8487179487179487, 0.847938144329897, 0.8523316062176166, 0.8435897435897436, 0.8435897435897436, 171, 183, 0.8487179487179487), (2, '3s&knn__uniform&BoS__shap&SMOTE', '3s', 'knn__uniform', 'BoS__shap', 'SMOTE', Index(['feature_118', 'feature_442', 'feature_22', 'feature_164',\n",
      "       'feature_435', 'feature_133', 'feature_461', 'feature_148',\n",
      "       'feature_34', 'feature_60', 'feature_478', 'feature_342', 'feature_65',\n",
      "       'feature_124', 'feature_332', 'feature_206', 'feature_66',\n",
      "       'feature_349', 'feature_299'],\n",
      "      dtype='object'), array([[248,  45],\n",
      "       [  9,  12]]), 0.8280254777070064, 0.3076923076923077, 0.21052631578947367, 0.5714285714285714, 0.5714285714285714, 45, 9, 0.7089224768405655, array([[ 976,  194],\n",
      "       [ 104, 1066]]), 0.8726495726495727, 0.8773662551440329, 0.846031746031746, 0.9111111111111111, 0.9111111111111111, 194, 104, 0.8726495726495727), (3, '3s&knn__uniform&BoS__shap&ROSE', '3s', 'knn__uniform', 'BoS__shap', 'ROSE', Index(['feature_34', 'feature_60', 'feature_478', 'feature_342', 'feature_118',\n",
      "       'feature_22', 'feature_133', 'feature_65', 'feature_164', 'feature_435',\n",
      "       'feature_206', 'feature_66', 'feature_349', 'feature_299',\n",
      "       'feature_148'],\n",
      "      dtype='object'), array([[245,  48],\n",
      "       [  8,  13]]), 0.821656050955414, 0.3170731707317073, 0.21311475409836064, 0.6190476190476191, 0.6190476190476191, 48, 8, 0.7276125467251747, array([[1011,  159],\n",
      "       [ 198,  972]]), 0.8474358974358974, 0.8448500651890481, 0.8594164456233422, 0.8307692307692308, 0.8307692307692308, 159, 198, 0.8474358974358975), (4, '3s&knn__uniform&BoS__gini&SMOTE', '3s', 'knn__uniform', 'BoS__gini', 'SMOTE', Index(['feature_427', 'feature_154', 'feature_354', 'feature_133',\n",
      "       'feature_65', 'feature_442', 'feature_171', 'feature_66', 'feature_289',\n",
      "       'feature_299', 'feature_461'],\n",
      "      dtype='object'), array([[221,  72],\n",
      "       [ 14,   7]]), 0.7261146496815286, 0.14, 0.08860759493670886, 0.3333333333333333, 0.3333333333333333, 72, 14, 0.5437997724687145, array([[922, 248],\n",
      "       [211, 959]]), 0.8038461538461539, 0.806899453092133, 0.7945318972659486, 0.8196581196581196, 0.8196581196581196, 248, 211, 0.8038461538461538), (5, '3s&knn__uniform&BoS__gini&ROSE', '3s', 'knn__uniform', 'BoS__gini', 'ROSE', Index(['feature_427', 'feature_354', 'feature_133', 'feature_65',\n",
      "       'feature_442', 'feature_171', 'feature_66', 'feature_289',\n",
      "       'feature_299', 'feature_461'],\n",
      "      dtype='object'), array([[250,  43],\n",
      "       [ 12,   9]]), 0.8248407643312102, 0.24657534246575344, 0.17307692307692307, 0.42857142857142855, 0.42857142857142855, 43, 12, 0.6409068746952706, array([[1030,  140],\n",
      "       [ 319,  851]]), 0.8038461538461539, 0.7875983341045811, 0.858728557013118, 0.7273504273504273, 0.7273504273504273, 140, 319, 0.8038461538461539), (6, '3s&knn__uniform&RFE__RF&SMOTE', '3s', 'knn__uniform', 'RFE__RF', 'SMOTE', Index(['feature_1', 'feature_20', 'feature_22', 'feature_32', 'feature_34',\n",
      "       'feature_60', 'feature_92', 'feature_122', 'feature_130', 'feature_131',\n",
      "       'feature_153', 'feature_206', 'feature_337', 'feature_427',\n",
      "       'feature_478', 'feature_489'],\n",
      "      dtype='object'), array([[257,  36],\n",
      "       [ 14,   7]]), 0.8407643312101911, 0.21875, 0.16279069767441862, 0.3333333333333333, 0.3333333333333333, 36, 14, 0.6052332195676905, array([[1042,  128],\n",
      "       [  42, 1128]]), 0.9273504273504274, 0.9299258037922505, 0.8980891719745223, 0.9641025641025641, 0.9641025641025641, 128, 42, 0.9273504273504273), (7, '3s&knn__uniform&RFE__RF&ROSE', '3s', 'knn__uniform', 'RFE__RF', 'ROSE', Index(['feature_1', 'feature_20', 'feature_22', 'feature_32', 'feature_34',\n",
      "       'feature_60', 'feature_104', 'feature_122', 'feature_130',\n",
      "       'feature_131', 'feature_342', 'feature_349', 'feature_424',\n",
      "       'feature_426', 'feature_427', 'feature_461'],\n",
      "      dtype='object'), array([[250,  43],\n",
      "       [ 11,  10]]), 0.8280254777070064, 0.27027027027027023, 0.18867924528301888, 0.47619047619047616, 0.47619047619047616, 43, 11, 0.6647163985047945, array([[1046,  124],\n",
      "       [ 177,  993]]), 0.8713675213675214, 0.8683865325754264, 0.8889883616830797, 0.8487179487179487, 0.8487179487179487, 124, 177, 0.8713675213675214), (8, '3s&MICE&BoP&SMOTE', '3s', 'MICE', 'BoP', 'SMOTE', Index(['feature_1', 'feature_20', 'feature_22', 'feature_34', 'feature_60',\n",
      "       'feature_104', 'feature_113', 'feature_122', 'feature_130',\n",
      "       'feature_131', 'feature_206', 'feature_248', 'feature_342',\n",
      "       'feature_349', 'feature_383', 'feature_478', 'feature_520'],\n",
      "      dtype='object'), array([[231,  62],\n",
      "       [ 10,  11]]), 0.7707006369426752, 0.2340425531914893, 0.1506849315068493, 0.5238095238095238, 0.5238095238095238, 62, 10, 0.6561027141231919, array([[ 960,  210],\n",
      "       [  29, 1141]]), 0.8978632478632479, 0.9051963506545022, 0.844559585492228, 0.9752136752136752, 0.9752136752136752, 210, 29, 0.8978632478632479), (9, '3s&MICE&BoP&ROSE', '3s', 'MICE', 'BoP', 'ROSE', Index(['feature_1', 'feature_20', 'feature_22', 'feature_34', 'feature_60',\n",
      "       'feature_104', 'feature_113', 'feature_122', 'feature_130',\n",
      "       'feature_131', 'feature_206', 'feature_248', 'feature_342',\n",
      "       'feature_349', 'feature_383', 'feature_478', 'feature_520'],\n",
      "      dtype='object'), array([[246,  47],\n",
      "       [ 11,  10]]), 0.8152866242038217, 0.2564102564102564, 0.17543859649122806, 0.47619047619047616, 0.47619047619047616, 47, 11, 0.6578904599382416, array([[1026,  144],\n",
      "       [ 131, 1039]]), 0.8824786324786325, 0.883127921801955, 0.8782755705832629, 0.888034188034188, 0.888034188034188, 144, 131, 0.8824786324786325), (10, '3s&MICE&BoS__shap&SMOTE', '3s', 'MICE', 'BoS__shap', 'SMOTE', Index(['feature_34', 'feature_60', 'feature_478', 'feature_342', 'feature_118',\n",
      "       'feature_22', 'feature_65', 'feature_164', 'feature_66', 'feature_349',\n",
      "       'feature_206', 'feature_299'],\n",
      "      dtype='object'), array([[246,  47],\n",
      "       [  8,  13]]), 0.8248407643312102, 0.32098765432098764, 0.21666666666666667, 0.6190476190476191, 0.6190476190476191, 47, 8, 0.729319031366813, array([[ 974,  196],\n",
      "       [ 154, 1016]]), 0.8504273504273504, 0.8530646515533165, 0.8382838283828383, 0.8683760683760684, 0.8683760683760684, 196, 154, 0.8504273504273505), (11, '3s&MICE&BoS__shap&ROSE', '3s', 'MICE', 'BoS__shap', 'ROSE', Index(['feature_34', 'feature_60', 'feature_478', 'feature_342', 'feature_65',\n",
      "       'feature_435', 'feature_332', 'feature_206', 'feature_349',\n",
      "       'feature_66', 'feature_299', 'feature_148'],\n",
      "      dtype='object'), array([[244,  49],\n",
      "       [ 10,  11]]), 0.8121019108280255, 0.2716049382716049, 0.18333333333333332, 0.5238095238095238, 0.5238095238095238, 49, 10, 0.6782870144644888, array([[982, 188],\n",
      "       [216, 954]]), 0.8273504273504273, 0.8252595155709342, 0.8353765323992994, 0.8153846153846154, 0.8153846153846154, 188, 216, 0.8273504273504274), (12, '3s&MICE&BoS__gini&SMOTE', '3s', 'MICE', 'BoS__gini', 'SMOTE', Index(['feature_427', 'feature_154', 'feature_65', 'feature_442', 'feature_66',\n",
      "       'feature_3'],\n",
      "      dtype='object'), array([[238,  55],\n",
      "       [ 13,   8]]), 0.7834394904458599, 0.19047619047619047, 0.12698412698412698, 0.38095238095238093, 0.38095238095238093, 55, 13, 0.596619535186088, array([[967, 203],\n",
      "       [354, 816]]), 0.7619658119658119, 0.7455459113750571, 0.8007850834151129, 0.6974358974358974, 0.6974358974358974, 203, 354, 0.7619658119658119), (13, '3s&MICE&BoS__gini&ROSE', '3s', 'MICE', 'BoS__gini', 'ROSE', Index(['feature_154', 'feature_442', 'feature_66', 'feature_299', 'feature_3'], dtype='object'), array([[245,  48],\n",
      "       [ 11,  10]]), 0.8121019108280255, 0.25316455696202533, 0.1724137931034483, 0.47619047619047616, 0.47619047619047616, 48, 11, 0.6561839752966033, array([[1040,  130],\n",
      "       [ 455,  715]]), 0.75, 0.7096774193548387, 0.8461538461538461, 0.6111111111111112, 0.6111111111111112, 130, 455, 0.75), (14, '3s&MICE&RFE__RF&SMOTE', '3s', 'MICE', 'RFE__RF', 'SMOTE', Index(['feature_20', 'feature_22', 'feature_34', 'feature_60', 'feature_104',\n",
      "       'feature_112', 'feature_122', 'feature_131', 'feature_288',\n",
      "       'feature_337', 'feature_342', 'feature_383', 'feature_427',\n",
      "       'feature_478', 'feature_489', 'feature_520'],\n",
      "      dtype='object'), array([[262,  31],\n",
      "       [ 15,   6]]), 0.8535031847133758, 0.20689655172413793, 0.16216216216216217, 0.2857142857142857, 0.2857142857142857, 31, 15, 0.5899561189663578, array([[1047,  123],\n",
      "       [  51, 1119]]), 0.9256410256410257, 0.927860696517413, 0.9009661835748792, 0.9564102564102565, 0.9564102564102565, 123, 51, 0.9256410256410258), (15, '3s&MICE&RFE__RF&ROSE', '3s', 'MICE', 'RFE__RF', 'ROSE', Index(['feature_1', 'feature_20', 'feature_22', 'feature_34', 'feature_60',\n",
      "       'feature_104', 'feature_112', 'feature_122', 'feature_130',\n",
      "       'feature_131', 'feature_248', 'feature_342', 'feature_383',\n",
      "       'feature_427', 'feature_461', 'feature_478'],\n",
      "      dtype='object'), array([[240,  53],\n",
      "       [ 11,  10]]), 0.7961783439490446, 0.23809523809523808, 0.15873015873015872, 0.47619047619047616, 0.47619047619047616, 53, 11, 0.6476515520884121, array([[ 995,  175],\n",
      "       [ 169, 1001]]), 0.852991452991453, 0.8533674339300937, 0.8511904761904762, 0.8555555555555555, 0.8555555555555555, 175, 169, 0.8529914529914531)]\n"
     ]
    }
   ],
   "source": [
    "X = read_features()\n",
    "y = read_target().iloc[:,0]\n",
    "\n",
    "result = []\n",
    "i = 0\n",
    "f = open('output/tracker.csv', 'w')\n",
    "# create the csv writer\n",
    "writer = csv.writer(f)\n",
    "\n",
    "\n",
    "\n",
    "#step 1:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1, stratify=y)\n",
    "\n",
    "\n",
    "#-----------\n",
    "#step 2:\n",
    "X_train = remove_duplicated_columns(X_train)\n",
    "#step 3:\n",
    "X_train = remove_constant_volatility(X_train)\n",
    "#step 4:\n",
    "X_train = remove_cols_with_high_pct_null(X_train, 0.8) #this can be in the loop too, may be later\n",
    "\n",
    "#step 5: remove the same columns from step 2-4 TRAIN_TEST split\n",
    "X_test = X_test.loc[:,X_train.columns]\n",
    "\n",
    "\n",
    "#------------\n",
    "\n",
    "\n",
    "# X_train = X_train.loc[:, fast_track_cols]\n",
    "# X_test = X_test.loc[:, fast_track_cols]\n",
    "\n",
    "#------------\n",
    "\n",
    "\n",
    "#step 6-9\n",
    "replace_outlier_options = ['3s']\n",
    "#replace_outlier_options = ['3s','NaN']\n",
    "impute_null_options = ['knn__uniform','MICE']\n",
    "#impute_null_options = ['knn__distance', 'MICE', 'knn__uniform']\n",
    "FS_options = ['BoP','BoS__shap','BoS__gini','RFE__RF']\n",
    "#FS_options = ['BoP','BoS__shap', 'BoS__gini', 'RFE__RF', 'RFE__SVM' ]\n",
    "sampling_options = ['SMOTE', 'ROSE']\n",
    "#sampling_options = ['SMOTE','ROSE','ADASYN','SMOTEENN']\n",
    "\n",
    "for replace_with in replace_outlier_options:\n",
    "    for knn_weight in impute_null_options:\n",
    "        for classifier_model in FS_options:\n",
    "            #<remove correlated columns, decide on the thresold 70%>\n",
    "            for sampling_technique in sampling_options:\n",
    "                X_train_temp = X_train\n",
    "                X_test_temp = X_test\n",
    "                y_train_temp = y_train\n",
    "                y_test_temp = y_test\n",
    "\n",
    "                #step 6: oulier treatement (on both TRAIN & TEST split)\n",
    "                X_train_temp = replace_outlier(X_train_temp, replace_with)\n",
    "                X_test_temp = replace_outlier(X_test_temp, replace_with)\n",
    "                \n",
    "                #step 7: missing value imputation (on both TRAIN & TEST split)\n",
    "                if knn_weight == 'knn__distance' or knn_weight == 'knn__uniform':\n",
    "                    X_train_temp, X_test_temp = impute_null_with_knn(X_train_temp, X_test_temp, knn_weight[-(len(knn_weight)-5):])\n",
    "                elif knn_weight == 'MICE':\n",
    "                    X_train_temp, X_test_temp = impute_null_with_mice(X_train_temp, X_test_temp)\n",
    "\n",
    "                #step 8: feature selection (on both TRAIN & TEST split)\n",
    "                if classifier_model == 'BoS__shap' or classifier_model == 'BoS__gini':\n",
    "                    X_train_temp = BorutaShap_FS(X_train_temp, y_train_temp, classifier_model[-(len(classifier_model)-5):])\n",
    "                elif classifier_model == 'RFE__RF' or classifier_model == 'RFE__SVM':\n",
    "                    X_train_temp = RFE_FS(X_train_temp, y_train_temp, classifier_model[-(len(classifier_model)-5):])\n",
    "                elif classifier_model == 'BoP':\n",
    "                    X_train_temp = BorutaPy_FS(X_train_temp, y_train_temp)\n",
    "                \n",
    "                #step 9: remove multilinear features\n",
    "                remove_collinear_features(X_train_temp, 0.7)\n",
    "\n",
    "                #apply the same result for TEST\n",
    "                X_test_temp = X_test_temp.loc[:,X_train_temp.columns]\n",
    "\n",
    "\n",
    "                #step 10: balancing only on TRAIN split\n",
    "                X_train_temp, y_train_temp = sampling(X_train_temp, y_train_temp, sampling_technique)\n",
    "\n",
    "\n",
    "                #step 11: train model, predict, and print scores\n",
    "                cf_matrix, accuracy, f1, precision, recall, specificity, type_1_error_FP, type_2_error_FN, auc, cf_matrix_train, accuracy_train, f1_train, precision_train, recall_train, specificity_train, type_1_error_FP_train, type_2_error_FN_train, auc_train = run_model(X_train_temp, y_train_temp, X_test_temp, y_test_temp)\n",
    "\n",
    "                combined_technique = replace_with +'&'+ knn_weight +'&'+ classifier_model +'&'+ sampling_technique\n",
    "                result.append((i, combined_technique, replace_with, knn_weight, classifier_model, sampling_technique, X_train_temp.columns, cf_matrix, accuracy, f1, precision, recall, specificity, type_1_error_FP, type_2_error_FN, auc, cf_matrix_train, accuracy_train, f1_train, precision_train, recall_train, specificity_train, type_1_error_FP_train, type_2_error_FN_train, auc_train))\n",
    "                \n",
    "                print(i, combined_technique,'\\n acc', accuracy,'\\n accuracy_train',accuracy_train, '\\n f1', f1,'\\n f1_train',f1_train, '\\n spec', specificity, '\\n auc', auc, '\\n cfm', '\\n', cf_matrix, '\\n cols', X_train_temp.columns, '\\n')\n",
    "                #print(i, combined_technique)\n",
    "                row = str(i) +' '+ combined_technique +' acc '+ str(accuracy) +' f1 '+ str(f1) +' acc_train '+ str(accuracy_train) +' f1_train '+ str(f1_train)\n",
    "                writer.writerow(row)\n",
    "\n",
    "                i+=1\n",
    "                #print out datasets for backup\n",
    "                # X_train_temp.to_csv('output/X_train_temp_'+str(i)+combined_technique'.csv')\n",
    "                # X_test_temp.to_csv('output/X_test_temp_'+str(i)+combined_technique'.csv')\n",
    "                # y_train_temp.to_csv('output/y_train_temp_'+str(i)+combined_technique'.csv')\n",
    "                # y_test_temp.to_csv('output/y_test_temp_'+str(i)+combined_technique'.csv')\n",
    "\n",
    "\n",
    "\n",
    "df_result = pd.DataFrame(result, columns = ['No.','combination','outlier_replace_with', 'imputation_knn_weight', 'FS_classifier_model', 'balancing_sampling_technique', 'cols','cf_matrix', 'accuracy', 'f1', 'precision', 'recall', 'specificity', 'type_1_error_FP', 'type_2_error_FN', 'auc', 'cf_matrix_train', 'accuracy_train', 'f1_train', 'precision_train', 'recall_train', 'specificity_train', 'type_1_error_FP_train', 'type_2_error_FN_train', 'auc_train'])\n",
    "df_result.to_csv('tracker/result.csv')\n",
    "print(result)\n",
    "# close the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems:\n",
    "1. We should take tentative features, ranking>1 (BurutaShap -> need to ressearch how)\n",
    "2. implement removing columns with high corr after FS_options \n",
    "3. Try other models, because RF is overfitting and we havd very bad F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------Appendix--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #------------------------------------\n",
    "# X_train = replace_outlier(X_train, how)\n",
    "# replace_outlier_options = ['NaN', '3s']\n",
    "# #--\n",
    "# X_train = impute_null_with_knn(X_train, which_weights) #\n",
    "# impute_null_options = ['knn__distance', 'knn__uniform', 'MICE']\n",
    "\n",
    "# X_train = impute_null_with_mice(X_train)\n",
    "# #--\n",
    "# BorutaShap_FS(X_train, y_train, method_option)\n",
    "# list_method=['shap','gini']\n",
    "\n",
    "# RFE_FS(X_train, y_train, classify) \n",
    "# list_clf=['RF','SVM']\n",
    "\n",
    "# BorutaPy_FS(X_train, y_train)\n",
    "\n",
    "# FS_options = ['BoS__shap', 'BoS__gini', 'RFE__RF', 'RFE__SVM', 'BoP']\n",
    "# #--\n",
    "# sampling(X_train, y_train, sampler)\n",
    "# sampling_options = ['SMOTE','ROSE','ADASYN','SMOTEENN']\n",
    "\n",
    "# #--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [2, 3, 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l = []\n",
    "# a=1\n",
    "# b=2\n",
    "# c=3\n",
    "\n",
    "# l.append([a,b,c])\n",
    "# l.append([b,c,a])\n",
    "\n",
    "# l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_null_impute = [knnimputation_distance, MICEimputation_distance]\n",
    "# list_null_outlier = [outlier_knn,outlier_3s]\n",
    "# list_feat_selection = ['Boruta_RF', 'Boruta_shap', 'RFE']\n",
    "# Boruta = ['RF', 'XGB']\n",
    "# Boruta_shap = ['RF', 'XGB', 'kNN']\n",
    "# RFE = ['RF', 'SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9\n",
       "3  1  1  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #TESTING\n",
    "# df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [1,1,1]]),\n",
    "#                    columns=['a', 'b', 'c'])\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3aeb15c8c31224d9ef37a76c0046f703a279f439b6efd04fb2681e5f2715bf2f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
