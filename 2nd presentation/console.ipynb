{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is to read, transform and join 2 data frame\n",
    "\n",
    "def read_features():\n",
    "    path = 'secom.data'\n",
    "    df = pd.read_csv(path, delimiter=' ', header=None, na_values=['NaN'])\n",
    "    df.columns = ['feature_'+str(x+1) for x in range(len(df.columns))]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def read_target():\n",
    "    path = 'secom_labels.data'\n",
    "    df = pd.read_csv(path, delimiter=' ', header=None, na_values=['NaN'])\n",
    "    df.columns = ['status','timestamp']\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'],dayfirst=True)\n",
    "    return df\n",
    "\n",
    "#for the testing purporse, trim to remain first 100 rows only\n",
    "X = read_features()\n",
    "y = read_target().iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 590)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 486)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the duplicated features (columns)\n",
    "def remove_duplicated_columns(df):\n",
    "    dict_duplicate_pair = {}\n",
    "    dict_duplicate_matches = {}\n",
    "    list_duplicate = []\n",
    "    to_remove = []\n",
    "    for i in range(0, len(df.columns)):\n",
    "        l = []\n",
    "        for j in range(i+1,len(df.columns)):\n",
    "            dict_duplicate_pair[str(i+1)+';'+str(j+1)] = df.iloc[:,i].equals(df.iloc[:,j])\n",
    "            if df.iloc[:,i].equals(df.iloc[:,j]) == True:\n",
    "                if j not in list_duplicate:\n",
    "                    l.append(j)\n",
    "                    to_remove.append('feature_'+str(j+1))\n",
    "                list_duplicate.append(i)\n",
    "                list_duplicate.append(j)\n",
    "        if len(l)!=0:\n",
    "            dict_duplicate_matches[i] = l\n",
    "\n",
    "\n",
    "    df_duplicate_pair = pd.DataFrame.from_dict(dict_duplicate_pair, orient='index')\n",
    "    df_duplicate_pair.columns=['duplicate']\n",
    "\n",
    "    df_duplicate_matches = pd.DataFrame.from_dict(dict_duplicate_matches, orient='index')\n",
    "\n",
    "    \n",
    "    df = df.drop(columns=to_remove, axis = 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "X = remove_duplicated_columns(X)\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove columns with Constant volatility (std=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 486)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 474)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_constant_volatility(df):\n",
    "    df_EDA= df.describe().T\n",
    "    df_EDA= df_EDA[df_EDA[\"std\"] == 0]\n",
    "    df = df.drop(axis=1, columns=df_EDA.index)\n",
    "    return df\n",
    "\n",
    "X = remove_constant_volatility(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove columns with high %Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 474)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 466)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_cols_with_high_pct_null(df, null_threshold):\n",
    "    list_column_with_pct_null = pd.concat([df.isnull().sum(), df.isnull().sum()/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\n",
    "    list_column_with_pct_null= list_column_with_pct_null[list_column_with_pct_null[\"Percentage (%)\"] >= null_threshold]\n",
    "    df = df.drop(axis=1, columns=list_column_with_pct_null.index)\n",
    "    return df\n",
    "\n",
    "X = remove_cols_with_high_pct_null(X, 0.8)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_581</th>\n",
       "      <th>feature_582</th>\n",
       "      <th>feature_583</th>\n",
       "      <th>feature_584</th>\n",
       "      <th>feature_585</th>\n",
       "      <th>feature_586</th>\n",
       "      <th>feature_587</th>\n",
       "      <th>feature_588</th>\n",
       "      <th>feature_589</th>\n",
       "      <th>feature_590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>-0.0033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>1.3424</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>-0.0057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>1.4333</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>43.5231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>1.4622</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>93.4941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>137.7844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows Ã— 466 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_1  feature_2  feature_3  feature_4  feature_5  feature_7  \\\n",
       "0       3030.93    2564.00  2187.7333  1411.1265     1.3602    97.6133   \n",
       "1       3095.78    2465.14  2230.4222  1463.6606     0.8294   102.3433   \n",
       "2       2932.61    2559.94  2186.4111  1698.0172     1.5102    95.4878   \n",
       "3       2988.72    2479.90  2199.0333   909.7926     1.3204   104.2367   \n",
       "4       3032.24    2502.87  2233.3667  1326.5200     1.5334   100.3967   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1562    2899.41    2464.36  2179.7333  3085.3781     1.4843    82.2467   \n",
       "1563    3052.31    2522.55  2198.5667  1124.6595     0.8763    98.4689   \n",
       "1564    2978.81    2379.78  2206.3000  1110.4967     0.8236    99.4122   \n",
       "1565    2894.92    2532.01  2177.0333  1183.7287     1.5726    98.7978   \n",
       "1566    2944.92    2450.76  2195.4444  2914.1792     1.5978    85.1011   \n",
       "\n",
       "      feature_8  feature_9  feature_10  feature_11  ...  feature_581  \\\n",
       "0        0.1242     1.5005      0.0162     -0.0034  ...          NaN   \n",
       "1        0.1247     1.4966     -0.0005     -0.0148  ...       0.0060   \n",
       "2        0.1241     1.4436      0.0041      0.0013  ...       0.0148   \n",
       "3        0.1217     1.4882     -0.0124     -0.0033  ...       0.0044   \n",
       "4        0.1235     1.5031     -0.0031     -0.0072  ...          NaN   \n",
       "...         ...        ...         ...         ...  ...          ...   \n",
       "1562     0.1248     1.3424     -0.0045     -0.0057  ...       0.0047   \n",
       "1563     0.1205     1.4333     -0.0061     -0.0093  ...          NaN   \n",
       "1564     0.1208        NaN         NaN         NaN  ...       0.0025   \n",
       "1565     0.1213     1.4622     -0.0072      0.0032  ...       0.0075   \n",
       "1566     0.1235        NaN         NaN         NaN  ...       0.0045   \n",
       "\n",
       "      feature_582  feature_583  feature_584  feature_585  feature_586  \\\n",
       "0             NaN       0.5005       0.0118       0.0035       2.3630   \n",
       "1        208.2045       0.5019       0.0223       0.0055       4.4447   \n",
       "2         82.8602       0.4958       0.0157       0.0039       3.1745   \n",
       "3         73.8432       0.4990       0.0103       0.0025       2.0544   \n",
       "4             NaN       0.4800       0.4766       0.1045      99.3032   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1562     203.1720       0.4988       0.0143       0.0039       2.8669   \n",
       "1563          NaN       0.4975       0.0131       0.0036       2.6238   \n",
       "1564      43.5231       0.4987       0.0153       0.0041       3.0590   \n",
       "1565      93.4941       0.5004       0.0178       0.0038       3.5662   \n",
       "1566     137.7844       0.4987       0.0181       0.0040       3.6275   \n",
       "\n",
       "      feature_587  feature_588  feature_589  feature_590  \n",
       "0             NaN          NaN          NaN          NaN  \n",
       "1          0.0096       0.0201       0.0060     208.2045  \n",
       "2          0.0584       0.0484       0.0148      82.8602  \n",
       "3          0.0202       0.0149       0.0044      73.8432  \n",
       "4          0.0202       0.0149       0.0044      73.8432  \n",
       "...           ...          ...          ...          ...  \n",
       "1562       0.0068       0.0138       0.0047     203.1720  \n",
       "1563       0.0068       0.0138       0.0047     203.1720  \n",
       "1564       0.0197       0.0086       0.0025      43.5231  \n",
       "1565       0.0262       0.0245       0.0075      93.4941  \n",
       "1566       0.0117       0.0162       0.0045     137.7844  \n",
       "\n",
       "[1567 rows x 466 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how = ['NaN', '3s']\n",
    "def replace_outlier(df, how):\n",
    "    for col in df:\n",
    "        ll_col = df[col].mean() - 3 * df[col].std()\n",
    "        ul_col = df[col].mean() + 3 * df[col].std()\n",
    "        if how == 'NaN':\n",
    "            df[col] = np.where(df[col]>ul_col,np.NaN,np.where(df[col]<ll_col,np.NaN,df[col]))\n",
    "        elif how == '3s':\n",
    "            df[col] = np.where(df[col]>ul_col,ul_col,np.where(df[col]<ll_col,ll_col,df[col]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9\n",
       "3  1  1  1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING\n",
    "df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [1,1,1]]),\n",
    "                   columns=['a', 'b', 'c'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.122281</td>\n",
       "      <td>7.162278</td>\n",
       "      <td>8.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b     c\n",
       "0  1.000000  2.000000  3.00\n",
       "1  4.000000  5.000000  6.00\n",
       "2  6.122281  7.162278  8.25\n",
       "3  1.000000  1.000000  1.25"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING\n",
    "#more complex outlier removal\n",
    "how = '3s'\n",
    "for col in df:\n",
    "    ll_col = df[col].mean() - 1 * df[col].std()\n",
    "    ul_col = df[col].mean() + 1 * df[col].std()\n",
    "    if how == 'NaN':\n",
    "        df[col] = np.where(df[col]>ul_col,np.NaN,np.where(df[col]<ll_col,np.NaN,df[col]))\n",
    "    elif how == '3s':\n",
    "        df[col] = np.where(df[col]>ul_col,ul_col,np.where(df[col]<ll_col,ll_col,df[col]))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_weights = ['','']\n",
    "def impute_null_with_knn(X_train, X_test, which_weights):\n",
    "    #First scale the data \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns= X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns= X_test.columns)\n",
    "\n",
    "    knn = KNNImputer(n_neighbors=20, weights='which_weights')\n",
    "\n",
    "    X_train = pd.DataFrame(knn.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(knn.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    X_train = pd.DataFrame(scaler.inverse_transform(X_train), columns= X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.inverse_transform(X_test), columns= X_test.columns)\n",
    "    return X_train, X_test\n",
    "\n",
    "X_train = null_imputation_knn(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null_with_mice(X_train, X_test): \n",
    "    imp = IterativeImputer(maX_train_iter=10, verbose=2, imputation_order='roman', random_state=0)\n",
    "    X_train = pd.DataFrame(imp.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(imp.transform(X_test), columns=X_test.columns)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1253, 466)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_method=['shap','gini']\n",
    "\n",
    "def BorutaShap_FS (X, y, method_option) :\n",
    "    modelshap = RandomForestClassifier(n_jobs=-1,n_estimators=100, class_weight='balanced_subsample', max_depth=5, random_state=100)\n",
    "    # define model for resp. classifier\n",
    "    modelshap.fit(X,y)\n",
    "    ##-- feature_names = np.array(X.columns)\n",
    "    # define Boruta Sahp feature selection method\n",
    "    feature_selector = BorutaShap(model=modelshap,\n",
    "                              importance_measure=method_option,\n",
    "                              classification=True)  # find all relevant features\n",
    "    feature_selector.fit(X,y,n_trials=100,sample = False, verbose = True,random_state=100)  \n",
    "    ##-- feature_selector.plot(which_features='accepted',figsize=(20,10))\n",
    "    # call transform() on X to filter it down to selected features\n",
    "    return  feature_selector.Subset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE function with random forest\n",
    "\n",
    "def RFE_FS (X, y, classify) :\n",
    "    feature_names = np.array(X.columns)\n",
    "    if classify == 'RF':\n",
    "    # define random forest classifier\n",
    "        model = RandomForestClassifier(n_jobs=-1, class_weight='balanced_subsample', max_depth=5, random_state=100)\n",
    "        #model.fit(X, y)\n",
    "        #rfe = RFE(estimator = model,n_features_to_select = 15)\n",
    "    if classify== 'SVM':\n",
    "        model = SVC(kernel='linear',C=5)\n",
    "        #model.fit(X, y)\n",
    "        #rfe = RFECV(estimator = model,scoring='accuracy')\n",
    "    # find all relevant features\n",
    "    model.fit(X, y)\n",
    "    rfe = RFE(estimator = model,n_features_to_select = 15)\n",
    "    rfe.fit(X,y)\n",
    "\n",
    "    # check selected features\n",
    "    ##--rfe.support_\n",
    "\n",
    "    # check ranking of features\n",
    "    ##--rfe.ranking_\n",
    "\n",
    "    # zip feature names, ranks, and decisions \n",
    "    feature_ranks = list(zip(feature_names, \n",
    "                             rfe.ranking_, \n",
    "                             rfe.support_))\n",
    "\n",
    "    # print the results\n",
    "    ##--for feat in feature_ranks:\n",
    "    ##--    print('Feature: {:<30} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    "        \n",
    "    final_features_rfe = list()\n",
    "    indexes = np.where(rfe.ranking_ <= 1)\n",
    "    for x in np.nditer(indexes):\n",
    "        final_features_rfe.append(feature_names[x])\n",
    "    ##-- print(final_features_rfe)\n",
    "    \n",
    " # call transform() on X to filter it down to selected features\n",
    "    return pd.DataFrame(X.filter(final_features_rfe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boruta function with random forest\n",
    "\n",
    "def BorutaPy_FS (X, y) :\n",
    "    feature_names = np.array(X.columns)\n",
    "\n",
    "    # define random forest classifier\n",
    "    model = RandomForestClassifier(n_jobs=-1, class_weight='balanced_subsample', max_depth=5, random_state=100)\n",
    "    model.fit(X, y)\n",
    "    # define Boruta feature selection method\n",
    "    \n",
    "    feature_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=100, max_iter=140)\n",
    "\n",
    "    # find all relevant features\n",
    "    feature_selector.fit(X.to_numpy(),y)\n",
    "\n",
    "    # check selected features\n",
    "    ##--feature_selector.support_\n",
    "\n",
    "    # check ranking of features\n",
    "    ##--feature_ranking=feature_selector.ranking_\n",
    "\n",
    "    # zip feature names, ranks, and decisions \n",
    "    feature_ranks = list(zip(feature_names, \n",
    "                             feature_selector.ranking_, \n",
    "                             feature_selector.support_))\n",
    "\n",
    "    # print the results\n",
    "    ##--for feat in feature_ranks:\n",
    "    ##--    print('Feature: {:<30} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    "        \n",
    "    final_features = list()\n",
    "    indexes = np.where(feature_selector.ranking_ <= 1)\n",
    "    for x in np.nditer(indexes):\n",
    "        final_features.append(feature_names[x])\n",
    "    ##--print(final_features)\n",
    "    \n",
    " # call transform() on X to filter it down to selected features\n",
    "    return pd.DataFrame(X.filter(final_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('output/X_train')\n",
    "y_train.to_csv('output/y_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.read_csv('output/X_train').iloc[:,1:]\n",
    "# y_train = pd.read_csv('output/y_train').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1253, 12)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 1170, 1: 1170})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2340, 12)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sampling(X_train, y_train, sampler):\n",
    "    \n",
    "    #SMOTE\n",
    "    if sampler == 'SMOTE':\n",
    "        sampler = SMOTE(random_state=100)    \n",
    "    \n",
    "    #ROSE\n",
    "    if sampler == 'ROSE':\n",
    "        sampler = RandomOverSampler(random_state=100, shrinkage=1)\n",
    "\n",
    "    #ADASYN\n",
    "    if sampler == 'ADASYN':\n",
    "        sampler = ADASYN(random_state=100)\n",
    "    \n",
    "\n",
    "    #SMOTTEENN\n",
    "    if sampler == 'SMOTEENN' :\n",
    "        sampler = SMOTEENN(random_state=100)\n",
    "        \n",
    "        \n",
    "    #Random under Sampling\n",
    "    if sampler == \"randomunder\":\n",
    "        sampler = RandomUnderSampler(random_state=100)\n",
    "\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    counter = Counter(y_resampled)\n",
    "    print(counter)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "X_train, y_train = sampling(X_train, y_train,'SMOTE')\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove cols that do not exist in Train data\n",
    "X_test = X_test.loc[:,X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = outlierknn(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 12)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X_train, y_train, X_test, y_test):\n",
    "    # building model before balancing data\n",
    "    model = RandomForestClassifier(random_state=1, n_estimators=1000, max_depth=5)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred= model.predict(X_test)\n",
    "    accuracy= accuracy_score(y_test, y_pred)\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    specificity = cf_matrix[1][1] / ( cf_matrix[1][1] + cf_matrix[1][0] )\n",
    "    return accuracy, specificity, cf_matrix\n",
    "\n",
    "run_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "i = 0\n",
    "\n",
    "#step 1\n",
    "X_train = remove_duplicated_columns(X_train)\n",
    "#step 2:\n",
    "X_train = remove_constant_volatility(X_train)\n",
    "#step 3:\n",
    "X_train = remove_cols_with_high_pct_null(X_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#step 4:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1,stratify=y)\n",
    "\n",
    "\n",
    "#step 5-8\n",
    "replace_outlier_options = ['NaN', '3s']\n",
    "impute_null_options = ['knn__distance', 'knn__uniform', 'MICE']\n",
    "FS_options = ['BoS__shap', 'BoS__gini', 'RFE__RF', 'RFE__SVM', 'BoP']\n",
    "sampling_options = ['SMOTE','ROSE','ADASYN','SMOTEENN']\n",
    "\n",
    "for replace_with in replace_outlier_options:\n",
    "    for knn_weight in impute_null_options:\n",
    "        for classifier_model in FS_options:\n",
    "            for sampling_technique in sampling_options:\n",
    "                X_train_temp = X_train\n",
    "                X_test_temp = X_test\n",
    "                y_train_temp = y_train\n",
    "                y_test_temp = y_test\n",
    "\n",
    "                #step 5: oulier treatement (on both TRAIN & TEST split)\n",
    "                X_train_temp = replace_outlier(X_train_temp, replace_with)\n",
    "                X_test_temp = replace_outlier(X_test_temp, replace_with)\n",
    "                \n",
    "                #step 6: missing value imputation (on both TRAIN & TEST split)\n",
    "                if knn_weight == 'knn__distance' or knn_weight == 'knn__uniform':\n",
    "                    X_train_temp, X_test_temp = impute_null_with_knn(X_train_temp, X_test_temp, knn_weight[-(len(knn_weight)-5):])\n",
    "                elif knn_weight == 'MICE':\n",
    "                    X_train_temp, X_test_temp = impute_null_with_mice(X_train_temp, X_test_temp)\n",
    "\n",
    "\n",
    "                if classifier_model == 'BoS__shap' or classifier_model == 'BoS__gini':\n",
    "                    X_train_temp = BorutaShap_FS(X_train_temp, y, classifier_model[-(len(classifier_model)-5):])\n",
    "                elif classifier_model == 'RFE__RF' or classifier_model == 'RFE__SVM':\n",
    "                    X_train_temp = RFE_FS(X_train_temp, y, classifier_model[-(len(classifier_model)-5):])\n",
    "                elif classifier_model == 'BoP':\n",
    "                    X_train_temp = BorutaPy_FS(X_train, y_train)\n",
    "                \n",
    "\n",
    "                #step 7: sampling only on TRAIN split\n",
    "                X_train_temp, y_train_temp = sampling(X_train_temp, y_train_temp, sampling_technique)\n",
    "                X_train_temp.to_csv('output/X_train_temp_'+str(i)+'.csv')\n",
    "                X_test_temp.to_csv('output/X_test_temp_'+str(i)+'.csv')\n",
    "                y_train_temp.to_csv('output/y_train_temp_'+str(i)+'.csv')\n",
    "                y_test_temp.to_csv('output/y_test_temp_'+str(i)+'.csv')\n",
    "                i+=1\n",
    "\n",
    "\n",
    "                #step 8: train model, predict, and print scores\n",
    "                accuracy, specificity, cf_matrix = run_model(X_train_temp, y_train_temp, X_test_temp, y_test_temp)\n",
    "                result.append([i, accuracy, specificity, cf_matrix, X_train_temp.columns])\n",
    "\n",
    "\n",
    "df_result = pd.DataFrame(result)\n",
    "df_result.to_csv('output/result.csv')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------Appendix--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------\n",
    "X_train = replace_outlier(X_train, how)\n",
    "replace_outlier_options = ['NaN', '3s']\n",
    "#--\n",
    "X_train = impute_null_with_knn(X_train, which_weights) #\n",
    "impute_null_options = ['knn__distance', 'knn__uniform', 'MICE']\n",
    "\n",
    "X_train = impute_null_with_mice(X_train):\n",
    "#--\n",
    "BorutaShap_FS(X_train, y_train, method_option)\n",
    "list_method=['shap','gini']\n",
    "\n",
    "RFE_FS(X_train, y_train, classify) \n",
    "list_clf=['RF','SVM']\n",
    "\n",
    "BorutaPy_FS(X_train, y_train)\n",
    "\n",
    "FS_options = ['BoS__shap', 'BoS__gini', 'RFE__RF', 'RFE__SVM', 'BoP']\n",
    "#--\n",
    "sampling(X_train, y_train, sampler)\n",
    "sampling_options = ['SMOTE','ROSE','ADASYN','SMOTEENN']\n",
    "\n",
    "#--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'di'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str = 'knn__di'\n",
    "str[-(len(str)-5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [2, 3, 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "a=1\n",
    "b=2\n",
    "c=3\n",
    "\n",
    "l.append([a,b,c])\n",
    "l.append([b,c,a])\n",
    "\n",
    "l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_null_impute = [knnimputation_distance, MICEimputation_distance]\n",
    "list_null_outlier = [outlier_knn,outlier_3s]\n",
    "list_feat_selection = ['Boruta_RF', 'Boruta_shap', 'RFE']\n",
    "Boruta = ['RF', 'XGB']\n",
    "Boruta_shap = ['RF', 'XGB', 'kNN']\n",
    "RFE = ['RF', 'SVC']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3aeb15c8c31224d9ef37a76c0046f703a279f439b6efd04fb2681e5f2715bf2f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
